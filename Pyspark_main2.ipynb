{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy.get_group(name: Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]]]) → FrameLike[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame([('falcon', 'bird', 389.0),\n",
    "                    ('parrot', 'bird', 24.0),\n",
    "                    ('lion', 'mammal', 80.5),\n",
    "                    ('monkey', 'mammal', np.nan)],\n",
    "                   columns=['name', 'class', 'max_speed'],\n",
    "                   index=[0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf.GroupBy(\"class\").get_group(\"mammal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf.GroupBy(df.A).cummax().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- pyspark.sql.GroupedData.sum -->\n",
    "df.groupBy().sum('age').collect()\n",
    "df3.groupBy().sum('age', 'height').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PandasCogroupedOps.applyInPandas(func: PandasCogroupedMapFunction, schema: Union[pyspark.sql.types.StructType, str]) →\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupedData.applyInPandas(func: PandasGroupedMapFunction, schema: Union[pyspark.sql.types.StructType, str])\n",
    "import pandas as pd  \n",
    "from pyspark.sql.functions import pandas_udf, ceil\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").applyInPandas(\n",
    "    normalize, schema=\"id long, v double\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row as Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row.asDict(recursive: bool = False) → Dict[str, Any\n",
    "Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
    "df.withColumn('a', df['a'].dropFields('b', 'c')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
    "df.select(df.r.getField(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.worldometers.info/world-population/\"\n",
    "    response= requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabl = soup.find(\"table\", class_=\"table table-hover table-condensed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []  # Created Empty List\n",
    "# Extract header row\n",
    "for i in tabl.find_all('th'):\n",
    "    header.append(i.text)\n",
    "with open(\"world_population_by_region.csv\", \"wt\",newline='',encoding='utf-8') as csv_file:\n",
    "    csv_writer = writer(csv_file, delimiter ='|')\n",
    "    csv_writer.writerow(header) # write header\n",
    "    # Write data to csv file\n",
    "    for row in tabl.find_all('tr')[1:]:\n",
    "        td = row.find_all('td')\n",
    "        r = [i.text.replace('\\n','') for i in td]\n",
    "        csv_writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.ilike.SQL ILIKE expression (case insensitive LIKE). Returns a boolean Column based on a case insensitive match.\n",
    "df.filter(df.name.ilike('%Ice')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windows\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\n",
    "df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\n",
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.GroupedData.min\n",
    "# GroupedData.min(*cols: str) \n",
    "df.groupBy().min('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.mean\n",
    "df.groupBy().mean('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sales').agg({'Price':'Count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\00824732\\\\Desktop\\\\geek6\\\\personal\\\\New folder (2)\\\\required'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country').agg({'Price':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.agg\n",
    "gdf = df.groupBy(df.name)\n",
    "sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "@pandas_udf('int', PandasUDFType.GROUPED_AGG)  \n",
    "def min_udf(v):\n",
    "    return v.min()\n",
    "sorted(gdf.agg(min_udf(df.age)).collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- FREQUENCY TABLE OR CROSS TABLE IN PYSPARK – 2 WAY CROSS TABLE -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crosstab('Item_group','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Item_group','price').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.drop('country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('custno').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['custno','city']\n",
    "df.drop(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "## drop multiple columns using position\n",
    " \n",
    "spark.createDataFrame(df_orders.select(df_orders.columns[:2]).take(5)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop column name which starts with the specific string  in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import null\n",
    "def drop_null(df_orders):\n",
    "    null_counts=df_orders.select(count())\n",
    "    to_drop=[k for k,v in null_counts.items() if v>0]\n",
    "    df_orders=df_orders.drop(*to_drop)\n",
    "    return df_orders\n",
    "\n",
    "drop_null(df_orders).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop column name which contains a specific string  in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_lis=df_orders.columns\n",
    "columns_to_drop=[i for i in some_list if i.endwith('p')]\n",
    "df_order.drop(*columns_to_drop).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics or summary statistics of dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('custno').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([mean('country_sales'),min('sales')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET DATA TYPE OF COLUMN IN PYSPARK (SINGLE & MULTIPLE COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting data type of single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_basket1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-5.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-5.ipynb?jupyter-notebook#X26sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m df_basket1\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mprintSchema()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_basket1' is not defined"
     ]
    }
   ],
   "source": [
    "df_basket1.select('Price').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of multiple column in pyspark : Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price','Item_name').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.select('Price','Item_name').dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of all the columns in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET LIST OF COLUMNS AND ITS DATA TYPE IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1:  using printSchema() function\n",
    "df_basket1.printSchema()\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price').printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of single column in pyspark using dtypes – Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price').dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEAN, VARIANCE AND STANDARD DEVIATION OF COLUMN IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the column in pyspark with example:\n",
    "df_basket1.agg({'Price': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Variance of the column in pyspark\n",
    "df_basket1.agg({'Price': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of the column in pyspark with example:\n",
    "df_basket1.agg({'Price': 'stddev'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.groupby('Item_group').agg({'Price': 'mean'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket1.groupby('Item_group').agg({'Price': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by keeping the Last duplicate occurrence in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import row_number\n",
    " \n",
    "df_orders1 = df_orders.select(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\", F.row_number().over(Window.partitionBy(\"received_date\").orderBy(df_orders['received_date'])).alias(\"row_num\")) \n",
    "df_orders1.groupBy(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\").max(\"row_num\").show()\n",
    "df_orders1=df_orders.where(\"cust_no!=23512\")\n",
    "df_orders1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NULL rows with where condition in pyspark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_orders1 = df_orders.where(col('Shipped_date').isNotNull())\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with NA or missing values in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders1=df_orders.dropna()\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with NA or missing values in pyspark : Method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_orders1 = df_orders.dropDuplicates()\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.dropDuplicates((['cust_no'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by keeping the first duplicate occurrence in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import row_number\n",
    "df_orders1 = df_orders.select(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\", F.row_number().over(Window.partitionBy(\"received_date\").orderBy(df_orders['received_date'])).alias(\"row_num\")) \n",
    "df_orders1.filter(df_orders1.row_num ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-select-single-multiple-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket1.select(df_basket1.columns[0],df_basket1.columns[2]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select using Regex with column name like in pyspark (select column name like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.select(df_basket1.colRegex(\"`(Item)+?.+`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTINCT ROWS OF DATAFRAME IN PYSPARK – DROP DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicate rows in pyspark by a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.dropDuplicates((['Price'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct value of a column in pyspark – distinct() \n",
    "df_basket.select(\"Item_group\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and orderby in pyspark:\n",
    "df_basket.orderBy(F.col(\"Price\").desc()).dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows by keeping the first duplicate occurrence in pyspark:\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number\n",
    "df_basket1 = df_basket.select(\"Item_group\",\"Item_name\",\"Price\", F.row_number().over(Window.partitionBy(\"Price\").orderBy(df_basket['price'])).alias(\"row_num\"))\n",
    "df_basket1.filter(df_basket1.row_num ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAISED TO POWER OF COLUMN IN PYSPARK – SQUARE, CUBE , SQUARE ROOT AND CUBE ROOT IN PYSPARK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 2).alias(\"Math_score_square\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube of the column in pyspark with example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 3).alias(\"Math_score_cube\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of N to the column in pyspark with example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 4).alias(\"Math_score_power\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square root of the column in pyspark with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 1/2).alias(\"Math_score_squareroot\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min and Max Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.agg({'Price': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.agg({'Price': 'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round off to decimal places using round() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "df_states.select(\"*\", round(col('hindex_score'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "df_states.select(\"*\", round(col('hindex_score'),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset-or-filter-data-with-multiple-conditions-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('mathematics_score > 50 or science_score > 50').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.filter((f.col('mathematics_score') > 50) & (f.col('science_score') > 50)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.filter((f.col('mathematics_score') > 60)| (f.col('science_score') > 60)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.colRegex(\"`(mathe)+?.+`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.endswith('i')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.startswith('Em')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.contains('an')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as f\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.window import Window\n",
    "df_percent = df_basket1.withColumn('price_percent',f.col('Price')/f.sum('Price').over(Window.partitionBy())*100)\n",
    "df_percent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum and Partition By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percent = df_basket1.withColumn('price_percent',f.col('Price')/f.sum('Price').over(Window.partitionBy())*100)\n",
    "df_cum_percent = df_percent.withColumn('cum_percent', f.sum(df_percent.price_percent).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "df_cum_percent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "df_percent = df_basket1.withColumn('price_percent', f.col('Price')/f.sum('Price').over(Window.partitionBy('Item_group'))*100)\n",
    "df_cum_percent_grp = df_percent.withColumn('cum_percent_grp', f.sum(df_percent.price_percent).over(Window.partitionBy('Item_group').orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "df_cum_percent_grp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dataframe in pyspark by multiple columns – descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_detail1 = df_student_detail.orderBy('grad_score','science_score', ascending=False)\n",
    "df_student_detail1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dataframe in pyspark by multiple columns – ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_detail1 = df_student_detail.orderBy('grad_score','science_score')\n",
    "df_student_detail1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUMULATIVE SUM OF COLUMN AND GROUP IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "cum_sum = df_basket1.withColumn('cumsum', f.sum(df_basket1.Price).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cumulative sum of the column by Group in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rowsBetween(-sys.maxsize, 0) along with sum function is \n",
    "# used to create cumulative sum of the column, \n",
    "# an additional partitionBy() function of Item_group column \n",
    "# calculates the cumulative sum of each group as shown below\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "cum_sum = df_basket1.withColumn('cumsum', f.sum(df_basket1.Price).over(Window.partitionBy('Item_group').orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At First we will be replacing the missing \n",
    "# and NaN values with 0, using fill.na(0) ;\n",
    "#  then will use Sum() function and partitionBy\n",
    "#  a column name is used to calculate the cumulative \n",
    "#   sum of the “Price” column\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "df_basket2=df_basket2.fillna(0)\n",
    "cum_sum = df_basket2.withColumn('cumsum', f.sum(df_basket2.Price).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of multiple columns in pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct value of all the columns in pyspark using – distinct() function : Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_basket.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of a column in pyspark – distinct() – Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.select(\"Price\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of a column – dropDuplicates() – Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_basket.dropDuplicates((['Price'])).select(\"Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct Value of multiple columns in pyspark: Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.select(\"Item_group\",\"Price\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct value of all the columns using dropDuplicates() function : Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_basket.dropDuplicates().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.groupBy(f, numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "result=rdd.groupby(lambda x:x%2).collect()\n",
    "sorted([(x,sorted(y)) for x,y in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "sc.parallelize([1,2,3]).reduce(add)\n",
    "sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.pipe(command, env=None, checkCode=False\n",
    "sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.isCheckpointed¶\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Returns a printable version of the configuration, as a list of key=value pairs, one per line.\n",
    "# pyspark.SparkConf.toDebugString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.count\n",
    "# RDD.count()[source]¶\n",
    "sc.parallelize([2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.name\n",
    "RDD.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.cache\n",
    "# RDD.cache()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.getNumPartitions\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.take\n",
    "# RDD.take(num)\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.coalesce\n",
    "RDD.coalesce(numPartitions, shuffle=False)\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.setJobDescription\n",
    "SparkContext.setJobDescription(value)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.groupWith\n",
    "w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "z = sc.parallelize([(\"b\", 42)])\n",
    "[(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.top\n",
    "# RDD.top(num, key=None)[source]\n",
    "sc.parallelize([10, 4, 2, 12, 3]).top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.takeOrdered\n",
    "# RDD.takeOrdered(num, key=None)[source]¶\n",
    "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.combineByKey\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(x.combineByKey(to_list, append, extend).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.Broadcast.destroy\n",
    "# Broadcast.destroy(blocking=False)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.show_profiles¶\n",
    "SparkContext.show_profiles()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.sortByKey\n",
    "# RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda>>)\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.hadoopFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.mapPartitions\n",
    "# RDD.mapPartitions(f, preservesPartitioning=False)[source]\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.toLocalIterator\n",
    "rdd = sc.parallelize(range(10))\n",
    "[x for x in rdd.toLocalIterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.saveAsPickleFile\n",
    "# RDD.saveAsPickleFile(path, batchSize=10)\n",
    "from tempfile import NamedTemporaryFile\n",
    "tmpFile = NamedTemporaryFile(delete=True)\n",
    "tmpFile.close()\n",
    "sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
    "sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.addFile\n",
    "from pyspark import SparkFiles\n",
    "path = os.path.join(tempdir, \"test.txt\")\n",
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"100\")\n",
    "sc.addFile(path)\n",
    "def func(iterator):\n",
    "   with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
    "       fileVal = int(testFile.readline())\n",
    "       return [x * fileVal for x in iterator]\n",
    "sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.BarrierTaskContext.allGather\n",
    "# BarrierTaskContext.allGather(message='')[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.textFile\n",
    "# SparkContext.textFile(name, minPartitions=None, use_unicode=True)[source]\n",
    "path = os.path.join(tempdir, \"sample-text.txt\")\n",
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"Hello world!\")\n",
    "textFile = sc.textFile(path)\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.binaryFiles\n",
    "# SparkContext.binaryFiles(path, minPartitions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.BarrierTaskContext.partitionId¶\n",
    "\n",
    "BarrierTaskContext.partitionId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDDBarrier.mapPartitionsWithIndex\n",
    "RDDBarrier.mapPartitionsWithIndex(f, preservesPartitioning=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkConf.setSparkHome\n",
    "# SparkConf.setSparkHome(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkFiles.getRootDirectory\n",
    "# classmethod SparkFiles.getRootDirectory()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDDBarrier.mapPartitions\n",
    "# RDDBarrier.mapPartitions(f, preservesPartitioning=False)[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.SparkContext.binaryRecords\n",
    "# # SparkContext.binaryRecords(path, recordLength)\n",
    "# Load data from a flat binary file, assuming each record is a set of numbers with the specified numerical format (see ByteBuffer), and the number of bytes per record is constant.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr\n",
    "# Directory to the input data files\n",
    "\n",
    "# recordLengthint\n",
    "# The length at which to split the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.StorageLevel.OFF_HEAP\n",
    "StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.broadcast\n",
    "# SparkContext.broadcast(value)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.applicationId\n",
    "# A unique identifier for the Spark application. Its format depends on the scheduler implementation.\n",
    "\n",
    "# in case of local spark app something like ‘local-1433865536131’\n",
    "\n",
    "# in case of YARN something like ‘application_1433865536131_34483’\n",
    "sc.applicationId  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.BarrierTaskContext.resources\n",
    "# BarrierTaskContext.resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.BarrierTaskContext.taskAttemptId\n",
    "# BarrierTaskContext.taskAttemptId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.BarrierTaskContext.stageId\n",
    "# BarrierTaskContext.stageId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.BarrierTaskContext.attemptNumber\n",
    "# BarrierTaskContext.attemptNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.newAPIHadoopFile\n",
    "# SparkContext.newAPIHadoopFile(path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.cancelJobGroup\n",
    "# SparkContext.cancelJobGroup(groupId)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.setIfMissing\n",
    "SparkConf.setIfMissing(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkConf\n",
    "# class pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None)\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"My app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.TaskContext.get\n",
    "# classmethod TaskContext.get()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.setMaster\n",
    "SparkConf.setMaster(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.saveAsNewAPIHadoopDataset\n",
    "# RDD.saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)[so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.fullOuterJoin\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "sorted(x.fullOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.saveAsHadoopFile\n",
    "# RDD.saveAsHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.saveAsSequenceFile\n",
    "RDD.saveAsSequenceFile(path, compressionCodecClass=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.set\n",
    "SparkConf.set(key, value)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.parallelize\n",
    "# Distribute a local Python collection to form an RDD. Using range is recommended if the input represents a range for performance.\n",
    "sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.TaskContext.getLocalProperty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.TaskContext.partitionId\n",
    "TaskContext.partitionId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkConf.setExecutorEnv    \n",
    "SparkConf.setExecutorEnv(key=None, value=None, pairs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.groupBy\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.BarrierTaskContext.getLocalProperty\n",
    "BarrierTaskContext.getLocalProperty(key)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classmethod BarrierTaskContext.get()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.foldByKey\n",
    "# RDD.foldByKey(zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash>)[source]\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "from operator import add\n",
    "sorted(rdd.foldByKey(0, add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.TaskContext.stageId\n",
    "TaskContext.stageId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.getStorageLevel\n",
    "RDD.getStorageLevel()[source]¶\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "rdd1.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.glom\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "sorted(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class pyspark.BarrierTaskInfo(address)[source]\n",
    "# TaskContext.attemptNumber()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.BarrierTaskInfo\n",
    "class pyspark.BarrierTaskInfo(address)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.BarrierTaskContext.getTaskInfos\n",
    "BarrierTaskContext.getTaskInfos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.TaskContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.TaskContext.resources\n",
    "# TaskContext.resources()[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.get\n",
    "SparkConf.get(key, defaultValue=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.BarrierTaskContext.barrier\n",
    "BarrierTaskContext.barrier()[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.cancelAllJobs\n",
    "SparkContext.cancelAllJobs()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.accumulator\n",
    "# SparkContext.accumulator(value, accum_param=None)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDDBarrier\n",
    "class pyspark.RDDBarrier(rdd)[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.TaskContext.taskAttemptId\n",
    "# TaskContext.taskAttemptId()[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkFiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.getAll\n",
    "SparkConf.getAll()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.range\n",
    "# SparkContext.range(start, end=None, step=1, numSlices=None)[source]\n",
    "sc.range(5).collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.sequenceFile\n",
    "# SparkContext.sequenceFile(path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.histogram\n",
    "rdd = sc.parallelize(range(51))\n",
    "rdd.histogram(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.hadoopRDD\n",
    "# SparkContext.hadoopRDD(inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.setAll\n",
    "parkConf.setAll(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkFiles.get\n",
    "classmethod SparkFiles.get(filename)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.zipWithUniqueId()\n",
    "sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.union\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.withResources\n",
    "# RDD.withResources(profile)[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.saveAsHadoopDataset\n",
    "# RDD.saveAsHadoopDataset(conf, keyConverter=None, valueConverter=None)[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.groupByKey\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.runJob\n",
    "# Executes the given partitionFunc on the specified set of partitions, returning the result as an array of elements.\n",
    "\n",
    "# If ‘partitions’ is not specified, this will run over all partitions.\n",
    "myRDD = sc.parallelize(range(6), 3)\n",
    "sc.runJob(myRDD, lambda part: [x * x for x in part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.setLocalProperty\n",
    "SparkContext.setLocalProperty(key, value)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.treeAggregate\n",
    "# RDD.treeAggregate(zeroValue, seqOp, combOp, depth=2)[source]¶\n",
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "rdd.treeAggregate(0, add, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.treeReduce\n",
    "# RDD.treeReduce(f, depth=2)[source]\n",
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "rdd.treeReduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.collectWithJobGroup\n",
    "RDD.collectWithJobGroup(groupId, description, interruptOnCancel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.zip\n",
    "# RDD.zip(other)[source]\n",
    "# Zips this RDD with another one, returning key-value pairs with the first element in each RDD second element in each RDD, etc. Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other)\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.AccumulatorParam.addInPlace\n",
    "Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.mapPartitionsWithIndex(f, preservesPartitioning=False)[source]\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.isCheckpointed\n",
    "RDD.isCheckpointed()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.Broadcast.unpersist\n",
    "# # Broadcast.unpersist(blocking=False)[source]¶\n",
    "# Delete cached copies of this broadcast on the executors. If the broadcast is used after this is called, it will need to be re-sent to each executor.\n",
    "\n",
    "# Parameters:\n",
    "# blockingbool, optional\n",
    "# Whether to block until unpersisting has completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.randomSplit\n",
    "# RDD.randomSplit(weights, seed=None)[source]\n",
    "rdd = sc.parallelize(range(500), 1)\n",
    "rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
    "len(rdd1.collect() + rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.resources\n",
    "property SparkContext.resources¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.emptyRDD\n",
    "SparkContext.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.contains\n",
    "SparkConf.contains(key)[source]¶    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.rightOuterJoin\n",
    "# RDD.rightOuterJoin(other, numPartitions=None)[source]¶\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(y.rightOuterJoin(x).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.setCheckpointDir\n",
    "# Set the directory under which RDDs are going to be checkpointed. The directory must be an HDFS path if running on a cluster.\n",
    "pyspark.SparkContext.setCheckpointDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.getCheckpointDir\n",
    "SparkContext.getCheckpointDir()[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.getLocalProperty\n",
    "# # SparkContext.getLocalProperty(key)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.sumApprox(timeout, confidence=0.95)\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "r = sum(range(1000))\n",
    "abs(rdd.sumApprox(1000) - r) / r < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # SparkContext.setLogLevel(logLevel)[source]\n",
    "    # Control our logLevel. This overrides any user-defined log settings. Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.sortBy\n",
    "# RDD.sortBy(keyfunc, ascending=True, numPartitions=None)[source]\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Broadcast.value\n",
    "property Broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Accumulator.value\n",
    "property Accumulator.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkConf.setAppName\n",
    "SparkConf.setAppName(value)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.sparkUser\n",
    "SparkContext.sparkUser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.statusTracker\n",
    "SparkContext.statusTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.sampleStdev\n",
    "sc.parallelize([1, 2, 3]).sampleStdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Broadcast.value\n",
    "property Broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.mapPartitionsWithSplit\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithSplit(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.Broadcast.dump\n",
    "# Broadcast.dump(value, f)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.Broadcast.load\n",
    "# Broadcast.load(file)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.uiWebUrl\n",
    "# property SparkContext.uiWebUrl\n",
    "# Return the URL of the SparkUI instance started by this SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.join\n",
    "# RDD.join(other, numPartitions=None)\n",
    "# Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "\n",
    "# Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.\n",
    "\n",
    "# Performs a hash join across the cluster\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.setName\n",
    "rdd1 = sc.parallelize([1, 2])\n",
    "rdd1.setName('RDD1').name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.setSystemProperty\n",
    "# Set a Java system property, such as spark.executor.memory. This must must be invoked before instantiating SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.pickleFile\n",
    "tmpFile = NamedTemporaryFile(delete=True)\n",
    "tmpFile.close()\n",
    "sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
    "sorted(sc.pickleFile(tmpFile.name, 3).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.AccumulatorParam.zero¶\n",
    "AccumulatorParam.zero(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.countApprox\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "rdd.countApprox(1000, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.isLocallyCheckpointed\n",
    "# RDD.isLocallyCheckpointed()[source]\n",
    "# Return whether this RDD is marked for local checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.map(f, preservesPartitioning=False)\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "sorted(rdd.map(lambda x: (x, 1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.keyBy\n",
    "x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
    "y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property SparkContext.defaultMinPartitions\n",
    "# Default min number of partitions for Hadoop RDDs when not given by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property SparkContext.defaultParallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.dump_profiles\n",
    "SparkContext.dump_profiles(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Accumulator\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "def f(x):\n",
    "    global a\n",
    "    a += x\n",
    "rdd.foreach(f)\n",
    "a.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.repartition\n",
    "# Return a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "# Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle.\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "sorted(rdd.glom().collect())\n",
    "len(rdd.repartition(2).glom().collect())\n",
    "2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.RDD.aggregate\n",
    "# # RDD.aggregate(zeroValue, seqOp, combOp)[source]\n",
    "# Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”\n",
    "\n",
    "# The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "\n",
    "# The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.repartitionAndSortWithinPartitions\n",
    "# RDD.repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=<function portable_hash>, ascending=True, keyfunc=<function RDD.<lambda>>)[source]\n",
    "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.reduceByKey\n",
    "# RDD.reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.union\n",
    "path = os.path.join(tempdir, \"union-text.txt\")\n",
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"Hello\")\n",
    "textFile = sc.textFile(path)\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Broadcast\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext('local', 'test')\n",
    "b = sc.broadcast([1, 2, 3, 4, 5])\n",
    "b.value\n",
    "sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.fold\n",
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.aggregateByKey\n",
    "RDD.aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.lookup\n",
    "l = range(1000)\n",
    "rdd = sc.parallelize(zip(l, l), 10)\n",
    "rdd.lookup(42)\n",
    "sorted = rdd.sortByKey()\n",
    "sorted.lookup(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.min\n",
    "rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
    "rdd.min()\n",
    "rdd.min(key=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.meanApprox\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "r = sum(range(1000)) / 1000.0\n",
    "abs(rdd.meanApprox(1000) - r) / r < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.sample\n",
    "# RDD.sample(withReplacement, fraction, seed=None)[source]\n",
    "rdd = sc.parallelize(range(100), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.barrier\n",
    "RDD.barrier()[source]¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.filter\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.cogroup\n",
    "# RDD.cogroup(other, numPartitions=None)\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.collect\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.getResourceProfile\n",
    "RDD.getResourceProfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.checkpoint\n",
    "RDD.checkpoint()[source]\n",
    "Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext.setCheckpointDir() and all references to its parent RDDs will be removed. This function must be called before any job has been executed on this RDD. It is strongly recommended that this RDD is persisted in memory, otherwise saving it on a file will require recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.addPyFile\n",
    "SparkContext.addPyFile(path)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.context\n",
    "property RDD.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.withResources\n",
    "# RDD.withResources(profile)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.sampleByKey\n",
    "fractions = {\"a\": 0.2, \"b\": 0.1}\n",
    "rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
    "sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
    "100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.countApproxDistinct\n",
    "n = sc.parallelize(range(1000)).map(str).countApproxDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.unpersist\n",
    "RDD.unpersist(blocking=False)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.collectAsMap\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "m[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.distinct\n",
    "sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.subtractByKey\n",
    "# RDD.subtractByKey(other, numPartitions=None)[source]\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtractByKey(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.toDebugString\n",
    "# RDD.toDebugString()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.version\n",
    "property SparkContext.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.takeSample\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "len(rdd.takeSample(True, 20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.cartesian\n",
    "rdd = sc.parallelize([1, 2])\n",
    "sorted(rdd.cartesian(rdd).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.reduceByKeyLocally\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKeyLocally(add).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.leftOuterJoin\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.meanApprox\n",
    "# RDD.meanApprox(timeout, confidence=0.95)\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "r = sum(range(1000)) / 1000.0\n",
    "abs(rdd.meanApprox(1000) - r) / r < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.countByValue\n",
    "# RDD.countByValue()[source]¶\n",
    "sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.first\n",
    "# RDD.first()\n",
    "sc.parallelize([2, 3, 4]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.sampleVariance()[source]\n",
    "sc.parallelize([1, 2, 3]).sampleVariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.Broadcast.load_from_path\n",
    "Broadcast.load_from_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.variance()[source]¶\n",
    "\n",
    "sc.parallelize([1, 2, 3]).variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.partitionBy\n",
    "# RDD.partitionBy(numPartitions, partitionFunc=<function portable_hash>)\n",
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "sets = pairs.partitionBy(2).glom().collect()\n",
    "len(set(sets[0]).intersection(set(sets[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.zipWithIndex\n",
    "sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.subtract(other, numPartitions=None)\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.sum\n",
    "# RDD.sum()\n",
    "sc.parallelize([1.0, 2.0, 3.0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.sum()[source]\n",
    "sc.parallelize([1.0, 2.0, 3.0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.stdev\n",
    "sc.parallelize([1, 2, 3]).stdev()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.stop\n",
    "SparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.groupBy\n",
    "# RDD.groupBy(f, numPartitions=None, partitionFunc=<function portable_hash>)[source]¶\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.keys\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
    "m.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.mapValues(f)[source]\n",
    "x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return len(x)\n",
    "x.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.values\n",
    "m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
    "m.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.foreach\n",
    "# RDD.foreach(f)\n",
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.foreachPartition\n",
    "# RDD.foreachPartition(f)\n",
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "         print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.getCheckpointFile\n",
    "# RDD.getCheckpointFile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.max\n",
    "# RDD.max(key=None)\n",
    "rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD.flatMap(f, preservesPartitioning=False)\n",
    "rdd = sc.parallelize([2, 3, 4])\n",
    "sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.SparkContext.startTime\n",
    "# Return the epoch time when the Spark Context was started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.getConf\n",
    "SparkContext.getConf()[source]¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.countByKey\n",
    "## Count the number of elements for each key, and return the result to the master as a dictionary.\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.countByKey().items())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.groupBy\n",
    "# RDD.groupBy(f, numPartitions=None, partitionFunc=<function portable_hash>)[source]¶\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.flatMapValues\n",
    "# Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD’s partitioning.\n",
    "x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
    "def f(x): \n",
    "    return x\n",
    "x.flatMapValues(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.stats\n",
    "RDD.stats()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.intersection(other)\n",
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.streaming.DStream.rightOuterJoin\n",
    "# Return a new DStream by applying ‘right outer join’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions.\n",
    "DStream.rightOuterJoin(other, numPartitions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQueryManager.resetTerminated\n",
    "# StreamingQueryManager.resetTerminated()[source]\n",
    "# Forget about past terminated queries so that awaitAnyTermination() can be used again to wait for new terminations.\n",
    "spark.streams.resetTerminated()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.partitionBy\n",
    "# DataStreamWriter.partitionBy(*cols)[source]\n",
    "# Partitions the output by the given columns on the file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.queryName\n",
    "# DataStreamWriter.queryName(queryName)[source]\n",
    "# Specifies the name of the StreamingQuery that can be started with start(). This name must be unique among all the currently active queries in the associated SparkSession.\n",
    "writer = sdf.writeStream.queryName('streaming_query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataStreamReader.text(path, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None)\n",
    "# pyspark.sql.streaming.DataStreamReader.text\n",
    "text_sdf = spark.readStream.text(tempfile.mkdtemp())\n",
    "text_sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.outputMode\n",
    "# DataStreamWriter.outputMode(outputMode)[source]\n",
    "# Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Options include:\n",
    "\n",
    "# append: Only the new rows in the streaming DataFrame/Dataset will be written to\n",
    "# the sink\n",
    "\n",
    "# complete: All the rows in the streaming DataFrame/Dataset will be written to the sink\n",
    "# every time these are some updates\n",
    "\n",
    "# update: only the rows that were updated in the streaming DataFrame/Dataset will be\n",
    "# written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to append mode.\n",
    "writer = sdf.writeStream.outputMode('append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamReader.load\n",
    "# DataStreamReader.load(path=None, format=None, schema=None, **options)[source]\n",
    "# Loads a data stream from a data source and returns it as a DataFrame.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr, optional\n",
    "# optional string for file-system backed data sources.\n",
    "\n",
    "# formatstr, optional\n",
    "# optional string for format of the data source. Default to ‘parquet’.\n",
    "\n",
    "# schemapyspark.sql.types.StructType or str, optional\n",
    "# optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# **optionsdict\n",
    "# all other string options\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This API is evolving.\n",
    "\n",
    "# Examples\n",
    "\n",
    "json_sdf = spark.readStream.format(\"json\") \\\n",
    "    .schema(sdf_schema) \\\n",
    "    .load(tempfile.mkdtemp())\n",
    "json_sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.window\n",
    "# DStream.window(windowDuration, slideDuration=None)[source]\n",
    "# Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream.\n",
    "\n",
    "# Parameters:\n",
    "# windowDurationint\n",
    "# width of the window; must be a multiple of this DStream’s batching interval\n",
    "\n",
    "# slideDurationint, optional\n",
    "# sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream’s batching interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.join\n",
    "DStream.join(other, numPartitions=None)[source]\n",
    "# Return a new DStream by applying ‘join’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.cogroup\n",
    "DStream.cogroup(other, numPartitions=None)[source]\n",
    "# Return a new DStream by applying ‘cogroup’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yspark.streaming.DStream.reduceByKeyAndWindow\n",
    "DStream.reduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration=None, numPartitions=None, filterFunc=None)[source]\n",
    "Return a new DStream by applying incremental reduceByKey over a sliding window.\n",
    "\n",
    "The reduced value of over a new window is calculated using the old window’s reduce value :\n",
    "reduce the new values that entered the window (e.g., adding new counts)\n",
    "\n",
    "“inverse reduce” the old values that left the window (e.g., subtracting old counts)\n",
    "\n",
    "invFunc can be None, then it will reduce all the RDDs in window, could be slower than having invFunc.\n",
    "\n",
    "Parameters:\n",
    "funcfunction\n",
    "associative and commutative reduce function\n",
    "\n",
    "invFuncfunction\n",
    "inverse function of reduceFunc\n",
    "\n",
    "windowDurationint\n",
    "width of the window; must be a multiple of this DStream’s batching interval\n",
    "\n",
    "slideDurationint, optional\n",
    "sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream’s batching interval\n",
    "\n",
    "numPartitionsint, optional\n",
    "number of partitions of each RDD in the new DStream.\n",
    "\n",
    "filterFuncfunction, optional\n",
    "function to filter expired key-value pairs; only pairs that satisfy the function are retained set this to null if you do not want to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.explain\n",
    "StreamingQuery.explain(extended=False)[source]\n",
    "# Prints the (logical and physical) plans to the console for debugging purpose.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# extendedbool, optional\n",
    "# default False. If False, prints only the physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.pprint\n",
    "DStream.pprint(num=10)[source]\n",
    "# Print the first num elements of each RDD generated in this DStream.\n",
    "\n",
    "# Parameters:\n",
    "# numint, optional\n",
    "# the number of elements from the first will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.flatMap\n",
    "DStream.flatMap(f, preservesPartitioning=False)[source]\n",
    "# Return a new DStream by applying a function to all elements of this DStream, and then flattening the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.streaming.kinesis.KinesisUtils.createStream\n",
    "static KinesisUtils.createStream(ssc, kinesisAppName, streamName, endpointUrl, regionName, initialPositionInStream, checkpointInterval, storageLevel=StorageLevel(True, True, False, False, 2), awsAccessKeyId=None, awsSecretKey=None, decoder=<function utf8_decoder>, stsAssumeRoleArn=None, stsSessionName=None, stsExternalId=None)[source]\n",
    "Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis.\n",
    "\n",
    "Parameters:\n",
    "sscStreamingContext\n",
    "StreamingContext object\n",
    "\n",
    "kinesisAppNamestr\n",
    "Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDB\n",
    "\n",
    "streamNamestr\n",
    "Kinesis stream name\n",
    "\n",
    "endpointUrlstr\n",
    "Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)\n",
    "\n",
    "regionNamestr\n",
    "Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)\n",
    "\n",
    "initialPositionInStreamint\n",
    "In the absence of Kinesis checkpoint info, this is the worker’s initial starting position in the stream. The values are either the beginning of the stream per Kinesis’ limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).\n",
    "\n",
    "checkpointIntervalint\n",
    "Checkpoint interval(in seconds) for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.\n",
    "\n",
    "storageLevelpyspark.StorageLevel, optional\n",
    "Storage level to use for storing the received objects (default is StorageLevel.MEMORY_AND_DISK_2)\n",
    "\n",
    "awsAccessKeyIdstr, optional\n",
    "AWS AccessKeyId (default is None. If None, will use DefaultAWSCredentialsProviderChain)\n",
    "\n",
    "awsSecretKeystr, optional\n",
    "AWS SecretKey (default is None. If None, will use DefaultAWSCredentialsProviderChain)\n",
    "\n",
    "decoderfunction, optional\n",
    "A function used to decode value (default is utf8_decoder)\n",
    "\n",
    "stsAssumeRoleArnstr, optional\n",
    "ARN of IAM role to assume when using STS sessions to read from the Kinesis stream (default is None).\n",
    "\n",
    "stsSessionNamestr, optional\n",
    "Name to uniquely identify STS sessions used to read from Kinesis stream, if STS is being used (default is None).\n",
    "\n",
    "stsExternalIdstr, optional\n",
    "External ID that can be used to validate against the assumed IAM role’s trust policy, if STS is being used (default is None).\n",
    "\n",
    "Returns:\n",
    "A DStream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.fullOuterJoin\n",
    " DStream.fullOuterJoin(other, numPartitions=None)[source]\n",
    "# Return a new DStream by applying ‘full outer join’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.checkpoint\n",
    "DStream.checkpoint(interval)[source]\n",
    "# Enable periodic checkpointing of RDDs of this DStream\n",
    "\n",
    "# Parameters:\n",
    "# intervalint\n",
    "# time in seconds, after each period of that, generated RDD will be checkpointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.foreachBatch\n",
    "# DataStreamWriter.foreachBatch(func)[source]\n",
    "# Sets the output of the streaming query to be processed using the provided function. This is supported only the in the micro-batch execution modes (that is, when the trigger is not continuous). In every micro-batch, the provided function will be called in every micro-batch with (i) the output rows as a DataFrame and (ii) the batch identifier. The batchId can be used deduplicate and transactionally write the output (that is, the provided Dataset) to external systems. The output DataFrame is guaranteed to exactly same for the same batchId (assuming all operations are deterministic in the query).\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This API is evolving.\n",
    "def func(batch_df, batch_id):\n",
    "    batch_df.collect()\n",
    "\n",
    "writer = sdf.writeStream.foreachBatch(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.streaming.DStream.reduceByWindow\n",
    "DStream.reduceByWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration)[source]\n",
    "Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream.\n",
    "\n",
    "if invReduceFunc is not None, the reduction is done incrementally using the old window’s reduced value :\n",
    "\n",
    "reduce the new values that entered the window (e.g., adding new counts)\n",
    "\n",
    "2. “inverse reduce” the old values that left the window (e.g., subtracting old counts) This is more efficient than invReduceFunc is None.\n",
    "\n",
    "Parameters:\n",
    "reduceFuncfunction\n",
    "associative and commutative reduce function\n",
    "\n",
    "invReduceFuncfunction\n",
    "inverse reduce function of reduceFunc; such that for all y, and invertible x: invReduceFunc(reduceFunc(x, y), x) = y\n",
    "\n",
    "windowDurationint\n",
    "width of the window; must be a multiple of this DStream’s batching interval\n",
    "\n",
    "slideDurationint\n",
    "sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream’s batching interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.groupByKeyAndWindow\n",
    "# DStream.groupByKeyAndWindow(windowDuration, slideDuration, numPartitions=None)[source]\n",
    "# Return a new DStream by applying groupByKey over a sliding window. Similar to DStream.groupByKey(), but applies it over a sliding window.\n",
    "\n",
    "# Parameters:\n",
    "# windowDurationint\n",
    "# width of the window; must be a multiple of this DStream’s batching interval\n",
    "\n",
    "# slideDurationint\n",
    "# sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream’s batching interval\n",
    "\n",
    "# numPartitionsint, optional\n",
    "# Number of partitions of each RDD in the new DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.streaming.DataStreamWriter.start\n",
    "DataStreamWriter.start(path=None, format=None, outputMode=None, partitionBy=None, queryName=None, **options)[source]\n",
    "Streams the contents of the DataFrame to a data source.\n",
    "\n",
    "The data source is specified by the format and a set of options. If format is not specified, the default data source configured by spark.sql.sources.default will be used.\n",
    "\n",
    "New in version 2.0.0.\n",
    "\n",
    "Parameters:\n",
    "pathstr, optional\n",
    "the path in a Hadoop supported file system\n",
    "\n",
    "formatstr, optional\n",
    "the format used to save\n",
    "\n",
    "outputModestr, optional\n",
    "specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.\n",
    "\n",
    "append: Only the new rows in the streaming DataFrame/Dataset will be written to the sink\n",
    "\n",
    "complete: All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates\n",
    "\n",
    "update: only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to append mode.\n",
    "\n",
    "partitionBystr or list, optional\n",
    "names of partitioning columns\n",
    "\n",
    "queryNamestr, optional\n",
    "unique name for the query\n",
    "\n",
    "**optionsdict\n",
    "All other string options. You may want to provide a checkpointLocation for most streams, however it is not required for a memory stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.streaming.DataStreamWriter.trigger\n",
    "DataStreamWriter.trigger(*, processingTime=None, once=None, continuous=None)[source]\n",
    "Set the trigger for the stream query. If this is not set it will run the query as fast as possible, which is equivalent to setting the trigger to processingTime='0 seconds'.\n",
    "\n",
    "New in version 2.0.0.\n",
    "\n",
    "Parameters:\n",
    "processingTimestr, optional\n",
    "a processing time interval as a string, e.g. ‘5 seconds’, ‘1 minute’. Set a trigger that runs a microbatch query periodically based on the processing time. Only one trigger can be set.\n",
    "\n",
    "oncebool, optional\n",
    "if set to True, set a trigger that processes only one batch of data in a streaming query then terminates the query. Only one trigger can be set.\n",
    "\n",
    "continuousstr, optional\n",
    "a time interval as a string, e.g. ‘5 seconds’, ‘1 minute’. Set a trigger that runs a continuous query with a given checkpoint interval. Only one trigger can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.streaming.DataStreamWriter.start\n",
    "DataStreamWriter.start(path=None, format=None, outputMode=None, partitionBy=None, queryName=None, **options)[source]\n",
    "Streams the contents of the DataFrame to a data source.\n",
    "\n",
    "The data source is specified by the format and a set of options. If format is not specified, the default data source configured by spark.sql.sources.default will be used.\n",
    "\n",
    "New in version 2.0.0.\n",
    "\n",
    "Parameters:\n",
    "pathstr, optional\n",
    "the path in a Hadoop supported file system\n",
    "\n",
    "formatstr, optional\n",
    "the format used to save\n",
    "\n",
    "outputModestr, optional\n",
    "specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.\n",
    "\n",
    "append: Only the new rows in the streaming DataFrame/Dataset will be written to the sink\n",
    "\n",
    "complete: All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates\n",
    "\n",
    "update: only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to append mode.\n",
    "\n",
    "partitionBystr or list, optional\n",
    "names of partitioning columns\n",
    "\n",
    "queryNamestr, optional\n",
    "unique name for the query\n",
    "\n",
    "**optionsdict\n",
    "All other string options. You may want to provide a checkpointLocation for most streams, however it is not required for a memory stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.streaming.DataStreamReader.csv\n",
    "# DataStreamReader.csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, unescapedQuoteHandling=None)[source]\n",
    "# Loads a CSV file stream and returns the result as a DataFrame.\n",
    "\n",
    "# This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr or list\n",
    "# string, or list of strings, for input path(s).\n",
    "\n",
    "# schemapyspark.sql.types.StructType or str, optional\n",
    "# an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# sepstr, optional\n",
    "# sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,.\n",
    "\n",
    "# encodingstr, optional\n",
    "# decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8.\n",
    "\n",
    "# quotestr, optional sets a single character used for escaping quoted values where the\n",
    "# separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "\n",
    "# escapestr, optional\n",
    "# sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \\.\n",
    "\n",
    "# commentstr, optional\n",
    "# sets a single character used for skipping lines beginning with this character. By default (None), it is disabled.\n",
    "\n",
    "# headerstr or bool, optional\n",
    "# uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "\n",
    "# inferSchemastr or bool, optional\n",
    "# infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.\n",
    "\n",
    "# enforceSchemastr or bool, optional\n",
    "# If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files or the first header in RDD if the header option is set to true. Field names in the schema and column names in CSV headers are checked by their positions taking into account spark.sql.caseSensitive. If None is set, true is used by default. Though the default value is true, it is recommended to disable the enforceSchema option to avoid incorrect results.\n",
    "\n",
    "# ignoreLeadingWhiteSpacestr or bool, optional\n",
    "# a flag indicating whether or not leading whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "\n",
    "# ignoreTrailingWhiteSpacestr or bool, optional\n",
    "# a flag indicating whether or not trailing whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "\n",
    "# nullValuestr, optional\n",
    "# sets the string representation of a null value. If None is set, it uses the default value, empty string. Since 2.0.1, this nullValue param applies to all supported types including the string type.\n",
    "\n",
    "# nanValuestr, optional\n",
    "# sets the string representation of a non-number value. If None is set, it uses the default value, NaN.\n",
    "\n",
    "# positiveInfstr, optional\n",
    "# sets the string representation of a positive infinity value. If None is set, it uses the default value, Inf.\n",
    "\n",
    "# negativeInfstr, optional\n",
    "# sets the string representation of a negative infinity value. If None is set, it uses the default value, Inf.\n",
    "\n",
    "# dateFormatstr, optional\n",
    "# sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. # noqa This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "\n",
    "# timestampFormatstr, optional\n",
    "# sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. # noqa This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX].\n",
    "\n",
    "# maxColumnsstr or int, optional\n",
    "# defines a hard limit of how many columns a record can have. If None is set, it uses the default value, 20480.\n",
    "\n",
    "# maxCharsPerColumnstr or int, optional\n",
    "# defines the maximum number of characters allowed for any given value being read. If None is set, it uses the default value, -1 meaning unlimited length.\n",
    "\n",
    "# maxMalformedLogPerPartitionstr or int, optional\n",
    "# this parameter is no longer used since Spark 2.2.0. If specified, it is ignored.\n",
    "\n",
    "# modestr, optional\n",
    "# allows a mode for dealing with corrupt records during parsing. If None is set, it uses the default value, PERMISSIVE.\n",
    "\n",
    "# PERMISSIVE: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets null to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.\n",
    "\n",
    "# DROPMALFORMED: ignores the whole corrupted records.\n",
    "\n",
    "# FAILFAST: throws an exception when it meets corrupted records.\n",
    "\n",
    "# columnNameOfCorruptRecordstr, optional\n",
    "# allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.\n",
    "\n",
    "# multiLinestr or bool, optional\n",
    "# parse one record, which may span multiple lines. If None is set, it uses the default value, false.\n",
    "\n",
    "# charToEscapeQuoteEscapingstr, optional\n",
    "# sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \\0 otherwise.\n",
    "\n",
    "# emptyValuestr, optional\n",
    "# sets the string representation of an empty value. If None is set, it uses the default value, empty string.\n",
    "\n",
    "# localestr, optional\n",
    "# sets a locale as language tag in IETF BCP 47 format. If None is set, it uses the default value, en-US. For instance, locale is used while parsing dates and timestamps.\n",
    "\n",
    "# lineSepstr, optional\n",
    "# defines the line separator that should be used for parsing. If None is set, it covers all \\\\r, \\\\r\\\\n and \\\\n. Maximum length is 1 character.\n",
    "\n",
    "# pathGlobFilterstr or bool, optional\n",
    "# an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery. # noqa\n",
    "\n",
    "# recursiveFileLookupstr or bool, optional\n",
    "# recursively scan a directory for files. Using this option disables partition discovery. # noqa\n",
    "\n",
    "# unescapedQuoteHandlingstr, optional\n",
    "# defines how the CsvParser will handle values with unescaped quotes. If None is set, it uses the default value, STOP_AT_DELIMITER.\n",
    "\n",
    "# STOP_AT_CLOSING_QUOTE: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.\n",
    "\n",
    "# BACK_TO_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.\n",
    "\n",
    "# STOP_AT_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.\n",
    "\n",
    "# SKIP_VALUE: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.\n",
    "\n",
    "# RAISE_ERROR: If unescaped quotes are found in the input, a TextParsingException will be thrown.\n",
    "\n",
    "# .. versionadded:: 2.0.0\n",
    "# Notes\n",
    "\n",
    "# This API is evolving.\n",
    "\n",
    "# Examples\n",
    "\n",
    "\n",
    "csv_sdf = spark.readStream.csv(tempfile.mkdtemp(), schema = sdf_schema)\n",
    "csv_sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.awaitTermination\n",
    "# StreamingQuery.awaitTermination(timeout=None)[source]\n",
    "# Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds.\n",
    "\n",
    "# If the query has terminated, then all subsequent calls to this method will either return immediately (if the query was terminated by stop()), or throw the exception immediately (if the query has terminated with exception).\n",
    "\n",
    "# throws StreamingQueryException, if this query has terminated with an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.streaming.DataStreamWriter.options\n",
    "DataStreamWriter.options(**options)[source]\n",
    "Adds output options for the underlying data source.\n",
    "\n",
    "You can set the following option(s) for writing files:\n",
    "timeZone: sets the string that indicates a time zone ID to be used to format\n",
    "timestamps in the JSON/CSV datasources or partition values. The following formats of timeZone are supported:\n",
    "\n",
    "Region-based zone ID: It should have the form ‘area/city’, such as ‘America/Los_Angeles’.\n",
    "\n",
    "Zone offset: It should be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’.\n",
    "\n",
    "Other short names like ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.processAllAvailable\n",
    "# StreamingQuery.processAllAvailable()[source]\n",
    "# Blocks until all available data in the source has been processed and committed to the sink. This method is intended for testing.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# In the case of continually arriving data, this method may block forever. Additionally, this method is only guaranteed to block until data that has been synchronously appended data to a stream source prior to invocation. (i.e. getOffset must immediately reflect the addition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.stop\n",
    "# StreamingContext.stop(stopSparkContext=True, stopGraceFully=False)[source]\n",
    "# Stop the execution of the streams, with option of ensuring all received data has been processed.\n",
    "\n",
    "# Parameters:\n",
    "# stopSparkContextbool, optional\n",
    "# Stop the associated SparkContext or not\n",
    "\n",
    "# stopGracefullybool, optional\n",
    "# Stop gracefully by waiting for the processing of all received data to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.sparkContext¶\n",
    "# property StreamingContext.sparkContext\n",
    "# Return SparkContext which is associated with this StreamingContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.addStreamingListener\n",
    "# StreamingContext.addStreamingListener(streamingListener)[source]\n",
    "# Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for receiving system events related to streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.addStreamingListener\n",
    "# StreamingContext.addStreamingListener(streamingListener)[source]\n",
    "# Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for receiving system events related to streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.transform\n",
    "# StreamingContext.transform(dstreams, transformFunc)[source]\n",
    "# Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams. The order of the JavaRDDs in the transform function parameter will be the same as the order of corresponding DStreams in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.union\n",
    "StreamingContext.union(*dstreams)[source]\n",
    "# Create a unified DStream from multiple DStreams of the same type and same slide duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.awaitTerminationOrTimeout\n",
    "StreamingContext.awaitTerminationOrTimeout(timeout)[source]\n",
    "# Wait for the execution to stop. Return true if it’s stopped; or throw the reported error during the execution; or false if the waiting time elapsed before returning from the method.\n",
    "\n",
    "# Parameters:\n",
    "# timeoutint\n",
    "# time to wait in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.remember\n",
    "# StreamingContext.remember(duration)[source]\n",
    "# Set each DStreams in this context to remember RDDs it generated in the last given duration. DStreams remember RDDs only for a limited duration of time and releases them for garbage collection. This method allows the developer to specify how long to remember the RDDs (if the developer wishes to query old data outside the DStream computation).\n",
    "\n",
    "# Parameters:\n",
    "# durationint\n",
    "# Minimum duration (in seconds) that each DStream should remember its RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.filter\n",
    "DStream.filter(f)[source]\n",
    "# Return a new DStream containing only the elements that satisfy predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.countByWindow\n",
    " DStream.countByWindow(windowDuration, slideDuration)[source]\n",
    "# Return a new DStream in which each RDD has a single element generated by counting the number of elements in a window over this DStream. windowDuration and slideDuration are as defined in the window() operation.\n",
    "\n",
    "# This is equivalent to window(windowDuration, slideDuration).count(), but will be more efficient if window is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.context\n",
    " DStream.context()[source]\n",
    "# Return the StreamingContext associated with this DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.repartition\n",
    "DStream.repartition(numPartitions)[source]\n",
    "# Return a new DStream with an increased or decreased level of parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.kinesis.InitialPositionInStream.LATEST\n",
    "# InitialPositionInStream.LATEST = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.streaming.kinesis.InitialPositionInStream.TRIM_HORIZON\n",
    "InitialPositionInStream.TRIM_HORIZON = 1¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.glom\n",
    "DStream.glom()[source]\n",
    "# Return a new DStream in which RDD is generated by applying glom() to RDD of this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.reduce¶\n",
    "DStream.reduce(func)[source]\n",
    "# Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.reduceByKey\n",
    "DStream.reduceByKey(func, numPartitions=None)[source]\n",
    "# Return a new DStream by applying reduceByKey to each RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.leftOuterJoin¶\n",
    "DStream.leftOuterJoin(other, numPartitions=None)[source]\n",
    "# Return a new DStream by applying ‘left outer join’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.persist\n",
    "DStream.persist(storageLevel)[source]\n",
    "# Persist the RDDs of this DStream with the given storage level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.map¶\n",
    " DStream.map(f, preservesPartitioning=False)[source]\n",
    "# Return a new DStream by applying a function to each element of DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.mapPartitions\n",
    "# DStream.mapPartitions(f, preservesPartitioning=False)[source]\n",
    "# Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.partitionBy\n",
    " DStream.partitionBy(numPartitions, partitionFunc=<function portable_hash>)[source]\n",
    "# Return a copy of the DStream in which each RDD are partitioned using the specified partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.countByValue\n",
    "DStream.countByValue()[source]\n",
    "# Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamReader.load\n",
    "# DataStreamReader.load(path=None, format=None, schema=None, **options)[source]¶\n",
    "json_sdf = spark.readStream.format(\"json\") \\\n",
    "    .schema(sdf_schema) \\\n",
    "    .load(tempfile.mkdtemp())\n",
    "json_sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQueryManager.active\n",
    "# property StreamingQueryManager.active\n",
    "# Returns a list of active queries associated with this SQLContext\n",
    "\n",
    "sq = sdf.writeStream.format('memory').queryName('this_query').start()\n",
    "sqm = spark.streams\n",
    "# get the list of active streaming queries\n",
    "[q.name for q in sqm.active]\n",
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.isActive¶\n",
    "property StreamingQuery.isActive\n",
    "# Whether this streaming query is currently active or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.lastProgress\n",
    "property StreamingQuery.lastProgress\n",
    "# Returns the most recent StreamingQueryProgress update of this streaming query or None if there were no progress updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.status\n",
    "property StreamingQuery.status\n",
    "# Returns the current status of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.recentProgress\n",
    "property StreamingQuery.recentProgress\n",
    "# Returns an array of the most recent [[StreamingQueryProgress]] updates for this query. The number of progress updates retained for each stream is configured by Spark session configuration spark.sql.streaming.numRecentProgressUpdates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.count\n",
    "DStream.count()[source]\n",
    "# Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.updateStateByKey¶\n",
    "DStream.updateStateByKey(updateFunc, numPartitions=None, initialRDD=None)[source]\n",
    "# Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key.\n",
    "\n",
    "# Parameters:\n",
    "# updateFuncfunction\n",
    "# State update function. If this function returns None, then corresponding state key-value pair will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.name\n",
    "property StreamingQuery.name\n",
    "# Returns the user-specified name of the query, or null if not specified. This name can be specified in the org.apache.spark.sql.streaming.DataStreamWriter as dataframe.writeStream.queryName(“query”).start(). This name, if set, must be unique across all active queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.rightOuterJoin\n",
    "DStream.rightOuterJoin(other, numPartitions=None)[source]\n",
    "# Return a new DStream by applying ‘right outer join’ between RDDs of this DStream and other DStream.\n",
    "\n",
    "# Hash partitioning is used to generate the RDDs with numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.streaming.DStream.cache\n",
    "DStream.cache()[source]\n",
    "#Persist the RDDs of this DStream with the default storage level (MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.id\n",
    "property StreamingQuery.id\n",
    "# Returns the unique id of this query that persists across restarts from checkpoint data. That is, this id is generated when a query is started for the first time, and will be the same every time it is restarted from checkpoint data. There can only be one query with the same id active in a Spark cluster. Also see, runId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.streaming.StreamingContext.checkpoint\n",
    "StreamingContext.checkpoint(directory)[source]\n",
    "Sets the context to periodically checkpoint the DStream operations for master fault-tolerance. The graph will be checkpointed every batch interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.streaming.StreamingContext.start\n",
    "StreamingContext.start()[source]\n",
    "#Start the execution of the streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.getOrCreate\n",
    "classmethod StreamingContext.getOrCreate(checkpointPath, setupFunc)[source]\n",
    "# Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. If checkpoint data exists in the provided checkpointPath, then StreamingContext will be recreated from the checkpoint data. If the data does not exist, then the provided setupFunc will be used to create a new context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.awaitTermination\n",
    "# StreamingContext.awaitTermination(timeout=None)[source]\n",
    "# Wait for the execution to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.getActive\n",
    "# classmethod StreamingContext.getActive()[source]\n",
    "# Return either the currently active StreamingContext (i.e., if there is a context started but not stopped) or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.mapValues\n",
    "DStream.mapValues(f)[source]\n",
    "# Return a new DStream by applying a map function to the value of each key-value pairs in this DStream without changing the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.groupByKey\n",
    " DStream.groupByKey(numPartitions=None)[source]\n",
    "# Return a new DStream by applying groupByKey on each RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.StreamingContext.getActiveOrCreate¶\n",
    "# classmethod StreamingContext.getActiveOrCreate(checkpointPath, setupFunc)[source]\n",
    "# Either return the active StreamingContext (i.e. currently started but not stopped), or recreate a StreamingContext from checkpoint data or create a new StreamingContext using the provided setupFunc function. If the checkpointPath is None or does not contain valid checkpoint data, then setupFunc will be called to create a new context and setup DStreams.\n",
    "\n",
    "# Parameters:\n",
    "# checkpointPathstr\n",
    "# Checkpoint directory used in an earlier streaming program. Can be None if the intention is to always create a new context when there is no active context.\n",
    "\n",
    "# setupFuncfunction\n",
    "# Function to create a new JavaStreamingContext and setup DStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.stop\n",
    "StreamingQuery.stop()[source]\n",
    "Stop this streaming query.\n",
    "\n",
    "New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.format\n",
    "# DataStreamWriter.format(source)[source]\n",
    "# Specifies the underlying output data source.\n",
    "writer = sdf.writeStream.format('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.runId\n",
    "property StreamingQuery.runId\n",
    "Returns the unique id of this query that does not persist across restarts. That is, every query that is started (or restarted from checkpoint) will have a different runId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQueryManager.get\n",
    "# StreamingQueryManager.get(id)[source]\n",
    "# Returns an active query from this SQLContext or throws exception if an active query with this name doesn’t exist.\n",
    "sq = sdf.writeStream.format('memory').queryName('this_query').start()\n",
    "sq.name\n",
    "sq = spark.streams.get(sq.id)\n",
    "sq.isActive\n",
    "sq = sqlContext.streams.get(sq.id)\n",
    "sq.isActive\n",
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamingQuery.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.exception\n",
    "StreamingQuery.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.foreachRDD\n",
    "DStream.foreachRDD(func)[source]\n",
    "# Apply a function to each RDD in this DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.transformWith\n",
    "DStream.transformWith(func, other, keepSerializer=False)[source]\n",
    "# Return a new DStream in which each RDD is generated by applying a function on each RDD of this DStream and ‘other’ DStream.\n",
    "\n",
    "# func can have two arguments of (rdd_a, rdd_b) or have three arguments of (time, rdd_a, rdd_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.transformWith\n",
    "# DStream.transformWith(func, other, keepSerializer=False)[source]\n",
    "# Return a new DStream in which each RDD is generated by applying a function on each RDD of this DStream and ‘other’ DStream.\n",
    "\n",
    "# func can have two arguments of (rdd_a, rdd_b) or have three arguments of (time, rdd_a, rdd_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.union\n",
    "DStream.union(other)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.streaming.DStream.transform\n",
    "DStream.transform(func)[source]\n",
    "\n",
    "# Return a new DStream in which each RDD is generated by applying a function on each RDD of this DStream.\n",
    "\n",
    "# func can have one argument of rdd, or have two arguments of (time, rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.streaming.DStream.slice\n",
    "# Return all the RDDs between ‘begin’ to ‘end’ (both included)\n",
    "# begin, end could be datetime.datetime() or unix_timestamp\n",
    "DStream.slice(begin, end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Columns Api\n",
    "# Column.alias(*alias, **kwargs)[source]\n",
    "df.select(df.age.alias(\"age2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.asc_nulls_first()\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
    "df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.asc_nulls_last\n",
    "df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.rlike(other)¶\n",
    "df.filter(df.name.rlike('ice$')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.getField(name)[source]¶\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
    "df.select(df.r.getField(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.desc()¶\n",
    "df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
    "df.select(df.name).orderBy(df.name.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.startswith(other)\n",
    "df.filter(df.name.startswith('Al')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.cast(dataType)[source]¶\n",
    "df.select(df.age.cast(\"string\").alias('ages')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.Column.between\n",
    "# Column.between(lowerBound, upperBound)\n",
    "df.select(df.name, df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.isin(*cols)[source]\n",
    "# A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n",
    "df[df.name.isin(\"Bob\", \"Mike\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Column.dropFields(*fieldNames)[source]\n",
    "# An expression that drops fields in StructType by name. This is a no-op if schema doesn’t contain field name(s).\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
    "df.withColumn('a', df['a'].dropFields('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.eqNullSafe(other)\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(id=1, value='foo'),\n",
    "    Row(id=2, value=None)\n",
    "])\n",
    "df1.select(\n",
    "    df1['value'] == 'foo',\n",
    "    df1['value'].eqNullSafe('foo'),\n",
    "    df1['value'].eqNullSafe(None)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.isin(*cols)\n",
    "df[df.name.isin(\"Bob\", \"Mike\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.withField(fieldName, col)\n",
    "df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
    "df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.createTempView(name)\n",
    "df.createOrReplaceTempView(\"df1\")\n",
    "df2=spark.sql(\"select * from people\")\n",
    "sorted(df.collect())=sorted(df2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array(*cols)\n",
    "df.select(array('age', 'age').alias(\"arr\")\n",
    "df.select(array([df.age, df.age]).alias(\"arr\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.select(*cols)[source]\n",
    "df.select('*').collect()\n",
    "df.select('name','age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.expm1\n",
    "# pyspark.sql.functions.expm1(col)[source]\n",
    "# Computes the exponential of the given value minus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.format_number\n",
    "# pyspark.sql.functions.format_number(col, d)[source]\n",
    "# Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# the column name of the numeric value to be formatted\n",
    "\n",
    "# dint\n",
    "# the N decimal places\n",
    "\n",
    "# >>> spark.createDataFrame([(5,)], [‘a’]).select(format_number(‘a’, 4).alias(‘v’)).collect()\n",
    "# [Row(v=’5.0000’)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.cosh\n",
    "# pyspark.sql.functions.cosh(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# hyperbolic angle\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# hyperbolic cosine of the angle, as if computed by java.lang.Math.cosh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.forall(col, f)[source]\n",
    "# Returns whether a predicate holds for every element in the array.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# (x: Column) -> Column: ... returning the Boolean expression. Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.date_trunc\n",
    "# pyspark.sql.functions.date_trunc(format, timestamp)[source]\n",
    "# Returns timestamp truncated to the unit specified by the format.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# formatstr\n",
    "# ‘year’, ‘yyyy’, ‘yy’ to truncate by year, ‘month’, ‘mon’, ‘mm’ to truncate by month, ‘day’, ‘dd’ to truncate by day, Other options are: ‘microsecond’, ‘millisecond’, ‘second’, ‘minute’, ‘hour’, ‘week’, ‘quarter’\n",
    "\n",
    "# timestampColumn or str\n",
    "df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
    "df.select(date_trunc('year', df.t).alias('year')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameNaFunctions\n",
    "# class pyspark.sql.DataFrameNaFunctions(df)[source]\n",
    "# Functionality for working with missing data in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.withColumn\n",
    "# DataFrame.withColumn(colName, col)[source]\n",
    "# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "# The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colNamestr\n",
    "# string, name of the new column.\n",
    "\n",
    "# colColumn\n",
    "# a Column expression for the new column.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This method introduces a projection internally. Therefore, calling it multiple times, for instance, via loops in order to add multiple columns can generate big plans which can cause performance issues and even StackOverflowException. To avoid this, use select() with the multiple columns at once.\n",
    "df.withColumn('age2', df.age + 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window.partitionBy\n",
    "# static Window.partitionBy(*cols)[source]\n",
    "# Creates a WindowSpec with the partitioning defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.cos\n",
    "# pyspark.sql.functions.cos(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in radians\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# cosine of the angle, as if computed by java.lang.Math.cos()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.config\n",
    "# builder.config(key=None, value=None, conf=None)\n",
    "# Sets a config option. Options set using this method are automatically propagated to both SparkConf and SparkSession’s own configuration.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# keystr, optional\n",
    "# a key name string for configuration property\n",
    "\n",
    "# valuestr, optional\n",
    "# a value for configuration property\n",
    "\n",
    "# confSparkConf, optional\n",
    "# an instance of SparkConf\n",
    "\n",
    "# Examples\n",
    "\n",
    "# For an existing SparkConf, use conf parameter.\n",
    "\n",
    "# >>>\n",
    "from pyspark.conf import SparkConf\n",
    "SparkSession.builder.config(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.locate\n",
    "# pyspark.sql.functions.locate(substr, str, pos=1)[source]\n",
    "# Locate the position of the first occurrence of substr in a string column, after position pos.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# substrstr\n",
    "# a string\n",
    "\n",
    "# strColumn or str\n",
    "# a Column of pyspark.sql.types.StringType\n",
    "\n",
    "# posint, optional\n",
    "# start position (zero based)\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str.\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(locate('b', df.s, 1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.log1p\n",
    "# pyspark.sql.functions.log1p(col)[source]\n",
    "# Computes the natural logarithm of the given value plus one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.lower\n",
    "# pyspark.sql.functions.lower(col)[source]\n",
    "# Converts a string expression to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.lpad\n",
    "# pyspark.sql.functions.lpad(col, len, pad)[source]\n",
    "# Left-pad the string column to width len with pad.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "# df.select(lpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.explode_outer\n",
    "# pyspark.sql.functions.explode_outer(col)[source]\n",
    "# Returns a new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# New in version 2.3.0\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.days\n",
    "# pyspark.sql.functions.days(col)[source]\n",
    "# Partition transform function: A transform for timestamps and dates to partition data into days.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "\n",
    "# Examples\n",
    "df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    days(\"ts\")\n",
    ").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.levenshtein\n",
    "# pyspark.sql.functions.levenshtein(left, right)[source]\n",
    "# Computes the Levenshtein distance of the two given strings.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
    "# df0.select(levenshtein('l', 'r').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.functions.from_csv\n",
    "pyspark.sql.functions.from_csv(col, schema, options={})[source]\n",
    "Parses a column containing a CSV string to a row with the specified schema. Returns null, in the case of an unparseable string.\n",
    "\n",
    "New in version 3.0.0.\n",
    "\n",
    "Parameters:\n",
    "colColumn or str\n",
    "string column in CSV format\n",
    "\n",
    "schema :class:`~pyspark.sql.Column` or str\n",
    "a string with schema in DDL format to use when parsing the CSV column.\n",
    "\n",
    "optionsdict, optional\n",
    "options to control parsing. accepts the same options as the CSV datasource\n",
    "\n",
    "Examples\n",
    "\n",
    ">>>\n",
    "data = [(\"1,2,3\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
    "[Row(csv=Row(a=1, b=2, c=3))]\n",
    "value = data[0][0]\n",
    "df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
    "[Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
    "data = [(\"   abc\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "options = {'ignoreLeadingWhiteSpace': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.readStream\n",
    "# property SparkSession.readStream\n",
    "# Returns a DataStreamReader that can be used to read data streams as a streaming DataFrame.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# DataStreamReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # pyspark.sql.functions.lit\n",
    "# pyspark.sql.functions.lit(col)[source]\n",
    "# Creates a Column of literal value.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Examples\n",
    "df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_entries\n",
    "# pyspark.sql.functions.map_entries(col)[source]\n",
    "# Collection function: Returns an unordered array of all entries in the given map.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "from pyspark.sql.functions import map_entries\n",
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.select(map_entries(\"data\").alias(\"entries\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_concat\n",
    "# pyspark.sql.functions.map_concat(*cols)[source]\n",
    "# Returns the union of all the given maps.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsColumn or str\n",
    "# column names or Columns\n",
    "from pyspark.sql.functions import map_concat\n",
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
    "df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.mode\n",
    "# DataFrameWriter.mode(saveMode)[source]\n",
    "# Specifies the behavior when data or table already exists.\n",
    "\n",
    "# Options include:\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# error or errorifexists: Throw an exception if data already exists.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.substring_index\n",
    "# pyspark.sql.functions.substring_index(str, delim, count)[source]\n",
    "# Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
    "df.select(substring_index(df.s, '.', 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.year\n",
    "# pyspark.sql.functions.year(col)[source]\n",
    "# Extract the year of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(year('dt').alias('year')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_entries\n",
    "# pyspark.sql.functions.map_entries(col)[source]\n",
    "# Collection function: Returns an unordered array of all entries in the given map.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql.functions import map_entries\n",
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.select(map_entries(\"data\").alias(\"entries\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.sortBy\n",
    "# DataFrameWriter.sortBy(col, *cols)[source]\n",
    "# Sorts the output in each bucket by the given columns on the file system.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colstr, tuple or list\n",
    "# a name of a column, or a list of names.\n",
    "\n",
    "# colsstr\n",
    "# additional names (optional). If col is a list it should be empty.\n",
    "\n",
    "# Examples\n",
    "\n",
    ">>>\n",
    "(df.write.format('parquet')  \n",
    "    .bucketBy(100, 'year', 'month')\n",
    "    .sortBy('day')\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable('sorted_bucketed_table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_zip_with\n",
    "# pyspark.sql.functions.map_zip_with(col1, col2, f)[source]\n",
    "# Merge two given maps, key-wise into a single map using a function.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1Column or str\n",
    "# name of the first column or expression\n",
    "\n",
    "# col2Column or str\n",
    "# name of the second column or expression\n",
    "\n",
    "# ffunction\n",
    "# a ternary function (k: Column, v1: Column, v2: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df = spark.createDataFrame([\n",
    "    (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
    "    (\"id\", \"base\", \"ratio\")\n",
    ")\n",
    "df.select(map_zip_with(\n",
    "    \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
    "# ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_from_entries\n",
    "# pyspark.sql.functions.map_from_entries(col)[source]\n",
    "# Collection function: Returns a map created from the given array of entries.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql.functions import map_from_entries\n",
    "df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
    "df.select(map_from_entries(\"data\").alias(\"map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_filter\n",
    "# pyspark.sql.functions.map_filter(col, f)[source]\n",
    "# Returns a map whose key-value pairs satisfy a predicate.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# Examples\n",
    "df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
    "df.select(map_filter(\n",
    "    \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.version\n",
    "# property SparkSession.version\n",
    "# The version of Spark on which this application is running.\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.zip_with\n",
    "# pyspark.sql.functions.zip_with(left, right, f)[source]\n",
    "# Merge two given arrays, element-wise, into a single array using a function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying the function.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# leftColumn or str\n",
    "# name of the first column or expression\n",
    "\n",
    "# rightColumn or str\n",
    "# name of the second column or expression\n",
    "\n",
    "# ffunction\n",
    "# a binary function (x1: Column, x2: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
    "df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.text\n",
    "# DataFrameWriter.text(path, compression=None, lineSep=None)[source]\n",
    "# Saves the content of the DataFrame in a text file at the specified path. The text files will be encoded as UTF-8.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr\n",
    "# the path in any Hadoop supported file system\n",
    "\n",
    "# compressionstr, optional\n",
    "# compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\n",
    "\n",
    "# lineSepstr, optional\n",
    "# defines the line separator that should be used for writing. If None is set, it uses the default value, \\n.\n",
    "\n",
    "# The DataFrame must have only one column that is of string type.\n",
    "# Each row becomes a new line in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.union\n",
    "# DataFrame.union(other)[source]\n",
    "# Return a new DataFrame containing union of rows in this and another DataFrame.\n",
    "\n",
    "# This is equivalent to UNION ALL in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by distinct().\n",
    "\n",
    "# Also as standard in SQL, this function resolves columns by position (not by name).\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.unionAll\n",
    "# DataFrame.unionAll(other)[source]\n",
    "# Return a new DataFrame containing union of rows in this and another DataFrame.\n",
    "\n",
    "# This is equivalent to UNION ALL in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by distinct().\n",
    "\n",
    "# Also as standard in SQL, this function resolves columns by position (not by name).\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.length\n",
    "# pyspark.sql.functions.length(col)[source]\n",
    "# Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_keys\n",
    "# pyspark.sql.functions.map_keys(col)[source]\n",
    "# Collection function: Returns an unordered array containing the keys of the map.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import map_keys\n",
    "# df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "# df.select(map_keys(\"data\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.structpyspark.sql.functions.soundex\n",
    "# pyspark.sql.functions.soundex(col)[source]\n",
    "# Returns the SoundEx encoding for a string\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
    "df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
    "# pyspark.sql.functions.struct(*cols)[source]\n",
    "# Creates a new struct column.\n",
    "\n",
    "# # New in version 1.4.0.\n",
    "\n",
    "# # Parameters:\n",
    "# # colslist, set, str or Column\n",
    "# # column names or Columns to contain in the output struct.\n",
    "# df.select(struct('age', 'name').alias(\"struct\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.DataFrameStatFunctions.sampleBy\n",
    "# DataFrameStatFunctions.sampleBy(col, fractions, seed=None)[source]\n",
    "# Returns a stratified sample without replacement based on the fraction given on each stratum.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# column that defines strata\n",
    "\n",
    "# Changed in version 3.0: Added sampling by a column of Column\n",
    "\n",
    "# fractionsdict\n",
    "# sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n",
    "\n",
    "# seedint, optional\n",
    "# random seed\n",
    "\n",
    "# Returns:\n",
    "# a new DataFrame that represents the stratified sample\n",
    "from pyspark.sql.functions import col\n",
    "dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.years\n",
    "# pyspark.sql.functions.years(col)[source]\n",
    "# Partition transform function: A transform for timestamps and dates to partition data into years.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "#     years(\"ts\")\n",
    "# ).createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.from_unixtime\n",
    "# pyspark.sql.functions.from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')[source]\n",
    "# Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "# time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
    "# time_df.select(from_unixtime('unix_time').alias('ts')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.jdbc\n",
    "# DataFrameWriter.jdbc(url, table, mode=None, properties=None)[source]\n",
    "# Saves the content of the DataFrame to an external database table via JDBC.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# urlstr\n",
    "# a JDBC URL of the form jdbc:subprotocol:subname\n",
    "\n",
    "# tablestr\n",
    "# Name of the table in the external database.\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "# propertiesdict\n",
    "# a dictionary of JDBC database connection arguments. Normally at least properties “user” and “password” with their corresponding values. For example { ‘user’ : ‘SYSTEM’, ‘password’ : ‘mypassword’ }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window.unboundedFollowing\n",
    "# Window.unboundedFollowing = 9223372036854775807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window.unboundedPreceding\n",
    "# Window.unboundedPreceding = -9223372036854775808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.WindowSpec.orderBy\n",
    "# WindowSpec.orderBy(*cols)[source]\n",
    "# Defines the ordering columns in a WindowSpec.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, Column or list\n",
    "# names of columns or expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.WindowSpec.partitionBy\n",
    "# WindowSpec.partitionBy(*cols)[source]\n",
    "# Defines the partitioning columns in a WindowSpec.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, Column or list\n",
    "# names of columns or expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_values\n",
    "# pyspark.sql.functions.map_values(col)[source]\n",
    "# Collection function: Returns an unordered array containing the values of the map.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import map_values\n",
    "# df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "# df.select(map_values(\"data\").alias(\"values\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.xxhash64\n",
    "# pyspark.sql.functions.xxhash64(*cols)[source]\n",
    "# Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm, and returns the result as a long column.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# spark.createDataFrame([('ABC',)], ['a']).select(xxhash64('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sumDistinct\n",
    "# pyspark.sql.functions.sumDistinct(col)[source]\n",
    "# Aggregate function: returns the sum of distinct values in the expression.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.decode\n",
    "# pyspark.sql.functions.decode(col, charset)[source]\n",
    "# Computes the first argument into a string from a binary using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.degrees\n",
    "# pyspark.sql.functions.degrees(col)[source]\n",
    "# Converts an angle measured in radians to an approximately equivalent angle measured in degrees.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in radians\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# angle in degrees, as if computed by java.lang.Math.toDegrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.substring\n",
    "# pyspark.sql.functions.substring(str, pos, len)[source]\n",
    "# Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(substring(df.s, 1, 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sum\n",
    "# pyspark.sql.functions.sum(col)[source]\n",
    "# Aggregate function: returns the sum of all values in the expression.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.dayofyear\n",
    "# pyspark.sql.functions.dayofyear(col)[source]\n",
    "# Extract the day of the year of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofyear('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.dayofweek\n",
    "# pyspark.sql.functions.dayofweek(col)[source]\n",
    "# Extract the day of the week of a given date as integer.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofweek('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.dayofmonth\n",
    "# pyspark.sql.functions.dayofmonth(col)[source]\n",
    "# Extract the day of the month of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofmonth('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.datediff\n",
    "# pyspark.sql.functions.datediff(end, start)[source]\n",
    "# Returns the number of days from start to end.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "df.select(datediff(df.d2, df.d1).alias('diff')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.log\n",
    "# pyspark.sql.functions.log(arg1, arg2=None)[source]\n",
    "# Returns the first argument-based logarithm of the second argument.\n",
    "\n",
    "# If there is only one argument, then this takes the natural logarithm of the argument.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
    "# ['0.30102', '0.69897']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.log10\n",
    "# pyspark.sql.functions.log10(col)[source]\n",
    "# Computes the logarithm of the given value in Base 10.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.log2\n",
    "# pyspark.sql.functions.log2(col)[source]\n",
    "# Returns the base-2 logarithm of the argument.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.ltrim\n",
    "# pyspark.sql.functions.ltrim(col)[source]\n",
    "# Trim the spaces from left end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameStatFunctions.crosstab\n",
    "# DataFrameStatFunctions.crosstab(col1, col2)[source]\n",
    "# Computes a pair-wise frequency table of the given columns. Also known as a contingency table. The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned. The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2. The name of the first column will be col1_\n",
    "# col2. Pairs that have no occurrences will have zero as their counts. DataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1str\n",
    "# The name of the first column. Distinct items will make the first item of each row.\n",
    "\n",
    "# col2str\n",
    "# The name of the second column. Distinct items will make the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.corr\n",
    "# pyspark.sql.functions.corr(col1, col2)[source]\n",
    "# Returns a new Column for the Pearson Correlation Coefficient for col1 and col2.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    ">>>\n",
    "a = range(20)\n",
    "b = [2 * x for x in range(20)]\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(corr(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Row\n",
    "# class pyspark.sql.Row[source]\n",
    "# A row in DataFrame. The fields in it can be accessed:\n",
    "\n",
    "# like attributes (row.key)\n",
    "\n",
    "# like dictionary values (row[key])\n",
    "\n",
    "# key in row will search through row keys.\n",
    "\n",
    "# Row can be used to create a row object by using named arguments. It is not allowed to omit a named argument to represent that the value is None or missing. This should be explicitly set to None in this case.\n",
    "\n",
    "# Changed in version 3.0.0: Rows created from named arguments no longer have field names sorted alphabetically and will be ordered in the position as entered.\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameStatFunctions.cov\n",
    "# DataFrameStatFunctions.cov(col1, col2)[source]\n",
    "# Calculate the sample covariance for the given columns, specified by their names, as a double value. DataFrame.cov() and DataFrameStatFunctions.cov() are aliases.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1str\n",
    "# The name of the first column\n",
    "\n",
    "# col2str\n",
    "# The name of the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window.rangeBetween\n",
    "# static Window.rangeBetween(start, end)[source]\n",
    "# Creates a WindowSpec with the frame boundaries defined, from start (inclusive) to end (inclusive).\n",
    "\n",
    "# Both start and end are relative from the current row. For example, “0” means “current row”, while “-1” means one off before the current row, and “5” means the five off after the current row.\n",
    "\n",
    "# We recommend users use Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "# A range-based boundary is based on the actual value of the ORDER BY expression(s). An offset is used to alter the value of the ORDER BY expression, for instance if the current ORDER BY expression has a value of 10 and the lower bound offset is -3, the resulting lower bound for the current row will be 10 - 3 = 7. This however puts a number of constraints on the ORDER BY expressions: there can be only one expression and this expression must have a numerical data type. An exception can be made when the offset is unbounded, because no value modification is needed, in this case multiple and non-numeric ORDER BY expression are allowed.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# startint\n",
    "# boundary start, inclusive. The frame is unbounded if this is Window.unboundedPreceding, or any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
    "\n",
    "# endint\n",
    "# boundary end, inclusive. The frame is unbounded if this is Window.unboundedFollowing, or any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\n",
    "df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\n",
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window\n",
    "# class pyspark.sql.Window[source]\n",
    "# Utility functions for defining window in DataFrames.\n",
    "\n",
    "# New in version 1.4.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# When ordering is not defined, an unbounded window frame (rowFrame, unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined, a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
    "\n",
    "# Examples\n",
    "window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.orc\n",
    "# DataFrameWriter.orc(path, mode=None, partitionBy=None, compression=None)[source]\n",
    "# Saves the content of the DataFrame in ORC format at the specified path.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr\n",
    "# the path in any Hadoop supported file system\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "# partitionBystr or list, optional\n",
    "# names of partitioning columns\n",
    "\n",
    "# compressionstr, optional\n",
    "# compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, snappy, zlib, and lzo). This will override orc.compress and spark.sql.orc.compression.codec. If None is set, it uses the value specified in spark.sql.orc.compression.codec.\n",
    "\n",
    "# Examples\n",
    "\n",
    ">>>\n",
    "orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
    "orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.parquet¶\n",
    "# DataFrameWriter.parquet(path, mode=None, partitionBy=None, compression=None)[source]\n",
    "# Saves the content of the DataFrame in Parquet format at the specified path.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr\n",
    "# the path in any Hadoop supported file system\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "# partitionBystr or list, optional\n",
    "# names of partitioning columns\n",
    "\n",
    "# compressionstr, optional\n",
    "# compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, uncompressed, snappy, gzip, lzo, brotli, lz4, and zstd). This will override spark.sql.parquet.compression.codec. If None is set, it uses the value specified in spark.sql.parquet.compression.codec.\n",
    "\n",
    "# Examples\n",
    "\n",
    ">>>\n",
    "df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.from_utc_timestamp\n",
    "# pyspark.sql.functions.from_utc_timestamp(timestamp, tz)[source]\n",
    "# This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.\n",
    "\n",
    "# However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to the given timezone.\n",
    "\n",
    "# This function may return confusing result if the input is a string with timezone, e.g. ‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# timestampColumn or str\n",
    "# the column that contains timestamps\n",
    "\n",
    "# tzColumn or str\n",
    "# A string detailing the time zone ID that the input should be adjusted to. It should be in the format of either region-based zone IDs or zone offsets. Region IDs must have the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’. Other short names are not recommended to use because they can be ambiguous.\n",
    "\n",
    "# Changed in version 2.4: tz can take a Column containing timezone ID strings.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
    "# df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.dense_rank\n",
    "# pyspark.sql.functions.dense_rank()[source]\n",
    "# Window function: returns the rank of rows within a window partition, without any gaps.\n",
    "\n",
    "# The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "# This is equivalent to the DENSE_RANK function in SQL.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.unpersist¶\n",
    "# DataFrame.unpersist(blocking=False)[source]\n",
    "# Marks the DataFrame as non-persistent, and remove all blocks for it from memory and disk.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# blocking default has changed to False to match Scala in 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.withWatermark\n",
    "# DataFrame.withWatermark(eventTime, delayThreshold)[source]\n",
    "# Defines an event time watermark for this DataFrame. A watermark tracks a point in time before which we assume no more late data is going to arrive.\n",
    "\n",
    "# Spark will use this watermark for several purposes:\n",
    "# To know when a given time window aggregation can be finalized and thus can be emitted when using output modes that do not allow updates.\n",
    "\n",
    "# To minimize the amount of state that we need to keep for on-going aggregations.\n",
    "\n",
    "# The current watermark is computed by looking at the MAX(eventTime) seen across all of the partitions in the query minus a user specified delayThreshold. Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. In some cases we may still process records that arrive more than delayThreshold late.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# eventTimestr\n",
    "# the name of the column that contains the event time of the row.\n",
    "\n",
    "# delayThresholdstr\n",
    "# the minimum delay to wait to data to arrive late, relative to the latest record that has been processed in the form of an interval (e.g. “1 minute” or “5 hours”).\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This API is evolving.\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import timestamp_seconds\n",
    "# sdf.select(\n",
    "#    'name',\n",
    "#    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
    "df1 = spark.range(10)\n",
    "df2 = spark.range(10)\n",
    "df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.withWatermark\n",
    "# DataFrame.withWatermark(eventTime, delayThreshold)[source]\n",
    "# Defines an event time watermark for this DataFrame. A watermark tracks a point in time before which we assume no more late data is going to arrive.\n",
    "\n",
    "# Spark will use this watermark for several purposes:\n",
    "# To know when a given time window aggregation can be finalized and thus can be emitted when using output modes that do not allow updates.\n",
    "\n",
    "# To minimize the amount of state that we need to keep for on-going aggregations.\n",
    "\n",
    "# The current watermark is computed by looking at the MAX(eventTime) seen across all of the partitions in the query minus a user specified delayThreshold. Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. In some cases we may still process records that arrive more than delayThreshold late.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# eventTimestr\n",
    "# the name of the column that contains the event time of the row.\n",
    "\n",
    "# delayThresholdstr\n",
    "# the minimum delay to wait to data to arrive late, relative to the latest record that has been processed in the form of an interval (e.g. “1 minute” or “5 hours”).\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This API is evolving.\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import timestamp_seconds\n",
    "# sdf.select(\n",
    "#    'name',\n",
    "#    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.unionByName\n",
    "# DataFrame.unionByName(other, allowMissingColumns=False)[source]\n",
    "# Returns a new DataFrame containing union of rows in this and another DataFrame.\n",
    "\n",
    "# This is different from both UNION ALL and UNION DISTINCT in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by distinct().\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# The difference between this function and union() is that this function resolves columns by name (not by position):\n",
    "\n",
    "# >>>\n",
    "# df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "# df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "# df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameStatFunctions.corr\n",
    "# DataFrameStatFunctions.corr(col1, col2, method=None)[source]\n",
    "# Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1str\n",
    "# The name of the first column\n",
    "\n",
    "# col2str\n",
    "# The name of the second column\n",
    "\n",
    "# methodstr, optional\n",
    "# The correlation method. Currently only supports “pearson”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.write\n",
    "# property DataFrame.write\n",
    "# Interface for saving the content of the non-streaming DataFrame out into external storage.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Returns:\n",
    "# DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.sql\n",
    "# SparkSession.sql(sqlQuery)[source]\n",
    "# Returns a DataFrame representing the result of the given query.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# DataFrame\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.createOrReplaceTempView(\"table1\")\n",
    "# df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
    "# df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.countDistinct\n",
    "# pyspark.sql.functions.countDistinct(col, *cols)[source]\n",
    "# Returns a new Column for distinct count of col or cols.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.agg(countDistinct(df.age, df.name).alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.enableHiveSupport\n",
    "# builder.enableHiveSupport()\n",
    "# Enables Hive support, including connectivity to a persistent Hive metastore, support for Hive SerDes, and Hive user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.appName\n",
    "# builder.appName(name)\n",
    "# Sets a name for the application, which will be shown in the Spark web UI.\n",
    "\n",
    "# If no application name is set, a randomly generated name will be used.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# namestr\n",
    "# an application name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.date_format\n",
    "# pyspark.sql.functions.date_format(date, format)[source]\n",
    "# Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\n",
    "\n",
    "# A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of datetime pattern. can be used.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# Whenever possible, use specialized functions like year.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "# df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.encode\n",
    "# pyspark.sql.functions.encode(col, charset)[source]\n",
    "# Computes the first argument into a binary from a string using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’).\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.withColumnRenamed\n",
    "# DataFrame.withColumnRenamed(existing, new)[source]\n",
    "# Returns a new DataFrame by renaming an existing column. This is a no-op if schema doesn’t contain the given column name.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# existingstr\n",
    "# string, name of the existing column to rename.\n",
    "\n",
    "# newstr\n",
    "# string, new name of the column.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.withColumnRenamed('age', 'age2').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.floor\n",
    "# pyspark.sql.functions.floor(col)[source]\n",
    "# Computes the floor of the given value.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.date_add\n",
    "# pyspark.sql.functions.date_add(start, days)[source]\n",
    "# Returns the date that is days days after start\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "# df.select(date_add(df.dt, 1).alias('next_date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.date_sub\n",
    "# pyspark.sql.functions.date_sub(start, days)[source]\n",
    "# Returns the date that is days days before start\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(date_sub(df.dt, 1).alias('prev_date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.explode\n",
    "# pyspark.sql.functions.explode(col)[source]\n",
    "# Returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.sparkContext\n",
    "# property SparkSession.sparkContext\n",
    "# Returns the underlying SparkContext.\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.stop\n",
    "# SparkSession.stop()[source]\n",
    "# Stop the underlying SparkContext.\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.streams\n",
    "# property SparkSession.streams\n",
    "# Returns a StreamingQueryManager that allows managing all the StreamingQuery instances active on this context.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# StreamingQueryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.desc\n",
    "# pyspark.sql.functions.desc(col)[source]\n",
    "# Returns a sort expression based on the descending order of the given column name.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.count\n",
    "# pyspark.sql.functions.count(col)[source]\n",
    "# Aggregate function: returns the number of items in a group.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby().mean('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupedData.min(*cols)\n",
    "df.groupby().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupedData.applyInPandas(func, schema)\n",
    "df=spark.createDataFrame([(1,1.0),(2,2.0)])\n",
    "def normalize(df):\n",
    "    v=df.v\n",
    "    return df.assign(v=(v-v.mean()))/v.std()\n",
    "df.groupby(\"id\").applyInPandas(normalize,schema=\"id long,v double\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupedData.apply(udf)\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.top(num: int, key: Optional[Callable[[T], S]] = None) → List[T][source]\n",
    "sc.parallelize([10, 4, 2, 12, 3]).top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.filter(f: Callable[[T], bool]) → pyspark.rdd.RDD[T][source]\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.RDD.isEmpty\n",
    "# Returns true if and only if the RDD contains no elements at all.\n",
    "sc.parallelize([]).isEmpty()\n",
    "sc.parallelize([1]).isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.emptyRDD¶\n",
    "SparkContext.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.window\n",
    "# pyspark.sql.functions.window(timeColumn, windowDuration, slideDuration=None, startTime=None)[source]\n",
    "# Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported.\n",
    "\n",
    "# The time column must be of pyspark.sql.types.TimestampType.\n",
    "\n",
    "# Durations are provided as strings, e.g. ‘1 second’, ‘1 day 12 hours’, ‘2 minutes’. Valid interval strings are ‘week’, ‘day’, ‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’. If the slideDuration is not provided, the windows will be tumbling windows.\n",
    "\n",
    "# The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals. For example, in order to have hourly tumbling windows that start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15… provide startTime as 15 minutes.\n",
    "\n",
    "# The output column will be a struct called ‘window’ by default with the nested columns ‘start’ and ‘end’, where ‘start’ and ‘end’ will be of pyspark.sql.types.TimestampType.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
    "w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
    "w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
    "         w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.range\n",
    "# SparkSession.range(start, end=None, step=1, numPartitions=None)[source]\n",
    "# Create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# startint\n",
    "# the start value\n",
    "\n",
    "# endint, optional\n",
    "# the end value (exclusive)\n",
    "\n",
    "# stepint, optional\n",
    "# the incremental step (default: 1)\n",
    "\n",
    "# numPartitionsint, optional\n",
    "# the number of partitions of the DataFrame\n",
    "\n",
    "\n",
    "\n",
    "spark.range(1, 7, 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.posexplode_outer\n",
    "# pyspark.sql.functions.posexplode_outer(col)[source]\n",
    "# Returns a new row for each element with position in the given array or map. Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced. Uses the default column name pos for position, and col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# @pandas_udf('int', PandasUDFType.GROUPED_AGG)  \n",
    "# def min_udf(v):\n",
    "#     return v.min()\n",
    "# sorted(gdf.agg(min_udf(df.age)).collect())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.PandasCogroupedOps.applyInPandas\n",
    "PandasCogroupedOps.applyInPandas(func, schema)[source]\n",
    "Applies a function to each cogroup using pandas and returns the result as a DataFrame.\n",
    "\n",
    "The function should take two pandas.DataFrames and return another pandas.DataFrame. For each side of the cogroup, all columns are passed together as a pandas.DataFrame to the user-function and the returned pandas.DataFrame are combined as a DataFrame.\n",
    "\n",
    "The schema should be a StructType describing the schema of the returned pandas.DataFrame. The column labels of the returned pandas.DataFrame must either match the field names in the defined schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. The length of the returned pandas.DataFrame can be arbitrary.\n",
    "\n",
    "New in version 3.0.0.\n",
    "\n",
    "Parameters:\n",
    "funcfunction\n",
    "a Python native function that takes two pandas.DataFrames, and outputs a pandas.DataFrame, or that takes one tuple (grouping keys) and two pandas DataFrames, and outputs a pandas DataFrame.\n",
    "\n",
    "schemapyspark.sql.types.DataType or str\n",
    "the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "\n",
    "See also\n",
    "\n",
    "pyspark.sql.functions.pandas_udf\n",
    "Notes\n",
    "\n",
    "This function requires a full shuffle. All the data of a cogroup will be loaded into memory, so the user should be aware of the potential OOM risk if data is skewed and certain groups are too large to fit in memory.\n",
    "\n",
    "If returning a new pandas.DataFrame constructed with a dictionary, it is recommended to explicitly index the columns by name to ensure the positions are correct, or alternatively use an OrderedDict. For example, pd.DataFrame({‘id’: ids, ‘a’: data}, columns=[‘id’, ‘a’]) or pd.DataFrame(OrderedDict([(‘id’, ids), (‘a’, data)])).\n",
    "\n",
    "This API is experimental.\n",
    "\n",
    "Examples\n",
    "\n",
    ">>>\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
    "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    asof_join, schema=\"time int, id int, v1 double, v2 string\"\n",
    ").show()  \n",
    "+--------+---+---+---+\n",
    "|    time| id| v1| v2|\n",
    "+--------+---+---+---+\n",
    "|20000101|  1|1.0|  x|\n",
    "|20000102|  1|3.0|  x|\n",
    "|20000101|  2|2.0|  y|\n",
    "|20000102|  2|4.0|  y|\n",
    "+--------+---+---+---+\n",
    "Alternatively, the user can define a function that takes three arguments. In this case, the grouping key(s) will be passed as the first argument and the data will be passed as the second and third arguments. The grouping key(s) will be passed as a tuple of numpy data types, e.g., numpy.int32 and numpy.float64. The data will still be passed in as two pandas.DataFrame containing all columns from the original Spark DataFrames.\n",
    "\n",
    ">>>\n",
    "def asof_join(k, l, r):\n",
    "    if k == (1,):\n",
    "        return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])\n",
    "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    asof_join, \"time int, id int, v1 double, v2 string\").show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ike ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default.\n",
    "\n",
    "# pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
    "# the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.\n",
    "\n",
    "# modifiedBefore: an optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfter: an optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ike ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default.\n",
    "\n",
    "# pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
    "# the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.\n",
    "\n",
    "# modifiedBefore: an optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfter: an optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.functions.aggregate\n",
    "# pyspark.sql.functions.aggregate(col, initialValue, merge, finish=None)[source]\n",
    "# Applies a binary operator to an initial state and all elements in the array, and reduces this to a single state. The final state is converted into the final result by applying a finish function.\n",
    "\n",
    "# Both functions can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# initialValueColumn or str\n",
    "# initial value. Name of column or expression\n",
    "\n",
    "# mergefunction\n",
    "# a binary function (acc: Column, x: Column) -> Column... returning expression of the same type as zero\n",
    "\n",
    "# finishfunction\n",
    "# an optional unary function (x: Column) -> Column: ... used to convert accumulated value.\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
    "# df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
    "# +----+\n",
    "# | sum|\n",
    "# +----+\n",
    "# |42.0|\n",
    "# +----+\n",
    "# >>>\n",
    "# def merge(acc, x):\n",
    "#     count = acc.count + 1\n",
    "#     sum = acc.sum + x\n",
    "#     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
    "# df.select(\n",
    "#     aggregate(\n",
    "#         \"values\",\n",
    "#         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
    "#         merge,\n",
    "#         lambda acc: acc.sum / acc.count,\n",
    "#     ).alias(\"mean\")\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.parquet\n",
    "# DataFrameReader.parquet(*paths, **options)[source]\n",
    "# Loads Parquet files, returning the result as a DataFrame.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathsstr\n",
    "# Other Parameters:\n",
    "# mergeSchemastr or bool, optional\n",
    "# sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. The default value is specified in spark.sql.parquet.mergeSchema.\n",
    "\n",
    "# pathGlobFilterstr or bool, optional\n",
    "# an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery. # noqa\n",
    "\n",
    "# recursiveFileLookupstr or bool, optional\n",
    "# recursively scan a directory for files. Using this option disables partition discovery. # noqa\n",
    "\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedBefore (batch only)an optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfter (batch only)an optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.createDataFrame\n",
    "# SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)[source]\n",
    "# Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
    "\n",
    "# When schema is a list of column names, the type of each column will be inferred from data.\n",
    "\n",
    "# When schema is None, it will try to infer the schema (column names and types) from data, which should be an RDD of either Row, namedtuple, or dict.\n",
    "\n",
    "# When schema is pyspark.sql.types.DataType or a datatype string, it must match the real data, or an exception will be thrown at runtime. If the given schema is not pyspark.sql.types.StructType, it will be wrapped into a pyspark.sql.types.StructType as its only field, and the field name will be “value”. Each record will also be wrapped into a tuple, which can be converted to row later.\n",
    "\n",
    "# If schema inference is needed, samplingRatio is used to determined the ratio of rows used for schema inference. The first row will be used if samplingRatio is None.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Changed in version 2.1.0: Added verifySchema.\n",
    "\n",
    "# Parameters:\n",
    "# dataRDD or iterable\n",
    "# an RDD of any kind of SQL data representation (Row, tuple, int, boolean, etc.), or list, or pandas.DataFrame.\n",
    "\n",
    "# schemapyspark.sql.types.DataType, str or list, optional\n",
    "# a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None. The data type string format equals to pyspark.sql.types.DataType.simpleString, except that top level struct type can omit the struct<> and atomic types use typeName() as their format, e.g. use byte instead of tinyint for pyspark.sql.types.ByteType. We can also use int as a short name for pyspark.sql.types.IntegerType.\n",
    "\n",
    "# samplingRatiofloat, optional\n",
    "# the sample ratio of rows used for inferring\n",
    "\n",
    "# verifySchemabool, optional\n",
    "# verify data types of every row against schema. Enabled by default.\n",
    "\n",
    "# Returns:\n",
    "# DataFrame\n",
    "# Notes\n",
    "\n",
    "# Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# l = [('Alice', 1)]\n",
    "# spark.createDataFrame(l).collect()\n",
    "# [Row(_1='Alice', _2=1)]\n",
    "# spark.createDataFrame(l, ['name', 'age']).collect()\n",
    "# [Row(name='Alice', age=1)]\n",
    "# >>>\n",
    "# d = [{'name': 'Alice', 'age': 1}]\n",
    "# spark.createDataFrame(d).collect()\n",
    "# [Row(age=1, name='Alice')]\n",
    "# >>>\n",
    "# rdd = sc.parallelize(l)\n",
    "# spark.createDataFrame(rdd).collect()\n",
    "# [Row(_1='Alice', _2=1)]\n",
    "# df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "# df.collect()\n",
    "# [Row(name='Alice', age=1)]\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# Person = Row('name', 'age')\n",
    "# person = rdd.map(lambda r: Person(*r))\n",
    "# df2 = spark.createDataFrame(person)\n",
    "# df2.collect()\n",
    "# [Row(name='Alice', age=1)]\n",
    "# >>>\n",
    "# from pyspark.sql.types import *\n",
    "# schema = StructType([\n",
    "#    StructField(\"name\", StringType(), True),\n",
    "#    StructField(\"age\", IntegerType(), True)])\n",
    "# df3 = spark.createDataFrame(rdd, schema)\n",
    "# df3.collect()\n",
    "# [Row(name='Alice', age=1)]\n",
    "# >>>\n",
    "# spark.createDataFrame(df.toPandas()).collect()  \n",
    "# [Row(name='Alice', age=1)]\n",
    "# spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.option\n",
    "# DataFrameReader.option(key, value)[source]\n",
    "# Adds an input option for the underlying data source.\n",
    "\n",
    "# You can set the following option(s) for reading files:\n",
    "# timeZone: sets the string that indicates a time zone ID to be used to parse\n",
    "# timestamps in the JSON/CSV datasources or partition values. The following formats of timeZone are supported:\n",
    "\n",
    "# Region-based zone ID: It should have the form ‘area/city’, such as ‘America/Los_Angeles’.\n",
    "\n",
    "# Zone offset: It should be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’.\n",
    "\n",
    "# Other short names like ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default.\n",
    "\n",
    "# pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
    "# the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.\n",
    "\n",
    "# modifiedBefore: an optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfter: an optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.approxQuantile\n",
    "# DataFrame.approxQuantile(col, probabilities, relativeError)[source]\n",
    "# Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "# The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p * N). More precisely,\n",
    "\n",
    "# floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
    "\n",
    "# This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670 Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.\n",
    "\n",
    "# Note that null values will be ignored in numerical columns before calculation. For columns only containing null values, an empty list is returned.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# col: str, tuple or list\n",
    "# Can be a single column name, or a list of names for multiple columns.\n",
    "\n",
    "# Changed in version 2.2: Added support for multiple columns.\n",
    "\n",
    "# probabilitieslist or tuple\n",
    "# a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "\n",
    "# relativeErrorfloat\n",
    "# The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but give the same result as 1.\n",
    "\n",
    "# Returns:\n",
    "# list\n",
    "# the approximate quantiles at the given probabilities. If the input col is a string, the output is a list of floats. If the input col is a list or tuple of strings, the output is also a list, but each element in it is a list of floats, i.e., the output is a list of list of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.csv\n",
    "# DataFrameReader.csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None)[source]\n",
    "# Loads a CSV file and returns the result as a DataFrame.\n",
    "\n",
    "# This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr or list\n",
    "# string, or list of strings, for input path(s), or RDD of Strings storing CSV rows.\n",
    "\n",
    "# schemapyspark.sql.types.StructType or str, optional\n",
    "# an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# sepstr, optional\n",
    "# sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,.\n",
    "\n",
    "# encodingstr, optional\n",
    "# decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8.\n",
    "\n",
    "# quotestr, optional\n",
    "# sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "\n",
    "# escapestr, optional\n",
    "# sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \\.\n",
    "\n",
    "# commentstr, optional\n",
    "# sets a single character used for skipping lines beginning with this character. By default (None), it is disabled.\n",
    "\n",
    "# headerstr or bool, optional\n",
    "# uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "\n",
    "# Note\n",
    "\n",
    "# if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists.\n",
    "\n",
    "# inferSchemastr or bool, optional\n",
    "# infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.\n",
    "\n",
    "# enforceSchemastr or bool, optional\n",
    "# If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files or the first header in RDD if the header option is set to true. Field names in the schema and column names in CSV headers are checked by their positions taking into account spark.sql.caseSensitive. If None is set, true is used by default. Though the default value is true, it is recommended to disable the enforceSchema option to avoid incorrect results.\n",
    "\n",
    "# ignoreLeadingWhiteSpacestr or bool, optional\n",
    "# A flag indicating whether or not leading whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "\n",
    "# ignoreTrailingWhiteSpacestr or bool, optional\n",
    "# A flag indicating whether or not trailing whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "\n",
    "# nullValuestr, optional\n",
    "# sets the string representation of a null value. If None is set, it uses the default value, empty string. Since 2.0.1, this nullValue param applies to all supported types including the string type.\n",
    "\n",
    "# nanValuestr, optional\n",
    "# sets the string representation of a non-number value. If None is set, it uses the default value, NaN.\n",
    "\n",
    "# positiveInfstr, optional\n",
    "# sets the string representation of a positive infinity value. If None is set, it uses the default value, Inf.\n",
    "\n",
    "# negativeInfstr, optional\n",
    "# sets the string representation of a negative infinity value. If None is set, it uses the default value, Inf.\n",
    "\n",
    "# dateFormatstr, optional\n",
    "# sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. # noqa This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "\n",
    "# timestampFormatstr, optional\n",
    "# sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. # noqa This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX].\n",
    "\n",
    "# maxColumnsstr or int, optional\n",
    "# defines a hard limit of how many columns a record can have. If None is set, it uses the default value, 20480.\n",
    "\n",
    "# maxCharsPerColumnstr or int, optional\n",
    "# defines the maximum number of characters allowed for any given value being read. If None is set, it uses the default value, -1 meaning unlimited length.\n",
    "\n",
    "# maxMalformedLogPerPartitionstr or int, optional\n",
    "# this parameter is no longer used since Spark 2.2.0. If specified, it is ignored.\n",
    "\n",
    "# modestr, optional\n",
    "# allows a mode for dealing with corrupt records during parsing. If None is set, it uses the default value, PERMISSIVE. Note that Spark tries to parse only required columns in CSV under column pruning. Therefore, corrupt records can be different based on required set of fields. This behavior can be controlled by spark.sql.csv.parser.columnPruning.enabled (enabled by default).\n",
    "\n",
    "# PERMISSIVE: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets null to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.\n",
    "\n",
    "# DROPMALFORMED: ignores the whole corrupted records.\n",
    "\n",
    "# FAILFAST: throws an exception when it meets corrupted records.\n",
    "\n",
    "# columnNameOfCorruptRecordstr, optional\n",
    "# allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.\n",
    "\n",
    "# multiLinestr or bool, optional\n",
    "# parse records, which may span multiple lines. If None is set, it uses the default value, false.\n",
    "\n",
    "# charToEscapeQuoteEscapingstr, optional\n",
    "# sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \\0 otherwise.\n",
    "\n",
    "# samplingRatiostr or float, optional\n",
    "# defines fraction of rows used for schema inferring. If None is set, it uses the default value, 1.0.\n",
    "\n",
    "# emptyValuestr, optional\n",
    "# sets the string representation of an empty value. If None is set, it uses the default value, empty string.\n",
    "\n",
    "# localestr, optional\n",
    "# sets a locale as language tag in IETF BCP 47 format. If None is set, it uses the default value, en-US. For instance, locale is used while parsing dates and timestamps.\n",
    "\n",
    "# lineSepstr, optional\n",
    "# defines the line separator that should be used for parsing. If None is set, it covers all \\\\r, \\\\r\\\\n and \\\\n. Maximum length is 1 character.\n",
    "\n",
    "# pathGlobFilterstr or bool, optional\n",
    "# an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery. # noqa\n",
    "\n",
    "# recursiveFileLookupstr or bool, optional\n",
    "# recursively scan a directory for files. Using this option disables partition discovery. # noqa\n",
    "\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedBefore (batch only)an optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfter (batch only)an optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# unescapedQuoteHandlingstr, optional\n",
    "# defines how the CsvParser will handle values with unescaped quotes. If None is set, it uses the default value, STOP_AT_DELIMITER.\n",
    "\n",
    "# STOP_AT_CLOSING_QUOTE: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.\n",
    "\n",
    "# BACK_TO_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.\n",
    "\n",
    "# STOP_AT_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.\n",
    "\n",
    "# SKIP_VALUE: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.\n",
    "\n",
    "# RAISE_ERROR: If unescaped quotes are found in the input, a TextParsingException will be thrown.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.csv('python/test_support/sql/ages.csv')\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.jdbc\n",
    "# DataFrameReader.jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)[source]\n",
    "# Construct a DataFrame representing the database table named table accessible via JDBC URL url and connection properties.\n",
    "\n",
    "# Partitions of the table will be retrieved in parallel if either column or predicates is specified. lowerBound, upperBound and numPartitions is needed when column is specified.\n",
    "\n",
    "# If both column and predicates are specified, column will be used.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# urlstr\n",
    "# a JDBC URL of the form jdbc:subprotocol:subname\n",
    "\n",
    "# tablestr\n",
    "# the name of the table\n",
    "\n",
    "# columnstr, optional\n",
    "# the name of a column of numeric, date, or timestamp type that will be used for partitioning; if this parameter is specified, then numPartitions, lowerBound (inclusive), and upperBound (exclusive) will form partition strides for generated WHERE clause expressions used to split the column column evenly\n",
    "\n",
    "# lowerBoundstr or int, optional\n",
    "# the minimum value of column used to decide partition stride\n",
    "\n",
    "# upperBoundstr or int, optional\n",
    "# the maximum value of column used to decide partition stride\n",
    "\n",
    "# numPartitionsint, optional\n",
    "# the number of partitions\n",
    "\n",
    "# predicateslist, optional\n",
    "# a list of expressions suitable for inclusion in WHERE clauses; each one defines one partition of the DataFrame\n",
    "\n",
    "# propertiesdict, optional\n",
    "# a dictionary of JDBC database connection arguments. Normally at least properties “user” and “password” with their corresponding values. For example { ‘user’ : ‘SYSTEM’, ‘password’ : ‘mypassword’ }\n",
    "\n",
    "# Returns:\n",
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.json\n",
    "# DataFrameReader.json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None, allowNonNumericNumbers=None, modifiedBefore=None, modifiedAfter=None)[source]\n",
    "# Loads JSON files and returns the results as a DataFrame.\n",
    "\n",
    "# JSON Lines (newline-delimited JSON) is supported by default. For JSON (one record per file), set the multiLine parameter to true.\n",
    "\n",
    "# If the schema parameter is not specified, this function goes through the input once to determine the input schema.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr, list or RDD\n",
    "# string represents path to the JSON dataset, or a list of paths, or RDD of Strings storing JSON objects.\n",
    "\n",
    "# schemapyspark.sql.types.StructType or str, optional\n",
    "# an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# primitivesAsStringstr or bool, optional\n",
    "# infers all primitive values as a string type. If None is set, it uses the default value, false.\n",
    "\n",
    "# prefersDecimalstr or bool, optional\n",
    "# infers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles. If None is set, it uses the default value, false.\n",
    "\n",
    "# allowCommentsstr or bool, optional\n",
    "# ignores Java/C++ style comment in JSON records. If None is set, it uses the default value, false.\n",
    "\n",
    "# allowUnquotedFieldNamesstr or bool, optional\n",
    "# allows unquoted JSON field names. If None is set, it uses the default value, false.\n",
    "\n",
    "# allowSingleQuotesstr or bool, optional\n",
    "# allows single quotes in addition to double quotes. If None is set, it uses the default value, true.\n",
    "\n",
    "# allowNumericLeadingZerostr or bool, optional\n",
    "# allows leading zeros in numbers (e.g. 00012). If None is set, it uses the default value, false.\n",
    "\n",
    "# allowBackslashEscapingAnyCharacterstr or bool, optional\n",
    "# allows accepting quoting of all character using backslash quoting mechanism. If None is set, it uses the default value, false.\n",
    "\n",
    "# modestr, optional\n",
    "# allows a mode for dealing with corrupt records during parsing. If None is\n",
    "# set, it uses the default value, PERMISSIVE.\n",
    "\n",
    "# PERMISSIVE: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema.\n",
    "\n",
    "# DROPMALFORMED: ignores the whole corrupted records.\n",
    "\n",
    "# FAILFAST: throws an exception when it meets corrupted records.\n",
    "\n",
    "# columnNameOfCorruptRecord: str, optional\n",
    "# allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.\n",
    "\n",
    "# dateFormatstr, optional\n",
    "# sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. # noqa This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "\n",
    "# timestampFormatstr, optional\n",
    "# sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. # noqa This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX].\n",
    "\n",
    "# multiLinestr or bool, optional\n",
    "# parse one record, which may span multiple lines, per file. If None is set, it uses the default value, false.\n",
    "\n",
    "# allowUnquotedControlCharsstr or bool, optional\n",
    "# allows JSON Strings to contain unquoted control characters (ASCII characters with value less than 32, including tab and line feed characters) or not.\n",
    "\n",
    "# encodingstr or bool, optional\n",
    "# allows to forcibly set one of standard basic or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If None is set, the encoding of input JSON will be detected automatically when the multiLine option is set to true.\n",
    "\n",
    "# lineSepstr, optional\n",
    "# defines the line separator that should be used for parsing. If None is set, it covers all \\r, \\r\\n and \\n.\n",
    "\n",
    "# samplingRatiostr or float, optional\n",
    "# defines fraction of input JSON objects used for schema inferring. If None is set, it uses the default value, 1.0.\n",
    "\n",
    "# dropFieldIfAllNullstr or bool, optional\n",
    "# whether to ignore column of all null values or empty array/struct during schema inference. If None is set, it uses the default value, false.\n",
    "\n",
    "# localestr, optional\n",
    "# sets a locale as language tag in IETF BCP 47 format. If None is set, it uses the default value, en-US. For instance, locale is used while parsing dates and timestamps.\n",
    "\n",
    "# pathGlobFilterstr or bool, optional\n",
    "# an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery. # noqa\n",
    "\n",
    "# recursiveFileLookupstr or bool, optional\n",
    "# recursively scan a directory for files. Using this option disables partition discovery. # noqa\n",
    "\n",
    "# allowNonNumericNumbersstr or bool\n",
    "# allows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values. If None is set, it uses the default value, true.\n",
    "\n",
    "# +INF: for positive infinity, as well as alias of\n",
    "# +Infinity and Infinity.\n",
    "\n",
    "# -INF: for negative infinity, alias -Infinity.\n",
    "\n",
    "# NaN: for other not-a-numbers, like result of division by zero.\n",
    "\n",
    "# modifiedBeforean optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfteran optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df1 = spark.read.json('python/test_support/sql/people.json')\n",
    "# df1.dtypes\n",
    "# [('age', 'bigint'), ('name', 'string')]\n",
    "# rdd = sc.textFile('python/test_support/sql/people.json')\n",
    "# df2 = spark.read.json(rdd)\n",
    "# df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.to_utc_timestamp¶\n",
    "# pyspark.sql.functions.to_utc_timestamp(timestamp, tz)[source]\n",
    "# This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given timezone, and renders that timestamp as a timestamp in UTC.\n",
    "\n",
    "# However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from the given timezone to UTC timezone.\n",
    "\n",
    "# This function may return confusing result if the input is a string with timezone, e.g. ‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# timestampColumn or str\n",
    "# the column that contains timestamps\n",
    "\n",
    "# tzColumn or str\n",
    "# A string detailing the time zone ID that the input should be adjusted to. It should be in the format of either region-based zone IDs or zone offsets. Region IDs must have the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are upported as aliases of ‘+00:00’. Other short names are not recommended to use because they can be ambiguous.\n",
    "\n",
    "# Changed in version 2.4.0: tz can take a Column containing timezone ID strings.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
    "# df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
    "# [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
    "# df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.agg\n",
    "# GroupedData.agg(*exprs)[source]\n",
    "# Compute aggregates and returns the result as a DataFrame.\n",
    "\n",
    "# The available aggregate functions can be:\n",
    "\n",
    "# built-in aggregation functions, such as avg, max, min, sum, count\n",
    "\n",
    "# group aggregate pandas UDFs, created with pyspark.sql.functions.pandas_udf()\n",
    "\n",
    "# Note\n",
    "\n",
    "# There is no partial aggregation with group aggregate UDFs, i.e., a full shuffle is required. Also, all the data of a group will be loaded into memory, so the user should be aware of the potential OOM risk if data is skewed and certain groups are too large to fit in memory.\n",
    "\n",
    "# See also\n",
    "\n",
    "# pyspark.sql.functions.pandas_udf()\n",
    "\n",
    "# If exprs is a single dict mapping from string to string, then the key is the column to perform aggregation on, and the value is the aggregate function.\n",
    "\n",
    "# Alternatively, exprs can also be a list of aggregate Column expressions.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# exprsdict\n",
    "# a dict mapping from column name (string) to aggregate functions (string), or a list of Column.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed in a single call to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.from_json\n",
    "# pyspark.sql.functions.from_json(col, schema, options={})[source]\n",
    "# Parses a column containing a JSON string into a MapType with StringType as keys type, StructType or ArrayType with the specified schema. Returns null, in the case of an unparseable string.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# string column in json format\n",
    "\n",
    "# schemaDataType or str\n",
    "# a StructType or ArrayType of StructType to use when parsing the json column.\n",
    "\n",
    "# Changed in version 2.3: the DDL-formatted string is also supported for schema.\n",
    "\n",
    "# optionsdict, optional\n",
    "# options to control parsing. accepts the same options as the json datasource\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.types import *\n",
    "# data = [(1, '''{\"a\": 1}''')]\n",
    "# schema = StructType([StructField(\"a\", IntegerType())])\n",
    "# df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "# df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
    "# [Row(json=Row(a=1))]\n",
    "# df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
    "# [Row(json=Row(a=1))]\n",
    "# df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
    "# [Row(json={'a': 1})]\n",
    "# data = [(1, '''[{\"a\": 1}]''')]\n",
    "# schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
    "# df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "# df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
    "# [Row(json=[Row(a=1)])]\n",
    "# schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
    "# df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
    "# [Row(json=Row(a=None))]\n",
    "# data = [(1, '''[1, 2, 3]''')]\n",
    "# schema = ArrayType(IntegerType())\n",
    "# df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "# df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
    "# [Row(json=[1, 2, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.load\n",
    "# DataFrameReader.load(path=None, format=None, schema=None, **options)[source]\n",
    "# Loads data from a data source and returns it as a DataFrame.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr or list, optional\n",
    "# optional string or a list of string for file-system backed data sources.\n",
    "\n",
    "# formatstr, optional\n",
    "# optional string for format of the data source. Default to ‘parquet’.\n",
    "\n",
    "# schemapyspark.sql.types.StructType or str, optional\n",
    "# optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# **optionsdict\n",
    "# all other string options\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
    "#     opt1=True, opt2=1, opt3='str')\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.getOrCreate\n",
    "# builder.getOrCreate()\n",
    "# Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default.\n",
    "\n",
    "# >>>\n",
    "# s1 = SparkSession.builder.config(\"k1\", \"v1\").getOrCreate()\n",
    "# s1.conf.get(\"k1\") == \"v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structured Streaming\n",
    "# MLlib (DataFrame-based)\n",
    "# Spark Streaming\n",
    "# MLlib (RDD-based)\n",
    "# Spark Core\n",
    "# Resource Management\n",
    "# pyspark.sql.DataFrameReader.orc\n",
    "# DataFrameReader.orc(path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None)[source]\n",
    "# Loads ORC files, returning the result as a DataFrame.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr or list\n",
    "# mergeSchemastr or bool, optional\n",
    "# sets whether we should merge schemas collected from all ORC part-files. This will override spark.sql.orc.mergeSchema. The default value is specified in spark.sql.orc.mergeSchema.\n",
    "\n",
    "# pathGlobFilterstr or bool\n",
    "# an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery. # noqa\n",
    "\n",
    "# recursiveFileLookupstr or bool\n",
    "# recursively scan a directory for files. Using this option disables partition discovery. # noqa\n",
    "\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedBeforean optional timestamp to only include files with\n",
    "# modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# modifiedAfteran optional timestamp to only include files with\n",
    "# modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.functions.pandas_udf\n",
    "pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)[source]\n",
    "Creates a pandas user defined function (a.k.a. vectorized user defined function).\n",
    "\n",
    "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "New in version 2.3.0.\n",
    "\n",
    "Parameters:\n",
    "ffunction, optional\n",
    "user-defined function. A python function if used as a standalone function\n",
    "\n",
    "returnTypepyspark.sql.types.DataType or str, optional\n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "\n",
    "functionTypeint, optional\n",
    "an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR. This parameter exists for compatibility. Using Python type hints is encouraged.\n",
    "\n",
    "See also\n",
    "\n",
    "pyspark.sql.GroupedData.agg\n",
    "pyspark.sql.DataFrame.mapInPandas\n",
    "pyspark.sql.GroupedData.applyInPandas\n",
    "pyspark.sql.PandasCogroupedOps.applyInPandas\n",
    "pyspark.sql.UDFRegistration.register\n",
    "Notes\n",
    "\n",
    "The user-defined functions do not support conditional expressions or short circuiting in boolean expressions and it ends up with being executed all internally. If the functions can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
    "\n",
    "The user-defined functions do not take keyword arguments on the calling side.\n",
    "\n",
    "The data type of returned pandas.Series from the user-defined functions should be matched with defined returnType (see types.to_arrow_type() and types.from_arrow_type()). When there is mismatch between them, Spark might do conversion on returned data. The conversion is not guaranteed to be correct and results should be checked for accuracy by users.\n",
    "\n",
    "Currently, pyspark.sql.types.ArrayType of pyspark.sql.types.TimestampType and nested pyspark.sql.types.StructType are currently not supported as output types.\n",
    "\n",
    "Examples\n",
    "\n",
    "In order to use this API, customarily the below are imported:\n",
    "\n",
    ">>>\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "From Spark 3.0 with Python 3.6+, Python type hints detect the function types as below:\n",
    "\n",
    ">>>\n",
    "@pandas_udf(IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()\n",
    "Prior to Spark 3.0, the pandas UDF used functionType to decide the execution type as below:\n",
    "\n",
    ">>>\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()\n",
    "It is preferred to specify type hints for the pandas UDF instead of specifying pandas UDF type via functionType which will be deprecated in the future releases.\n",
    "\n",
    "Note that the type hint should use pandas.Series in all cases but there is one variant that pandas.DataFrame should be used for its input or output type hint instead when the input or output column is of pyspark.sql.types.StructType. The following example shows a Pandas UDF which takes long column, string column and struct column, and outputs a struct column. It requires the function to specify the type hints of pandas.Series and pandas.DataFrame as below:\n",
    "\n",
    ">>>\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()\n",
    "root\n",
    "|-- long_column: long (nullable = true)\n",
    "|-- string_column: string (nullable = true)\n",
    "|-- struct_column: struct (nullable = true)\n",
    "|    |-- col1: string (nullable = true)\n",
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()\n",
    "|-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
    "|    |-- col1: string (nullable = true)\n",
    "|    |-- col2: long (nullable = true)\n",
    "In the following sections, it describes the combinations of the supported type hints. For simplicity, pandas.DataFrame variant is omitted.\n",
    "\n",
    "Series to Series\n",
    "pandas.Series, … -> pandas.Series\n",
    "\n",
    "The function takes one or more pandas.Series and outputs one pandas.Series. The output of the function should always be of the same length as the input.\n",
    "\n",
    ">>>\n",
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(to_upper(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.functions.udf\n",
    "pyspark.sql.functions.udf(f=None, returnType=StringType)[source]\n",
    "Creates a user defined function (UDF).\n",
    "\n",
    "New in version 1.3.0.\n",
    "\n",
    "Parameters:\n",
    "ffunction\n",
    "python function if used as a standalone function\n",
    "\n",
    "returnTypepyspark.sql.types.DataType or str\n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "\n",
    "Notes\n",
    "\n",
    "The user-defined functions are considered deterministic by default. Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query. If your function is not deterministic, call asNondeterministic on the user defined function. E.g.:\n",
    "\n",
    ">>>\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
    "The user-defined functions do not support conditional expressions or short circuiting in boolean expressions and it ends up with being executed all internally. If the functions can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
    "\n",
    "The user-defined functions do not take keyword arguments on the calling side.\n",
    "\n",
    "Examples\n",
    "\n",
    ">>>\n",
    "from pyspark.sql.types import IntegerType\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "@udf\n",
    "def to_upper(s):\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def add_one(x):\n",
    "    if x is not None:\n",
    "        return x + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structured Streaming\n",
    "# MLlib (DataFrame-based)\n",
    "# Spark Streaming\n",
    "# MLlib (RDD-based)\n",
    "# Spark Core\n",
    "# Resource Management\n",
    "# pyspark.sql.DataFrameReader.format\n",
    "# DataFrameReader.format(source)[source]\n",
    "# Specifies the input data source format.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# sourcestr\n",
    "# string, name of the data source, e.g. ‘json’, ‘parquet’.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
    "# df.dtypes\n",
    "# [('age', 'bigint'), ('name', 'string')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.avro.functions.from_avro\n",
    "# pyspark.sql.avro.functions.from_avro(data, jsonFormatSchema, options={})[source]\n",
    "# Converts a binary column of Avro format into its corresponding catalyst value. The specified schema must match the read data, otherwise the behavior is undefined: it may fail or return arbitrary result. To deserialize the data with a compatible and evolved schema, the expected Avro schema can be set via the option avroSchema.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# dataColumn or str\n",
    "# the binary column.\n",
    "\n",
    "# jsonFormatSchemastr\n",
    "# the avro schema in JSON string format.\n",
    "\n",
    "# optionsdict, optional\n",
    "# options to control how the Avro record is parsed.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of “Apache Avro Data Source Guide”.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "# data = [(1, Row(age=2, name='Alice'))]\n",
    "# df = spark.createDataFrame(data, (\"key\", \"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.schema\n",
    "# DataFrameReader.schema(schema)[source]\n",
    "# Specifies the input schema.\n",
    "\n",
    "# Some data sources (e.g. JSON) can infer the input schema automatically from data. By specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# schemapyspark.sql.types.StructType or str\n",
    "# a pyspark.sql.types.StructType object or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "# >>> s = spark.read.schema(“col0 INT, col1 DOUBLE”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameReader.table\n",
    "# DataFrameReader.table(tableName)[source]\n",
    "# Returns the specified table as a DataFrame.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# tableNamestr\n",
    "# string, name of the table.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
    "# df.createOrReplaceTempView('tmpTable')\n",
    "# spark.read.table('tmpTable').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.read\n",
    "# property SparkSession.read\n",
    "# Returns a DataFrameReader that can be used to read data in as a DataFrame.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# DataFrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.conf\n",
    "# property SparkSession.conf\n",
    "# Runtime configuration interface for Spark.\n",
    "\n",
    "# This is the interface through which the user can get and set all Spark and Hadoop configurations that are relevant to Spark SQL. When getting the value of a config, this defaults to the value set in the underlying SparkContext, if any.\n",
    "\n",
    "# Returns:\n",
    "# pyspark.sql.conf.RuntimeConfig\n",
    "# New in version 2.0: .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameStatFunctions.approxQuantile\n",
    "# DataFrameStatFunctions.approxQuantile(col, probabilities, relativeError)[source]\n",
    "# Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "# The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p * N). More precisely,\n",
    "\n",
    "# floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
    "\n",
    "# This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670 Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.\n",
    "\n",
    "# Note that null values will be ignored in numerical columns before calculation. For columns only containing null values, an empty list is returned.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# col: str, tuple or list\n",
    "# Can be a single column name, or a list of names for multiple columns.\n",
    "\n",
    "# Changed in version 2.2: Added support for multiple columns.\n",
    "\n",
    "# probabilitieslist or tuple\n",
    "# a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "\n",
    "# relativeErrorfloat\n",
    "# The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but give the same result as 1.\n",
    "\n",
    "# Returns:\n",
    "# list\n",
    "# the approximate quantiles at the given probabilities. If the input col is a string, the output is a list of floats. If the input col is a list or tuple of strings, the output is also a list, but each element in it is a list of floats, i.e., the output is a list of list of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.getActiveSession\n",
    "# classmethod SparkSession.getActiveSession()[source]\n",
    "# Returns the active SparkSession for the current thread, returned by the builder\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Returns:\n",
    "# SparkSession\n",
    "# Spark session if an active session exists for the current thread\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# s = SparkSession.getActiveSession()\n",
    "# l = [('Alice', 1)]\n",
    "# rdd = s.sparkContext.parallelize(l)\n",
    "# df = s.createDataFrame(rdd, ['name', 'age'])\n",
    "# df.select(\"age\").collect()\n",
    "# [Row(age=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.saveAsTable¶\n",
    "# DataFrameWriter.saveAsTable(name, format=None, mode=None, partitionBy=None, **options)[source]\n",
    "# Saves the content of the DataFrame as the specified table.\n",
    "\n",
    "# In the case the table already exists, behavior of this function depends on the save mode, specified by the mode function (default to throwing an exception). When mode is Overwrite, the schema of the DataFrame does not need to be the same as that of the existing table.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# error or errorifexists: Throw an exception if data already exists.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# namestr\n",
    "# the table name\n",
    "\n",
    "# formatstr, optional\n",
    "# the format used to save\n",
    "\n",
    "# modestr, optional\n",
    "# one of append, overwrite, error, errorifexists, ignore (default: error)\n",
    "\n",
    "# partitionBystr or list\n",
    "# names of partitioning columns\n",
    "\n",
    "# **optionsdict\n",
    "# all other string options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.save\n",
    "# DataFrameWriter.save(path=None, format=None, mode=None, partitionBy=None, **options)[source]\n",
    "# Saves the contents of the DataFrame to a data source.\n",
    "\n",
    "# The data source is specified by the format and a set of options. If format is not specified, the default data source configured by spark.sql.sources.default will be used.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr, optional\n",
    "# the path in a Hadoop supported file system\n",
    "\n",
    "# formatstr, optional\n",
    "# the format used to save\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "# partitionBylist, optional\n",
    "# names of partitioning columns\n",
    "\n",
    "# **optionsdict\n",
    "# all other string options\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.write.mode(\"append\").save(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.catalog\n",
    "# property SparkSession.catalog\n",
    "# Interface through which the user may create, drop, alter or query underlying databases, tables, functions, etc.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.master\n",
    "# builder.master(master)\n",
    "# Sets the Spark master URL to connect to, such as “local” to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077” to run on a Spark standalone cluster.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# masterstr\n",
    "# a url for spark master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.newSession\n",
    "SparkSession.newSession()[source]\n",
    "# Returns a new SparkSession as new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.apply\n",
    "# GroupedData.apply(udf)\n",
    "# It is an alias of pyspark.sql.GroupedData.applyInPandas(); however, it takes a pyspark.sql.functions.pandas_udf() whereas pyspark.sql.GroupedData.applyInPandas() takes a Python native function.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# udfpyspark.sql.functions.pandas_udf()\n",
    "# a grouped map user-defined function returned by pyspark.sql.functions.pandas_udf().\n",
    "\n",
    "# See also\n",
    "\n",
    "# pyspark.sql.functions.pandas_udf\n",
    "# Notes\n",
    "\n",
    "# It is preferred to use pyspark.sql.GroupedData.applyInPandas() over this API. This API will be deprecated in the future releases.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.option\n",
    "# DataFrameWriter.option(key, value)[source]\n",
    "# Adds an output option for the underlying data source.\n",
    "\n",
    "# You can set the following option(s) for writing files:\n",
    "# timeZone: sets the string that indicates a time zone ID to be used to format\n",
    "# timestamps in the JSON/CSV datasources or partition values. The following formats of timeZone are supported:\n",
    "\n",
    "# Region-based zone ID: It should have the form ‘area/city’, such as ‘America/Los_Angeles’.\n",
    "\n",
    "# Zone offset: It should be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’.\n",
    "\n",
    "# Other short names like ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.avro.functions.to_avro\n",
    "# pyspark.sql.avro.functions.to_avro(data, jsonFormatSchema='')[source]\n",
    "# Converts a column into binary of avro format.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# dataColumn or str\n",
    "# the data column.\n",
    "\n",
    "# jsonFormatSchemastr, optional\n",
    "# user-specified output avro schema in JSON string format.\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.avro.functions import to_avro\n",
    "data = ['SPADES']\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.select(to_avro(df.value).alias(\"suite\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.format_string\n",
    "# pyspark.sql.functions.format_string(format, *cols)[source]\n",
    "# Formats the arguments in printf-style and returns the result as a string column.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# formatstr\n",
    "# string that can contain embedded format tags and used as result column’s value\n",
    "\n",
    "# colsColumn or str\n",
    "# column names or Columns to be used in formatting\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
    "# df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.map_from_arrays¶\n",
    "# pyspark.sql.functions.map_from_arrays(col1, col2)[source]\n",
    "# Creates a new map from two arrays.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1Column or str\n",
    "# name of column containing a set of keys. All elements should not be null\n",
    "\n",
    "# col2Column or str\n",
    "# name of column containing a set of values\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
    "# df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.insertInto\n",
    "# DataFrameWriter.insertInto(tableName, overwrite=None)[source]\n",
    "# Inserts the content of the DataFrame to the specified table.\n",
    "\n",
    "# It requires that the schema of the DataFrame is the same as the schema of the table.\n",
    "\n",
    "# Optionally overwriting any existing data.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.csv¶\n",
    "# DataFrameWriter.csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)[source]\n",
    "# Saves the content of the DataFrame in CSV format at the specified path.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# pathstr\n",
    "# the path in any Hadoop supported file system\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already\n",
    "# exists.\n",
    "\n",
    "# compressionstr, optional\n",
    "# compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).\n",
    "\n",
    "# sepstr, optional\n",
    "# sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,.\n",
    "\n",
    "# quotestr, optional\n",
    "# sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If an empty string is set, it uses u0000 (null character).\n",
    "\n",
    "# escapestr, optional\n",
    "# sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \\\n",
    "\n",
    "# escapeQuotesstr or bool, optional\n",
    "# a flag indicating whether values containing quotes should always be enclosed in quotes. If None is set, it uses the default value true, escaping all values containing a quote character.\n",
    "\n",
    "# quoteAllstr or bool, optional\n",
    "# a flag indicating whether all values should always be enclosed in quotes. If None is set, it uses the default value false, only escaping values containing a quote character.\n",
    "\n",
    "# headerstr or bool, optional\n",
    "# writes the names of columns as the first line. If None is set, it uses the default value, false.\n",
    "\n",
    "# nullValuestr, optional\n",
    "# sets the string representation of a null value. If None is set, it uses the default value, empty string.\n",
    "\n",
    "# dateFormatstr, optional\n",
    "# sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. # noqa This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "\n",
    "# timestampFormatstr, optional\n",
    "# sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. # noqa This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX].\n",
    "\n",
    "# ignoreLeadingWhiteSpacestr or bool, optional\n",
    "# a flag indicating whether or not leading whitespaces from values being written should be skipped. If None is set, it uses the default value, true.\n",
    "\n",
    "# ignoreTrailingWhiteSpacestr or bool, optional\n",
    "# a flag indicating whether or not trailing whitespaces from values being written should be skipped. If None is set, it uses the default value, true.\n",
    "\n",
    "# charToEscapeQuoteEscapingstr, optional\n",
    "# sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \\0 otherwise..\n",
    "\n",
    "# encodingstr, optional\n",
    "# sets the encoding (charset) of saved csv files. If None is set, the default UTF-8 charset will be used.\n",
    "\n",
    "# emptyValuestr, optional\n",
    "# sets the string representation of an empty value. If None is set, it uses the default value, \"\".\n",
    "\n",
    "# lineSepstr, optional\n",
    "# defines the line separator that should be used for writing. If None is set, it uses the default value, \\\\n. Maximum length is 1 character.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.DataFrameWriter.bucketBy\n",
    "DataFrameWriter.bucketBy(numBuckets, col, *cols)[source]\n",
    "Buckets the output by the given columns.If specified, the output is laid out on the file system similar to Hive’s bucketing scheme.\n",
    "\n",
    "New in version 2.3.0.\n",
    "\n",
    "Parameters:\n",
    "numBucketsint\n",
    "the number of buckets to save\n",
    "\n",
    "colstr, list or tuple\n",
    "a name of a column, or a list of names.\n",
    "\n",
    "colsstr\n",
    "additional names (optional). If col is a list it should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.DataFrameWriter.options\n",
    "# DataFrameWriter.options(**options)[source]\n",
    "# Adds output options for the underlying data source.\n",
    "\n",
    "# You can set the following option(s) for writing files:\n",
    "# timeZone: sets the string that indicates a time zone ID to be used to format\n",
    "# timestamps in the JSON/CSV datasources or partition values. The following formats of timeZone are supported:\n",
    "\n",
    "# Region-based zone ID: It should have the form ‘area/city’, such as ‘America/Los_Angeles’.\n",
    "\n",
    "# Zone offset: It should be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’.\n",
    "\n",
    "# Other short names like ‘CST’ are not recommended to use because they can be ambiguous. If it isn’t set, the current value of the SQL config spark.sql.session.timeZone is used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.format\n",
    "# DataFrameWriter.format(source)[source]\n",
    "# Specifies the underlying output data source.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# sourcestr\n",
    "# string, name of the data source, e.g. ‘json’, ‘parquet’.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.transform\n",
    "# DataFrame.transform(func)[source]\n",
    "# Returns a new DataFrame. Concise syntax for chaining custom transformations.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# funcfunction\n",
    "# a function that takes and returns a DataFrame.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import col\n",
    "# df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
    "# def cast_all_to_int(input_df):\n",
    "#     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
    "# def sort_columns_asc(input_df):\n",
    "#     return input_df.select(*sorted(input_df.columns))\n",
    "# df.transform(cast_all_to_int).transform(sort_columns_asc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.partitionBy\n",
    "# DataFrameWriter.partitionBy(*cols)[source]\n",
    "# Partitions the output by the given columns on the file system.\n",
    "\n",
    "# If specified, the output is laid out on the file system similar to Hive’s partitioning scheme.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr or list\n",
    "# name of columns\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowSpec.rowsBetween(start, end)[source]\n",
    "# Defines the frame boundaries, from start (inclusive) to end (inclusive).\n",
    "\n",
    "# Both start and end are relative positions from the current row. For example, “0” means “current row”, while “-1” means the row before the current row, and “5” means the fifth row after the current row.\n",
    "\n",
    "# We recommend users use Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# startint\n",
    "# boundary start, inclusive. The frame is unbounded if this is Window.unboundedPreceding, or any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
    "\n",
    "# endint\n",
    "# boundary end, inclusive. The frame is unbounded if this is Window.unboundedFollowing, or any value greater than or equal to min(sys.maxsize, 9223372036854775807)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.WindowSpec.rangeBetween\n",
    "# WindowSpec.rangeBetween(start, end)[source]\n",
    "# Defines the frame boundaries, from start (inclusive) to end (inclusive).\n",
    "\n",
    "# Both start and end are relative from the current row. For example, “0” means “current row”, while “-1” means one off before the current row, and “5” means the five off after the current row.\n",
    "\n",
    "# We recommend users use Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# startint\n",
    "# boundary start, inclusive. The frame is unbounded if this is Window.unboundedPreceding, or any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
    "\n",
    "# endint\n",
    "# boundary end, inclusive. The frame is unbounded if this is Window.unboundedFollowing, or any value greater than or equal to min(sys.maxsize, 9223372036854775807)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Window.rowsBetween\n",
    "# static Window.rowsBetween(start, end)[source]\n",
    "# Creates a WindowSpec with the frame boundaries defined, from start (inclusive) to end (inclusive).\n",
    "\n",
    "# Both start and end are relative positions from the current row. For example, “0” means “current row”, while “-1” means the row before the current row, and “5” means the fifth row after the current row.\n",
    "\n",
    "# We recommend users use Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "# A row based boundary is based on the position of the row within the partition. An offset indicates the number of rows above or below the current row, the frame for the current row starts or ends. For instance, given a row based sliding frame with a lower bound offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from index 4 to index 7.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# startint\n",
    "# boundary start, inclusive. The frame is unbounded if this is Window.unboundedPreceding, or any value less than or equal to -9223372036854775808.\n",
    "\n",
    "# endint\n",
    "# boundary end, inclusive. The frame is unbounded if this is Window.unboundedFollowing, or any value greater than or equal to 9223372036854775807.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Window\n",
    "# from pyspark.sql import functions as func\n",
    "# from pyspark.sql import SQLContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\n",
    "# df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\n",
    "# window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
    "# df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\", \"sum\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameWriter.jdbc\n",
    "# DataFrameWriter.jdbc(url, table, mode=None, properties=None)[source]\n",
    "# Saves the content of the DataFrame to an external database table via JDBC.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# urlstr\n",
    "# a JDBC URL of the form jdbc:subprotocol:subname\n",
    "\n",
    "# tablestr\n",
    "# Name of the table in the external database.\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "# append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "# overwrite: Overwrite existing data.\n",
    "\n",
    "# ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "# error or errorifexists (default case): Throw an exception if data already exists.\n",
    "\n",
    "# propertiesdict\n",
    "# a dictionary of JDBC database connection arguments. Normally at least properties “user” and “password” with their corresponding values. For example { ‘user’ : ‘SYSTEM’, ‘password’ : ‘mypassword’ }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.countByValue() \n",
    "sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.subtractByKey\n",
    "x=sc.parallelize([(\"a\",1),(\"b\",2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtractByKey(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.RDD.reduceByKey\n",
    "from operator import add\n",
    "rdd=sc.parallelize([1,2,3,4,5])\n",
    "sorted(rdd.reduceByKey(add)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda>>)\n",
    "tmp=[('a',1),('b',2),('c',3)]\n",
    "sc.parallelize(tmp).sortByKey().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.takeOrdered(num, key=None)\n",
    "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the union of this RDD and another one.\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.union(other)[source]\n",
    "rdd=sc.parallelize([1,2,3])\n",
    "rdd1=sc.parallelize([4,5,6])\n",
    "rdd.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.variance()\n",
    "sc.parallelize([4,5,6]).variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\00824732\\Desktop\\New folder (2)\\Pyspark_main2.ipynb Cell 142\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/00824732/Desktop/New%20folder%20%282%29/Pyspark_main2.ipynb#Y261sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# RDD.lookup(key)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/00824732/Desktop/New%20folder%20%282%29/Pyspark_main2.ipynb#Y261sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m l\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/00824732/Desktop/New%20folder%20%282%29/Pyspark_main2.ipynb#Y261sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rdd\u001b[39m=\u001b[39msc\u001b[39m.\u001b[39mparallelize(\u001b[39mzip\u001b[39m(l,\u001b[39m1\u001b[39m),\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/00824732/Desktop/New%20folder%20%282%29/Pyspark_main2.ipynb#Y261sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rdd\u001b[39m.\u001b[39mlookup(\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# RDD.lookup(key)\n",
    "l=range(1000)\n",
    "rdd=sc.parallelize(zip(l,1),10)\n",
    "rdd.lookup(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.zip(other)\n",
    "x=sc.parallelize([1,2,3])\n",
    "y=sc.parallelize([4,5,6])\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coreclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.streaming.ForeachBatchFunction(sql_ctx, func)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = sdf.writeStream.format('memory').queryName('query_explain').start()\n",
    "sq.processAllAvailable() # Wait a bit to generate the runtime plans.\n",
    "sq.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the unique id of this query that persists across restarts from checkpoint data. That is, this id is generated when a query is started for the first time, and will be the same every time it is restarted from checkpoint data. There can only be one query with the same id active in a Spark cluster. Also see, runId.\n",
    "property StreamingQuery.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.DataStreamWriter.trigger¶\n",
    "writer = sdf.writeStream.trigger(processingTime='5 seconds')\n",
    "# trigger the query for just once batch of data\n",
    "writer = sdf.writeStream.trigger(once=True)\n",
    "# trigger the query for execution every 5 seconds\n",
    "writer = sdf.writeStream.trigger(continuous='5 seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.StreamingQuery.isActive\n",
    "property StreamingQuery.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataStreamReader.format(source)[source]\n",
    "s=spark.readstream.format(\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property StreamingQuery.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataStreamWriter.partitionBy(*cols)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append: Only the new rows in the streaming DataFrame/Dataset will be written to\n",
    "# the sink\n",
    "\n",
    "# complete: All the rows in the streaming DataFrame/Dataset will be written to the sink\n",
    "# every time these are some updates\n",
    "\n",
    "# update: only the rows that were updated in the streaming DataFrame/Dataset will be\n",
    "# written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to append mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=sdf.writeStream.outputMode('append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamingQuery.stop()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property StreamingQuery.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property StreamingQuery.id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Spark Session -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "SparkSession.builder.config(conf=SparkConf())\n",
    "SparkSession.builder.config(\"spark.some.config.option\", \"some-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.enableHiveSupport\n",
    "builder.enableHiveSupport() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default.\n",
    "s1 = SparkSession.builder.config(\"k1\", \"v1\").getOrCreate()\n",
    "s1.conf.get(\"k1\") == \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case an existing SparkSession is returned, the config options specified in this builder will be applied to the existing SparkSession.\n",
    "s2 = SparkSession.builder.config(\"k2\", \"v2\").getOrCreate(\n",
    "s1.conf.get(\"k1\") == s2.conf.get(\"k1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.builder.master\n",
    "builder.master(master: str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property SparkSession.catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.read\n",
    "property SparkSession.read\n",
    "Returns a DataFrameReader that can be used to read data in as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property SparkSession.readStream\n",
    "# Returns a DataStreamReader that can be used to read data streams as a streaming DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property SparkSession.sparkContext\n",
    "Returns the underlying SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the underlying SparkContext.\n",
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.streams\n",
    "property SparkSession.streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.udf\n",
    "property SparkSession.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The version of Spark on which this application is running.\n",
    "property SparkSession.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.drop_duplicates\n",
    "# drop_duplicates() is an alias for dropDuplicates().\n",
    "DataFrame.drop_duplicates(subset=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.isLocal\n",
    "# Returns True if the collect() and take() methods can be run locally (without any Spark executors).\n",
    "DataFrame.isLocal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.cache\n",
    "DataFrame.cache()\n",
    "# Persists the DataFrame with the default storage level (MEMORY_AND_DISK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.inputFiles\n",
    "# DataFrame.inputFiles()[source]\n",
    "# Returns a best-effort snapshot of the files that compose this DataFrame. This method simply asks each constituent BaseRelation for its respective files and takes the union of all results. Depending on the source relations, this may not find all input files. Duplicates are removed.\n",
    "# New in version 3.1.0.\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "len(df.inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.toPandas\n",
    "# DataFrame.toPandas()¶\n",
    "# This method should only be used if the resulting Pandas’s DataFrame is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "# Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
    "df.toPandas()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.dropna\n",
    "# DataFrame.dropna(how='any', thresh=None, subset=None)[source]¶\n",
    "df4.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.drop\n",
    "# Returns a new DataFrame that drops the specified column. This is a no-op if schema doesn’t contain the given column name(s).\n",
    "# New in version 1.4.0.\n",
    "df.drop('age').collect()\n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.distinct\n",
    "# Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "# New in version 1.3.0.\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # pyspark.sql.functions.add_months\n",
    "# # # pyspark.sql.functions.add_months(start, months)\n",
    "# # Returns the date that is months months after start\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(add_months(df.dt, 1).alias('next_month')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.sql.DataFrame.stat\n",
    "# # property DataFrame.stat\n",
    "# Returns a DataFrameStatFunctions for statistic functions.\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.select\n",
    "# rojects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, Column, or list\n",
    "# column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n",
    "df.select('*').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.head\n",
    "# # DataFrame.head(n=None)\n",
    "# Returns the first n rows.\n",
    "# New in version 1.3.0.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.hint\n",
    "# DataFrame.hint(name, *parameters)\n",
    "# Specifies some hint on the current DataFrame.\n",
    "# New in version 2.2.0.\n",
    "df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.rdd\n",
    "# property DataFrame.rdd¶\n",
    "# Returns the content as an pyspark.RDD of Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.sql.DataFrame.foreach\n",
    "# # DataFrame.foreach(f)\n",
    "# Applies the f function to all Row of this DataFrame.\n",
    "\n",
    "# This is a shorthand for df.rdd.foreach().\n",
    "def f(person):\n",
    "    print(person.name)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.array_except\n",
    "# pyspark.sql.functions.array_except(col1, col2)\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_except(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.sql.DataFrame.schema\n",
    "# # Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n",
    "# New in version 1.3.0.\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.agg\n",
    "# DataFrame.agg(*exprs)[source]¶\n",
    "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df.agg({\"age\": \"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.toJSON(use_unicode=True)[source]\n",
    "# Converts a DataFrame into a RDD of string.\n",
    "\n",
    "# Each row is turned into a JSON document as one element in the returned RDD.\n",
    "\n",
    "# New in versio\n",
    "df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_union\n",
    "# pyspark.sql.functions.array_union(col1, col2)[source]\n",
    "# Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1Column or str\n",
    "# name of column containing array\n",
    "\n",
    "# col2Column or str\n",
    "# name of column containing array\n",
    "\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_union(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.arrays_zip\n",
    "# pyspark.sql.functions.arrays_zip(*cols)[source]\n",
    "# Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsColumn or str\n",
    "# columns of arrays to be merged.\n",
    "from pyspark.sql.functions import arrays_zip\n",
    "df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
    "df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_distinct\n",
    "# pyspark.sql.functions.array_distinct(col)[source]\n",
    "# Collection function: removes duplicate values from the array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
    "df.select(array_distinct(df.data)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.alias\n",
    "# DataFrame.alias(alias)[source]\n",
    "# Returns a new DataFrame with an alias set.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# aliasstr\n",
    "# an alias name to be set for the DataFrame.\n",
    "from pyspark.sql.functions import *\n",
    "df_as1 = df.alias(\"df_as1\")\n",
    "df_as2 = df.alias(\"df_as2\")\n",
    "joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
    "joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_join\n",
    "# pyspark.sql.functions.array_join(col, delimiter, null_replacement=None)[source]\n",
    "# Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
    "df.select(array_join(df.data, \",\").alias(\"joined\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_intersect\n",
    "# pyspark.sql.functions.array_intersect(col1, col2)[source]\n",
    "# Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1Column or str\n",
    "# name of column containing array\n",
    "\n",
    "# col2Column or str\n",
    "# # name of column containing array\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_intersect(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.cume_dist\n",
    "# pyspark.sql.functions.cume_dist()[source]\n",
    "# Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.radians\n",
    "pyspark.sql.functions.radians(col)[source]\n",
    "# Converts an angle measured in degrees to an approximately equivalent angle measured in radians.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in degrees\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# angle in radians, as if computed by java.lang.Math.toRadians()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.isStreaming\n",
    "# property DataFrame.isStreaming\n",
    "# Returns True if this DataFrame contains one or more sources that continuously return data as it arrives. A DataFrame that reads data from a streaming source must be executed as a StreamingQuery using the start() method in DataStreamWriter. Methods that return a single answer, (e.g., count() or collect()) will throw an AnalysisException when there is a streaming source present.\n",
    "\n",
    "# New in version 2.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.col\n",
    "# pyspark.sql.functions.col(col)[source]\n",
    "# Returns a Column based on the given column name.’ Examples ——– >>> col(‘x’) Column<’x’> >>> column(‘x’) Column<’x’>\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.min¶\n",
    "# pyspark.sql.functions.min(col)[source]\n",
    "# Aggregate function: returns the minimum value of the expression in a group.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.atanh\n",
    "# pyspark.sql.functions.atanh(col)[source]\n",
    "# Computes inverse hyperbolic tangent of the input column.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
    "df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.radians\n",
    "pyspark.sql.functions.radians(col)[source]\n",
    "# Converts an angle measured in degrees to an approximately equivalent angle measured in radians.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in degrees\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# angle in radians, as if computed by java.lang.Math.toRadians()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.ntile(n)[source]\n",
    "# Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4.\n",
    "\n",
    "# This is equivalent to the NTILE function in SQL.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# nint\n",
    "# an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.percent_rank¶\n",
    "pyspark.sql.functions.percent_rank()[source]\n",
    "# Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hour\n",
    "# pyspark.sql.functions.hour(col)[source]\n",
    "# Extract the hours of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
    "df.select(hour('ts').alias('hour')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.trunc\n",
    "# pyspark.sql.functions.trunc(date, format)[source]\n",
    "# Returns date truncated to the unit specified by the format.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# dateColumn or str\n",
    "# formatstr\n",
    "# ‘year’, ‘yyyy’, ‘yy’ to truncate by year, or ‘month’, ‘mon’, ‘mm’ to truncate by month Other options are: ‘week’, ‘quarter’\n",
    "df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
    "df.select(trunc(df.d, 'year').alias('year')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql.functions.atan\n",
    "# pyspark.sql.functions.atan(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# inverse tangent of col, as if computed by java.lang.Math.atan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.transform_values\n",
    "# pyspark.sql.functions.transform_values(col, f)[source]\n",
    "# Applies a function to every key-value pair in a map and returns a map with the results of those applications as the new values for the pairs.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
    "df.select(transform_values(\n",
    "    \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
    ").alias(\"new_data\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.explain\n",
    "# DataFrame.explain(extended=None, mode=None)[source]\n",
    "# Prints the (logical and physical) plans to the console for debugging purpose.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# extendedbool, optional\n",
    "# default False. If False, prints only the physical plan. When this is a string without specifying the mode, it works as the mode is specified.\n",
    "\n",
    "# modestr, optional\n",
    "# specifies the expected output format of plans.\n",
    "\n",
    "# simple: Print only a physical plan.\n",
    "\n",
    "# extended: Print both logical and physical plans.\n",
    "\n",
    "# codegen: Print a physical plan and generated codes if they are available.\n",
    "\n",
    "# cost: Print a logical plan and statistics if they are available.\n",
    "\n",
    "# formatted: Split explain output into two sections: a physical plan outline and node details.\n",
    "\n",
    "# Changed in version 3.0.0: Added optional argument mode to specify the expected output format of plans.\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.orderBy\n",
    "# DataFrame.orderBy(*cols, **kwargs)\n",
    "# Returns a new DataFrame sorted by the specified column(s).\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, list, or Column, optional\n",
    "# list of Column or column names to sort by.\n",
    "\n",
    "# Other Parameters:\n",
    "# ascendingbool or list, optional\n",
    "# boolean or list of boolean (default True). Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols.\n",
    "df.sort(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.withField\n",
    "# Column.withField(fieldName, col)[source]\n",
    "# An expression that adds/replaces a field in StructType by name.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import lit\n",
    "# df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
    "# df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.to_date\n",
    "# pyspark.sql.functions.to_date(col, format=None)[source]\n",
    "# Converts a Column into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to datetime pattern. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted. Equivalent to col.cast(\"date\").\n",
    "\n",
    "# New in version 2.2.0.\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(to_date(df.t).alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_contains¶\n",
    "# pyspark.sql.functions.array_contains(col, value)[source]\n",
    "# Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column containing array\n",
    "\n",
    "# value :\n",
    "# value or column to check for in array\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
    "df.select(array_contains(df.data, \"a\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.intersectAll\n",
    "# DataFrame.intersectAll(other)[source]\n",
    "# Return a new DataFrame containing rows in both this DataFrame and another DataFrame while preserving duplicates.\n",
    "\n",
    "# This is equivalent to INTERSECT ALL in SQL. As standard in SQL, this function resolves columns by position (not by name).\n",
    "\n",
    "# New in version 2.4.0.\n",
    "df1.intersectAll(df2).sort(\"C1\", \"C2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.sortWithinPartitions\n",
    "# DataFrame.sortWithinPartitions(*cols, **kwargs)[source]\n",
    "# Returns a new DataFrame with each partition sorted by the specified column(s).\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, list or Column, optional\n",
    "# list of Column or column names to sort by.\n",
    "\n",
    "# Other Parameters:\n",
    "# ascendingbool or list, optional\n",
    "# boolean or list of boolean (default True). Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols.\n",
    "df.sortWithinPartitions(\"age\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.toLocalIterator\n",
    "# DataFrame.toLocalIterator(prefetchPartitions=False)[source]\n",
    "# Returns an iterator that contains all of the rows in this DataFrame. The iterator will consume as much memory as the largest partition in this DataFrame. With prefetch it may consume up to the memory of the 2 largest partitions.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# prefetchPartitionsbool, optional\n",
    "# If Spark should pre-fetch the next partition before it is needed.\n",
    "list(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.transform_keys\n",
    "# pyspark.sql.functions.transform_keys(col, f)[source]\n",
    "# Applies a function to every key-value pair in a map and returns a map with the results of those applications as the new keys for the pairs.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
    "df.select(transform_keys(\n",
    "    \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.variance\n",
    "# pyspark.sql.functions.variance(col)[source]\n",
    "# Aggregate function: alias for var_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.transform\n",
    "# pyspark.sql.functions.transform(col, f)[source]\n",
    "# Returns an array of elements after applying a transformation to each element in the input array.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# a function that is applied to each element of the input array. Can take one of the following forms:\n",
    "\n",
    "# Unary (x: Column) -> Column: ...\n",
    "\n",
    "# Binary (x: Column, i: Column) -> Column..., where the second argument is\n",
    "# a 0-based index of the element.\n",
    "\n",
    "# and can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
    "df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.describe\n",
    "# DataFrame.describe(*cols)[source]\n",
    "# Computes basic statistics for numeric and string columns.\n",
    "\n",
    "# New in version 1.3.1.\n",
    "\n",
    "# This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.\n",
    "df.describe(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # pyspark.sql.functions.var_pop\n",
    "    # pyspark.sql.functions.var_pop(col)[source]\n",
    "    # Aggregate function: returns the population variance of the values in a group.\n",
    "\n",
    "    # New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array\n",
    "# pyspark.sql.functions.array(*cols)[source]\n",
    "# Creates a new array column.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsColumn or str\n",
    "# column names or Columns that have the same data type.\n",
    "df.select(array('age', 'age').alias(\"arr\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.posexplode\n",
    "# pyspark.sql.functions.posexplode(col)[source]\n",
    "# Returns a new row for each element with position in the given array or map. Uses the default column name pos for position, and col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(posexplode(eDF.intlist)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.sampleBy\n",
    "# DataFrame.sampleBy(col, fractions, seed=None)[source]\n",
    "# Returns a stratified sample without replacement based on the fraction given on each stratum.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# column that defines strata\n",
    "\n",
    "# Changed in version 3.0: Added sampling by a column of Column\n",
    "\n",
    "# fractionsdict\n",
    "# sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n",
    "\n",
    "# seedint, optional\n",
    "# random seed\n",
    "\n",
    "# Returns:\n",
    "# a new DataFrame that represents the stratified sample\n",
    "from pyspark.sql.functions import col\n",
    "dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.when\n",
    "# Column.when(condition, value)[source]\n",
    "# Evaluates a list of conditions and returns one of multiple possible result expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# conditionColumn\n",
    "# a boolean Column expression.\n",
    "\n",
    "# value\n",
    "# a literal value, or a Column expression.\n",
    "from pyspark.sql import functions as F\n",
    "df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.randomSplit\n",
    "# DataFrame.randomSplit(weights, seed=None)[source]\n",
    "# Randomly splits this DataFrame with the provided weights.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# weightslist\n",
    "# list of doubles as weights with which to split the DataFrame. Weights will be normalized if they don’t sum up to 1.0.\n",
    "\n",
    "# seedint, optional\n",
    "# The seed for sampling.\n",
    "splits = df4.randomSplit([1.0, 2.0], 24)\n",
    "splits[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.DataFrame.corr\n",
    "# DataFrame.corr(col1, col2, method=None)[source]\n",
    "# Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1str\n",
    "# The name of the first column\n",
    "\n",
    "# col2str\n",
    "# The name of the second column\n",
    "\n",
    "# methodstr, optional\n",
    "# The correlation method. Currently only supports “pearson”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.create_map\n",
    "# pyspark.sql.functions.create_map(*cols)[source]\n",
    "# Creates a new map column.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsColumn or str\n",
    "# column names or Columns that are grouped as key-value pairs, e.g. (key1, value1, key2, value2, …).\n",
    "df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
    "df.select(create_map([df.name, df.age]).alias(\"map\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.eqNullSafe\n",
    "# Column.eqNullSafe(other)\n",
    "# Equality test that is safe for null values.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# other\n",
    "# a value or Column\n",
    "\n",
    "# Notes\n",
    "\n",
    "# Unlike Pandas, PySpark doesn’t consider NaN values to be NULL. See the NaN Semantics for details.\n",
    "\n",
    "# Examples\n",
    "\n",
    "from pyspark.sql import Row\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(id=1, value='foo'),\n",
    "    Row(id=2, value=None)\n",
    "])\n",
    "df1.select(\n",
    "    df1['value'] == 'foo',\n",
    "    df1['value'].eqNullSafe('foo'),\n",
    "    df1['value'].eqNullSafe(None)\n",
    ").show()\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(value = 'bar'),\n",
    "    Row(value = None)\n",
    "])\n",
    "df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
    "0\n",
    "df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
    "1\n",
    "df2 = spark.createDataFrame([\n",
    "#     Row(id=1, value=float('NaN')),\n",
    "#     Row(id=2, value=42.0),\n",
    "#     Row(id=3, value=None)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.unhex\n",
    "# pyspark.sql.functions.unhex(col)[source]\n",
    "# Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of number.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.join\n",
    "# DataFrame.join(other, on=None, how=None)[source]\n",
    "# Joins with another DataFrame, using the given join expression.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# otherDataFrame\n",
    "# Right side of the join\n",
    "\n",
    "# onstr, list or Column, optional\n",
    "# a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\n",
    "\n",
    "# howstr, optional\n",
    "# default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.\n",
    "from pyspark.sql.functions import desc\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.timestamp_seconds\n",
    "# pyspark.sql.functions.timestamp_seconds(col)[source]\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql.functions import timestamp_seconds\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "# time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
    "# time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.toDegrees\n",
    "# pyspark.sql.functions.toDegrees(col)[source]\n",
    "# Deprecated since version 2.1.0: Use degrees() instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.toRadians\n",
    "# pyspark.sql.functions.toRadians(col)[source]\n",
    "# Deprecated since version 2.1.0: Use radians()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.tanh¶\n",
    "# pyspark.sql.functions.tanh(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# hyperbolic angle\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# hyperbolic tangent of the given value as if computed by java.lang.Math.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.tan\n",
    "# pyspark.sql.functions.tan(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in radians\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# tangent of the given value, as if computed by java.lang.Math.tan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.lead\n",
    "# pyspark.sql.functions.lead(col, offset=1, default=None)[source]\n",
    "# Window function: returns the value that is offset rows after the current row, and default if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "\n",
    "# This is equivalent to the LEAD function in SQL.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# offsetint, optional\n",
    "# number of row to extend\n",
    "\n",
    "# defaultoptional\n",
    "# default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.conf.RuntimeConfig\n",
    "# class pyspark.sql.conf.RuntimeConfig(jconf)[source]\n",
    "# User-facing configuration API, accessible through SparkSession.conf.\n",
    "\n",
    "# Options set here are automatically propagated to the Hadoop configuration during I/O.\n",
    "\n",
    "# Methods\n",
    "\n",
    "# get(key[, default])\n",
    "\n",
    "# Returns the value of Spark runtime configuration property for the given key, assuming it is set.\n",
    "\n",
    "# isModifiable(key)\n",
    "\n",
    "# Indicates whether the configuration property with the given key is modifiable in the current session.\n",
    "\n",
    "# set(key, value)\n",
    "\n",
    "# Sets the given Spark runtime configuration property.\n",
    "\n",
    "# unset(key)\n",
    "\n",
    "# Resets the configuration property for the given key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.asc_nulls_last\n",
    "# pyspark.sql.functions.asc_nulls_last(col)[source]\n",
    "# Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.\n",
    "\n",
    "# New in version 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.asc_nulls_first\n",
    "# pyspark.sql.functions.asc_nulls_first(col)[source]\n",
    "# Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.\n",
    "\n",
    "# New in version 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.weekofyear\n",
    "# pyspark.sql.functions.weekofyear(col)[source]\n",
    "# Extract the week number of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(weekofyear(df.dt).alias('week')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rns a new DataFrame omitting rows with null values. DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.\n",
    "\n",
    "# New in version 1.3.1.\n",
    "\n",
    "# Parameters:\n",
    "# howstr, optional\n",
    "# ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "# thresh: int, optional\n",
    "# default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "# subsetstr, tuple or list, optional\n",
    "# optional list of column names to consider.\n",
    "\n",
    "# Examples\n",
    "\n",
    "df4.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.sort¶\n",
    "# DataFrame.sort(*cols, **kwargs)[source]\n",
    "# Returns a new DataFrame sorted by the specified column(s).\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr, list, or Column, optional\n",
    "# list of Column or column names to sort by.\n",
    "\n",
    "# Other Parameters:\n",
    "# ascendingbool or list, optional\n",
    "# boolean or list of boolean (default True). Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols.\n",
    "df.sort(df.age.desc()).collect()\n",
    "df.sort(\"age\", ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.second\n",
    "# pyspark.sql.functions.second(col)[source]\n",
    "# Extract the seconds of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
    "df.select(second('ts').alias('second')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_remove\n",
    "# pyspark.sql.functions.array_remove(col, element)[source]\n",
    "# Collection function: Remove all elements that equal to element from the given array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column containing array\n",
    "\n",
    "# element :\n",
    "# element to be removed from the array\n",
    "df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
    "df.select(array_remove(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.unbase64\n",
    "# pyspark.sql.functions.unbase64(col)[source]\n",
    "# Decodes a BASE64 encoded string column and returns it as a binary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.exp\n",
    "# pyspark.sql.functions.exp(col)[source]\n",
    "# Computes the exponential of the given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.mapInPandas\n",
    "# DataFrame.mapInPandas(func, schema)\n",
    "# Maps an iterator of batches in the current DataFrame using a Python native function that takes and outputs a pandas DataFrame, and returns the result as a DataFrame.\n",
    "\n",
    "# The function should take an iterator of pandas.DataFrames and return another iterator of pandas.DataFrames. All columns are passed together as an iterator of pandas.DataFrames to the function and the returned iterator of pandas.DataFrames are combined as a DataFrame. Each pandas.DataFrame size can be controlled by spark.sql.execution.arrow.maxRecordsPerBatch.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# funcfunction\n",
    "# a Python native function that takes an iterator of pandas.DataFrames, and outputs an iterator of pandas.DataFrames.\n",
    "\n",
    "# schemapyspark.sql.types.DataType or str\n",
    "# the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "df.mapInPandas(filter_func, df.schema).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.collect_list\n",
    "# pyspark.sql.functions.collect_list(col)[source]\n",
    "# Aggregate function: returns a list of objects with duplicates.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The function is non-deterministic because the order of collected results depends on the order of the rows which may be non-deterministic after a shuffle.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "# df2.agg(collect_list('age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.writeStream¶\n",
    "# property DataFrame.writeStream\n",
    "# Interface for saving the content of the streaming DataFrame out into external storage.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# DataStreamWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.exists\n",
    "# pyspark.sql.functions.exists(col, f)[source]\n",
    "# Returns whether a predicate holds for one or more elements in the array.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# (x: Column) -> Column: ... returning the Boolean expression. Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# :return: a :class:`~pyspark.sql.Column`\n",
    "df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
    "df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.get_json_object\n",
    "# pyspark.sql.functions.get_json_object(col, path)[source]\n",
    "# Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# string column in json format\n",
    "\n",
    "# pathstr\n",
    "# path to the json object to extract\n",
    "data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
    "                  get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.grouping_id¶\n",
    "# pyspark.sql.functions.grouping_id(*cols)[source]\n",
    "# Aggregate function: returns the level of grouping, equals to\n",
    "\n",
    "# (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + … + grouping(cn)\n",
    "\n",
    "# New in version 2.0.0.\n",
    "df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.greatest\n",
    "# pyspark.sql.functions.greatest(*cols)[source]\n",
    "# Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.grouping\n",
    "# pyspark.sql.functions.grouping(col)[source]\n",
    "# Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yspark.sql.DataFrame.dropDuplicates\n",
    "# DataFrame.dropDuplicates(subset=None)[source]\n",
    "# Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n",
    "\n",
    "# For a static batch DataFrame, it just drops duplicate rows. For a streaming DataFrame, it will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark() to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates.\n",
    "\n",
    "# drop_duplicates() is an alias for dropDuplicates().\n",
    "\n",
    "# New in version 1.4.0.\n",
    "from pyspark.sql import Row\n",
    "df = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.writeTo\n",
    "# DataFrame.writeTo(table)[source]\n",
    "# Create a write configuration builder for v2 sources.\n",
    "\n",
    "# This builder is used to configure and execute write operations.\n",
    "\n",
    "# For example, to append or create or replace existing tables.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Examples\n",
    "f.writeTo(\"catalog.db.table\").append()  \n",
    "df.writeTo(                              \n",
    "    \"catalog.db.table\"\n",
    ").partitionedBy(\"col\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.json_tuple\n",
    "# pyspark.sql.functions.json_tuple(col, *fields)[source]\n",
    "# Creates a new row for a json column according to the given field names.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# string column in json format\n",
    "\n",
    "# fieldsstr\n",
    "# fields to extract\n",
    "data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.unix_timestamp\n",
    "# pyspark.sql.functions.unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')[source]\n",
    "# Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.\n",
    "\n",
    "# if timestamp is None, then it returns current timestamp.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.least\n",
    "# pyspark.sql.functions.least(*cols)[source]\n",
    "# Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    ">>>\n",
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.explode\n",
    "# pyspark.sql.functions.explode(col)[source]\n",
    "# Returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.schema_of_csv\n",
    "# pyspark.sql.functions.schema_of_csv(csv, options={})[source]\n",
    "# Parses a CSV string and infers its schema in DDL format.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# csvColumn or str\n",
    "# a CSV string or a foldable string column containing a CSV string.\n",
    "\n",
    "# optionsdict, optional\n",
    "# options to control parsing. accepts the same options as the CSV datasource\n",
    "df = spark.range(1)\n",
    "df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.to_csv\n",
    "# pyspark.sql.functions.to_csv(col, options={})[source]\n",
    "# Converts a column containing a StructType into a CSV string. Throws an exception, in the case of an unsupported type.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column containing a struct.\n",
    "\n",
    "# options: dict, optional\n",
    "# options to control converting. accepts the same options as the CSV datasource.\n",
    "from pyspark.sql import Row\n",
    "data = [(1, Row(age=2, name='Alice'))]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_csv(df.value).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.desc_nulls_last¶\n",
    "# pyspark.sql.functions.desc_nulls_last(col)[source]\n",
    "# Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.kurtosis\n",
    "# pyspark.sql.functions.kurtosis(col)[source]\n",
    "# Aggregate function: returns the kurtosis of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.lag\n",
    "# pyspark.sql.functions.lag(col, offset=1, default=None)[source]\n",
    "# Window function: returns the value that is offset rows before the current row, and default if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition.\n",
    "\n",
    "# This is equivalent to the LAG function in SQL.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# offsetint, optional\n",
    "# number of row to extend\n",
    "\n",
    "# defaultoptional\n",
    "# default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.last\n",
    "# pyspark.sql.functions.last(col, ignorenulls=False)[source]\n",
    "# Aggregate function: returns the last value in a group.\n",
    "\n",
    "# The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The function is non-deterministic because its results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.element_at\n",
    "# pyspark.sql.functions.element_at(col, extraction)[source]\n",
    "# Collection function: Returns element of array at given index in extraction if col is array. Returns value for the given key in extraction if col is map.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column containing array or map\n",
    "\n",
    "# extraction :\n",
    "# index to check for in array or key to check for in map\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
    "df.select(element_at(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.repartitionByRange\n",
    "# DataFrame.repartitionByRange(numPartitions, *cols)[source]\n",
    "# Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is range partitioned.\n",
    "\n",
    "# At least one partition-by expression must be specified. When no explicit sort order is specified, “ascending nulls first” is assumed.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# numPartitionsint\n",
    "# can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "# colsstr or Column\n",
    "# partitioning columns.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# Due to performance reasons this method uses sampling to estimate the ranges. Hence, the output may not be consistent, since sampling can return different values. The sample size can be controlled by the config spark.sql.execution.rangeExchange.sampleSizePerPartition.\n",
    "df.repartitionByRange(2, \"age\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.summary(*statistics)[source]\n",
    "# Computes specified statistics for numeric and string columns. Available statistics are: - count - mean - stddev - min - max - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
    "\n",
    "# If no statistics are given, this function computes count, mean, stddev, min, approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
    "df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.repartition\n",
    "# DataFrame.repartition(numPartitions, *cols)[source]\n",
    "# Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# numPartitionsint\n",
    "# can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "# colsstr or Column\n",
    "# partitioning columns.\n",
    "\n",
    "# Changed in version 1.6: Added optional arguments to specify the partitioning columns. Also made numPartitions optional if partitioning columns are specified.\n",
    "df.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.coalesce\n",
    "# DataFrame.coalesce(numPartitions)[source]\n",
    "# Returns a new DataFrame that has exactly numPartitions partitions.\n",
    "\n",
    "# Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "# However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is).\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# numPartitionsint\n",
    "# specify the target number of partitions\n",
    "df.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.degrees\n",
    "# pyspark.sql.functions.degrees(col)[source]\n",
    "# Converts an angle measured in radians to an approximately equivalent angle measured in degrees.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "pyspark.sql.functions.unbase64\n",
    "pyspark.sql.functions.unbase64(col)[source]\n",
    "Decodes a BASE64 encoded string column and returns it as a binary column.\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# angle in radians\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# angle in degrees, as if computed by java.lang.Math.toDegrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameNaFunctions.fill\n",
    "# DataFrameNaFunctions.fill(value, subset=None)[source]\n",
    "# Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n",
    "\n",
    "# New in version 1.3.1.\n",
    "\n",
    "# Parameters:\n",
    "# valueint, float, string, bool or dict\n",
    "# Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "# subsetstr, tuple or list, optional\n",
    "# optional list of column names to consider. Columns specified in subset that do not have matching data type are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "df4.na.fill(50).show()\n",
    "\n",
    "df5.na.fill(False).show()\n",
    "df4.na.fill({'age': 50, 'name': 'unknown'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sequence\n",
    "# pyspark.sql.functions.sequence(start, stop, step=None)[source]\n",
    "# Generate a sequence of integers from start to stop, incrementing by step. If step is not set, incrementing by 1 if start is less than or equal to stop, otherwise -1.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
    "df1.select(sequence('C1', 'C2').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.rtrim\n",
    "# pyspark.sql.functions.rtrim(col)[source]\n",
    "# Trim the spaces from right end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hash\n",
    "# pyspark.sql.functions.hash(*cols)[source]\n",
    "# Calculates the hash code of given columns, and returns the result as an int column.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "    spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hex\n",
    "# pyspark.sql.functions.hex(col)[source]\n",
    "# Computes hex value of the given column, which could be pyspark.sql.types.StringType, pyspark.sql.types.BinaryType, pyspark.sql.types.IntegerType or pyspark.sql.types.LongType.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.upper\n",
    "# pyspark.sql.functions.upper(col)[source]\n",
    "# Converts a string expression to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.var_samp\n",
    "# pyspark.sql.functions.var_samp(col)[source]\n",
    "# Aggregate function: returns the unbiased sample variance of the values in a group.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.when\n",
    "# pyspark.sql.functions.when(condition, value)[source]\n",
    "# Evaluates a list of conditions and returns one of multiple possible result expressions. If pyspark.sql.Column.otherwise() is not invoked, None is returned for unmatched conditions.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# conditionColumn\n",
    "# a boolean Column expression.\n",
    "\n",
    "# value :\n",
    "# a literal value, or a Column expression.\n",
    "\n",
    "# >>> df.select(when(df[‘age’] == 2, 3).otherwise(4).alias(“age”)).collect()\n",
    "# [Row(age=3), Row(age=4)]\n",
    "# >>> df.select(when(df.age == 2, df.age + 1).alias(“age”)).collect()\n",
    "# [Row(age=3), Row(age=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.column\n",
    "# pyspark.sql.functions.column(col)\n",
    "# Returns a Column based on the given column name.’ Examples ——– >>> col(‘x’) Column<’x’> >>> column(‘x’) Column<’x’>\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.concat\n",
    "# pyspark.sql.functions.concat(*cols)[source]\n",
    "# Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat(df.s, df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.schema_of_json\n",
    "# pyspark.sql.functions.schema_of_json(json, options={})[source]\n",
    "# Parses a JSON string and infers its schema in DDL format.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# jsonColumn or str\n",
    "# a JSON string or a foldable string column containing a JSON string.\n",
    "\n",
    "# optionsdict, optional\n",
    "# options to control parsing. accepts the same options as the JSON datasource\n",
    "\n",
    "# Changed in version 3.0: It accepts options parameter to control schema inferring.\n",
    "df = spark.range(1)\n",
    "df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.nth_value\n",
    "# pyspark.sql.functions.nth_value(col, offset, ignoreNulls=False)[source]\n",
    "# Window function: returns the value that is the offsetth row of the window frame (counting from 1), and null if the size of window frame is less than offset rows.\n",
    "\n",
    "# It will return the offsetth non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "# This is equivalent to the nth_value function in SQL.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# offsetint, optional\n",
    "# number of row to use as the value\n",
    "\n",
    "# ignoreNullsbool, optional\n",
    "# indicates the Nth value should skip null in the determination of which row to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.slice\n",
    "# pyspark.sql.functions.slice(x, start, length)[source]\n",
    "# Collection function: returns an array containing all the elements in x from index start (array indices start at 1, or from the end if start is negative) with the specified length.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# xColumn or str\n",
    "# the array to be sliced\n",
    "\n",
    "# startColumn or int\n",
    "# the starting index\n",
    "\n",
    "# lengthColumn or int\n",
    "# the length of the slice\n",
    "df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
    "df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.flatten\n",
    "# pyspark.sql.functions.flatten(col)[source]\n",
    "# Collection function: creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
    "df.select(flatten(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.filter\n",
    "# pyspark.sql.functions.filter(col, f)[source]\n",
    "# Returns an array of elements for which a predicate holds in a given array.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "\n",
    "# ffunction\n",
    "# A function that returns the Boolean expression. Can take one of the following forms:\n",
    "\n",
    "# Unary (x: Column) -> Column: ...\n",
    "\n",
    "# Binary (x: Column, i: Column) -> Column..., where the second argument is\n",
    "# a 0-based index of the element.\n",
    "\n",
    "# and can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "def after_second_quarter(x):\n",
    "    return month(to_date(x)) > 6\n",
    "df.select(\n",
    "    filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession.table\n",
    "# SparkSession.table(tableName)[source]\n",
    "# Returns the specified table as a DataFrame.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Returns:\n",
    "# DataFrame\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.table(\"table1\")\n",
    "sorted(df.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.mean\n",
    "# pyspark.sql.functions.mean(col)[source]\n",
    "# Aggregate function: returns the average of the values in a group.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.sum\n",
    "# GroupedData.sum(*cols)[source]\n",
    "# Computes the sum for each numeric columns for each group.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr\n",
    "# column names. Non-numeric columns are ignored.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.groupBy().sum('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.to_json\n",
    "# pyspark.sql.functions.to_json(col, options={})[source]\n",
    "# Converts a column containing a StructType, ArrayType or a MapType into a JSON string. Throws an exception, in the case of an unsupported type.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column containing a struct, an array or a map.\n",
    "\n",
    "# optionsdict, optional\n",
    "# options to control converting. accepts the same options as the JSON datasource. Additionally the function supports the pretty option which enables pretty JSON generation.\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "data = [(1, Row(age=2, name='Alice'))]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sha2\n",
    "# pyspark.sql.functions.sha2(col, numBits)[source]\n",
    "# Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The numBits indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
    "digests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.first\n",
    "# pyspark.sql.functions.first(col, ignorenulls=False)[source]\n",
    "# Aggregate function: returns the first value in a group.\n",
    "\n",
    "# The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The function is non-deterministic because its results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.shuffle\n",
    "# pyspark.sql.functions.shuffle(col)[source]\n",
    "# Collection function: Generates a random permutation of the given array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
    "df.select(shuffle(df.data).alias('s')).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.size\n",
    "# pyspark.sql.functions.size(col)[source]\n",
    "# Collection function: returns the length of the array or map stored in the column.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
    "df.select(size(df.data)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.percentile_approx\n",
    "# pyspark.sql.functions.percentile_approx(col, percentage, accuracy=10000)[source]\n",
    "# Returns the approximate percentile of the numeric column col which is the smallest value in the ordered col values (sorted from least to greatest) such that no more than percentage of col values is less than the value or equal to that value. The value of percentage must be between 0.0 and 1.0.\n",
    "\n",
    "# The accuracy parameter (default: 10000) is a positive numeric literal which controls approximation accuracy at the cost of memory. Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error of the approximation.\n",
    "\n",
    "# When percentage is an array, each value of the percentage array must be between 0.0 and 1.0. In this case, returns the approximate percentile array of column col at the given percentage array.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "key = (col(\"id\") % 3).alias(\"key\")\n",
    "value = (randn(42) + key * 10).alias(\"value\")\n",
    "df = spark.range(0, 1000, 1, 1).select(key, value)\n",
    "df.select(\n",
    "    percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.skewness¶\n",
    "# pyspark.sql.functions.skewness(col)[source]\n",
    "# Aggregate function: returns the skewness of the values in a group.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.shiftLeft\n",
    "# pyspark.sql.functions.shiftLeft(col, numBits)[source]\n",
    "# Shift the given value numBits left.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.shiftRight\n",
    "# pyspark.sql.functions.shiftRight(col, numBits)[source]\n",
    "# (Signed) shift the given value numBits right.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.shiftRightUnsigned\n",
    "# pyspark.sql.functions.shiftRightUnsigned(col, numBits)[source]\n",
    "# Unsigned shift the given value numBits right.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "df = spark.createDataFrame([(-42,)], ['a'])\n",
    "df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.min\n",
    "# GroupedData.min(*cols)[source]\n",
    "# Computes the min value for each numeric column for each group.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr\n",
    "# column names. Non-numeric columns are ignored.\n",
    "df.groupBy().min('age').collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.rpad\n",
    "# pyspark.sql.functions.rpad(col, len, pad)[source]\n",
    "# Right-pad the string column to width len with pad.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(rpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sha1\n",
    "# pyspark.sql.functions.sha1(col)[source]\n",
    "# Returns the hex string result of SHA-1.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.max\n",
    "# GroupedData.max(*cols)[source]\n",
    "# Computes the max value for each numeric columns for each group.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df.groupBy().max('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.mean\n",
    "# GroupedData.mean(*cols)[source]\n",
    "# Computes average values for each numeric columns for each group.\n",
    "\n",
    "# mean() is an alias for avg().\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr\n",
    "# column names. Non-numeric columns are ignored.\n",
    "df.groupBy().mean('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Row.asDict\n",
    "# Row.asDict(recursive=False)[source]\n",
    "# Return as a dict\n",
    "\n",
    "# Parameters:\n",
    "# recursivebool, optional\n",
    "# turns the nested Rows to dict (default: False).\n",
    "\n",
    "# Notes\n",
    "\n",
    "# If a row contains duplicate field names, e.g., the rows of a join between two DataFrame that both have the fields of same names, one of the duplicate fields will be selected by asDict. __getitem__ will also return one of the duplicate fields, however returned value might be different to asDict.\n",
    "Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
    "row = Row(key=1, value=Row(name='a', age=2))\n",
    "row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
    "\n",
    "row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.split¶\n",
    "# pyspark.sql.functions.split(str, pattern, limit=- 1)[source]\n",
    "# Splits str around matches of the given pattern.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# strColumn or str\n",
    "# a string expression to split\n",
    "\n",
    "# patternstr\n",
    "# a string representing a regular expression. The regex string should be a Java regular expression.\n",
    "\n",
    "# limitint, optional\n",
    "# an integer which controls the number of times pattern is applied.\n",
    "\n",
    "# limit > 0: The resulting array’s length will not be more than limit, and the\n",
    "# resulting array’s last entry will contain all input beyond the last matched pattern.\n",
    "\n",
    "# limit <= 0: pattern will be applied as many times as possible, and the resulting\n",
    "# array can be of any size.\n",
    "\n",
    "# Changed in version 3.0: split now takes an optional limit field. If not provided, default limit value is -1.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
    "df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
    "[Row(s=['one', 'twoBthreeC'])]\n",
    "df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
    "[Row(s=['one', 'two', 'three', ''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.pivot\n",
    "# GroupedData.pivot(pivot_col, values=None)[source]\n",
    "# Pivots a column of the current DataFrame and perform the specified aggregation. There are two versions of pivot function: one that requires the caller to specify the list of distinct values to pivot on, and one that does not. The latter is more concise but less efficient, because Spark needs to first compute the list of distinct values internally.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Parameters:\n",
    "# pivot_colstr\n",
    "# Name of the column to pivot.\n",
    "\n",
    "# valueslist, optional\n",
    "# List of values that will be translated to columns in the output DataFrame.\n",
    "df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sin\n",
    "# pyspark.sql.functions.sin(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# Returns:\n",
    "# Column\n",
    "# sine of the angle, as if computed by java.lang.Math.sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sinh\n",
    "# pyspark.sql.functions.sinh(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# hyperbolic angle\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# hyperbolic sine of the given value, as if computed by java.lang.Math.sinh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrameNaFunctions.replace¶\n",
    "# DataFrameNaFunctions.replace(to_replace, value=<no value>, subset=None)[source]\n",
    "# Returns a new DataFrame replacing a value with another value. DataFrame.replace() and DataFrameNaFunctions.replace() are aliases of each other. Values to_replace and value must have the same type and can only be numerics, booleans, or strings. Value can have None. When replacing, the new value will be cast to the type of the existing column. For numeric replacements all values to be replaced should have unique floating point representation. In case of conflicts (for example with {42: -1, 42.0: 1}) and arbitrary replacement will be used.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# to_replacebool, int, float, string, list or dict\n",
    "# Value to be replaced. If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement.\n",
    "\n",
    "# valuebool, int, float, string or None, optional\n",
    "# The replacement value must be a bool, int, float, string or None. If value is a list, value should be of the same length and type as to_replace. If value is a scalar and to_replace is a sequence, then value is used as a replacement for each item in to_replace.\n",
    "\n",
    "# subsetlist, optional\n",
    "# optional list of column names to consider. Columns specified in subset that do not have matching data type are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "df4.na.replace('Alice', None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.avg\n",
    "# GroupedData.avg(*cols)[source]\n",
    "# Computes average values for each numeric columns for each group.\n",
    "\n",
    "# mean() is an alias for avg().\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colsstr\n",
    "# column names. Non-numeric columns are ignored.\n",
    "df.groupBy().avg('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.signum\n",
    "# pyspark.sql.functions.signum(col)[source]\n",
    "# Computes the signum of the given value.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.cogroup\n",
    "# GroupedData.cogroup(other)\n",
    "# Cogroups this group with another group so that we can run cogrouped operations.\n",
    "\n",
    "# New in version 3.0.0.\n",
    "\n",
    "# See PandasCogroupedOps for the operations that can be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.count\n",
    "# GroupedData.count()[source]\n",
    "# Counts the number of records for each group.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "\n",
    "sorted(df.groupBy(df.age).count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.max\n",
    "# pyspark.sql.functions.max(col)[source]\n",
    "# Aggregate function: returns the maximum value of the expression in a group.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.md5\n",
    "# pyspark.sql.functions.md5(col)[source]\n",
    "# Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.expr¶\n",
    "# pyspark.sql.functions.expr(str)[source]\n",
    "# Parses the expression string into the column that it represents\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df.select(expr(\"length(name)\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.factorial\n",
    "# pyspark.sql.functions.factorial(col)[source]\n",
    "# Computes the factorial of the given value.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([(5,)], ['n'])\n",
    "df.select(factorial(df.n).alias('f')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.regexp_replace\n",
    "# pyspark.sql.functions.regexp_replace(str, pattern, replacement)[source]\n",
    "# Replace all substrings of the specified string value that match regexp with rep.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.rint\n",
    "pyspark.sql.functions.rint(col)[source]\n",
    "# Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.round\n",
    "# pyspark.sql.functions.round(col, scale=0)[source]\n",
    "# Round the given value to scale decimal places using HALF_UP rounding mode if scale >= 0 or at integral part when scale < 0.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.repeat\n",
    "# pyspark.sql.functions.repeat(col, n)[source]\n",
    "# Repeats a string column n times, and returns it as a new string column.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('ab',)], ['s',])\n",
    "df.select(repeat(df.s, 3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.reverse\n",
    "# pyspark.sql.functions.reverse(col)[source]\n",
    "# Collection function: returns a reversed string or an array with reverse order of elements.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
    "df.select(reverse(df.data).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.row_number\n",
    "# pyspark.sql.functions.row_number()[source]\n",
    "# Window function: returns a sequential number starting at 1 within a window partition.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.current_date\n",
    "pyspark.sql.functions.current_date()[source]\n",
    "# Returns the current date at the start of query evaluation as a DateType column. All calls of current_date within the same query return the same value.\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.current_timestamp\n",
    "# pyspark.sql.functions.current_timestamp()[source]\n",
    "# Returns the current timestamp at the start of query evaluation as a TimestampType column. All calls of current_timestamp within the same query return the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.exceptAll\n",
    "# DataFrame.exceptAll(other)[source]\n",
    "# Return a new DataFrame containing rows in this DataFrame but not in another DataFrame while preserving duplicates.\n",
    "\n",
    "# This is equivalent to EXCEPT ALL in SQL. As standard in SQL, this function resolves columns by position (not by name).\n",
    "\n",
    "# New in version 2.4.0.\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
    "df1.exceptAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.months_between\n",
    "# pyspark.sql.functions.months_between(date1, date2, roundOff=True)[source]\n",
    "# Returns number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). The result is rounded off to 8 digits unless roundOff is set to False.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
    "df.select(months_between(df.date1, df.date2).alias('months')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.persist\n",
    "# DataFrame.persist(storageLevel=StorageLevel(True, True, False, True, 1))[source]\n",
    "# Sets the storage level to persist the contents of the DataFrame across operations after the first time it is computed. This can only be used to assign a new storage level if the DataFrame does not have a storage level set yet. If no storage level is specified defaults to (MEMORY_AND_DISK_DESER)\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The default storage level has changed to MEMORY_AND_DISK_DESER to match Scala in 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.conv\n",
    "# pyspark.sql.functions.conv(col, fromBase, toBase)[source]\n",
    "# Convert a number in a string column from one base to another.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    " df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
    "df.select(conv(df.n, 2, 16).alias('hex')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.asc_nulls_first\n",
    "# Column.asc_nulls_first()\n",
    "# Returns a sort expression based on ascending order of the column, and null values return before non-null values.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
    "df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.cast\n",
    "# Column.cast(dataType)[source]\n",
    "# Casts the column into type dataType.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "df.select(df.age.cast(\"string\").alias('ages')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.bitwiseOR\n",
    "# Column.bitwiseOR(other)\n",
    "# Compute bitwise OR of this expression with another expression.\n",
    "\n",
    "# Parameters:\n",
    "# other\n",
    "# a value or Column to calculate bitwise or(|) with this Column.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([Row(a=170, b=75)])\n",
    "# df.select(df.a.bitwiseOR(df.b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.bitwiseAND¶\n",
    "# Column.bitwiseAND(other)\n",
    "# Compute bitwise AND of this expression with another expression.\n",
    "\n",
    "# Parameters:\n",
    "# other\n",
    "# a value or Column to calculate bitwise and(&) with this Column.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([Row(a=170, b=75)])\n",
    "# df.select(df.a.bitwiseAND(df.b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.over\n",
    "# Column.over(window)[source]\n",
    "# Define a windowing column.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "from pyspark.sql import Window\n",
    "window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "from pyspark.sql.functions import rank, min\n",
    "from pyspark.sql.functions import desc\n",
    "df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.between\n",
    "# Column.between(lowerBound, upperBound)[source]\n",
    "# True if the current column is between the lower bound and upper bound, inclusive.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df.select(df.name, df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.desc_nulls_last\n",
    "# Column.desc_nulls_last()\n",
    "# Returns a sort expression based on the descending order of the column, and null values appear after non-null values.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
    "df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.assert_true\n",
    "# pyspark.sql.functions.assert_true(col, errMsg=None)[source]\n",
    "# Returns null if the input column is true; throws an exception with the provided error message otherwise.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
    "df.select(assert_true(df.a < df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.bitwiseXOR\n",
    "# Column.bitwiseXOR(other)\n",
    "# Compute bitwise XOR of this expression with another expression.\n",
    "\n",
    "# Parameters:\n",
    "# other\n",
    "# a value or Column to calculate bitwise xor(^) with this Column.\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([Row(a=170, b=75)])\n",
    "# df.select(df.a.bitwiseXOR(df.b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.contains\n",
    "# Column.contains(other)\n",
    "# Contains the other element. Returns a boolean Column based on a string match.\n",
    "\n",
    "# Parameters:\n",
    "# other\n",
    "# string in line. A value as a literal or a Column.\n",
    "\n",
    "\n",
    "df.filter(df.name.contains('o')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.asc_nulls_last\n",
    "# Column.asc_nulls_last()\n",
    "# Returns a sort expression based on ascending order of the column, and null values appear after non-null values.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
    "df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.astype\n",
    "# Column.astype(dataType)\n",
    "# astype() is an alias for cast().\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.dropFields\n",
    "# Column.dropFields(*fieldNames)[source]\n",
    "# An expression that drops fields in StructType by name. This is a no-op if schema doesn’t contain field name(s).\n",
    "\n",
    "# New in version 3.1.0.\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
    "df.withColumn('a', df['a'].dropFields('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_repeat\n",
    "# pyspark.sql.functions.array_repeat(col, count)[source]\n",
    "# Collection function: creates an array containing a column repeated count times.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "df = spark.createDataFrame([('ab',)], ['data'])\n",
    "df.select(array_repeat(df.data, 3).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_sort\n",
    "# pyspark.sql.functions.array_sort(col)[source]\n",
    "# Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
    "df.select(array_sort(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.endswith¶\n",
    "# Column.endswith(other)\n",
    "# String ends with. Returns a boolean Column based on a string match.\n",
    "\n",
    "# Parameters:\n",
    "# otherColumn or str\n",
    "# string at end of line (do not use a regex $)\n",
    "df.filter(df.name.endswith('ice')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.desc_nulls_first¶\n",
    "# Column.desc_nulls_first()\n",
    "# Returns a sort expression based on the descending order of the column, and null values appear before non-null values.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
    "# df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
    "# [Row(name=None), Row(name='Tom'), Row(name='Alice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.months_between¶\n",
    "# pyspark.sql.functions.months_between(date1, date2, roundOff=True)[source]\n",
    "# Returns number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). The result is rounded off to 8 digits unless roundOff is set to False.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
    "df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
    "df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.asc¶\n",
    "# Column.asc()\n",
    "# Returns a sort expression based on ascending order of the column.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
    "# df.select(df.name).orderBy(df.name.asc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.desc¶\n",
    "# Column.desc()\n",
    "# Returns a sort expression based on the descending order of the column.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# from pyspark.sql import Row\n",
    "# df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
    "# df.select(df.name).orderBy(df.name.desc()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.atan2\n",
    "# # pyspark.sql.functions.atan2(col1, col2)[source]pyspark.sql.functions.months\n",
    "# pyspark.sql.functions.months(col)[source]\n",
    "# Partition transform function: A transform for timestamps and dates to partition data into months.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# df.writeTo(\"catalog.db.table\").partitionedBy(\n",
    "#     months(\"ts\")\n",
    "# ).createOrReplace()  \n",
    "# # New in version 1.4.0.\n",
    "\n",
    "# # Parameters:\n",
    "# # col1str, Column or float\n",
    "# # coordinate on y-axis\n",
    "\n",
    "# # col2str, Column or float\n",
    "# # coordinate on x-axis\n",
    "\n",
    "# # Returns:\n",
    "# # Column\n",
    "# # the theta component of the point (r, theta) in polar coordinates that corresponds to the point (x, y) in Cartesian coordinates, as if computed by java.lang.Math.atan2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.nanvl\n",
    "# pyspark.sql.functions.nanvl(col1, col2)[source]\n",
    "# Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
    "\n",
    "# Both inputs should be floating point columns (DoubleType or FloatType).\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.next_day\n",
    "# pyspark.sql.functions.next_day(date, dayOfWeek)[source]\n",
    "# Returns the first date which is later than the value of the date column.\n",
    "\n",
    "# Day of the week parameter is case insensitive, and accepts:\n",
    "# “Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
    "df.select(next_day(df.d, 'Sun').alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.minute\n",
    "# pyspark.sql.functions.minute(col)[source]\n",
    "# Extract the minutes of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
    "df.select(minute('ts').alias('minute')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.monotonically_increasing_id\n",
    "# pyspark.sql.functions.monotonically_increasing_id()[source]\n",
    "# A column that generates monotonically increasing 64-bit integers.\n",
    "\n",
    "# The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
    "df0.select(monotonically_increasing_id().alias('id')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.month\n",
    "# pyspark.sql.functions.month(col)[source]\n",
    "# Extract the month of a given date as integer.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(month('dt').alias('month')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.stddev_samp\n",
    "# pyspark.sql.functions.stddev_samp(col)[source]\n",
    "# Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
    "\n",
    "# New in version 1.6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.spark_partition_id\n",
    "# pyspark.sql.functions.spark_partition_id()[source]\n",
    "# A column for partition ID.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.collect_set¶\n",
    "# pyspark.sql.functions.collect_set(col)[source]\n",
    "# Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The function is non-deterministic because the order of collected results depends on the order of the rows which may be non-deterministic after a shuffle.\n",
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(collect_set('age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.sqrt\n",
    "# pyspark.sql.functions.sqrt(col)[source]\n",
    "# Computes the square root of the specified float value.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.stddev\n",
    "# pyspark.sql.functions.stddev(col)[source]\n",
    "# Aggregate function: alias for stddev_samp.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.stddev_pop\n",
    "# pyspark.sql.functions.stddev_pop(col)[source]\n",
    "# Aggregate function: returns population standard deviation of the expression in a group.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.name\n",
    "# Column.name(*alias, **kwargs)\n",
    "# name() is an alias for alias().\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.name\n",
    "# Column.name(*alias, **kwargs)\n",
    "# name() is an alias for alias().\n",
    "\n",
    "# New in version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.coalesce\n",
    "# pyspark.sql.functions.coalesce(*cols)[source]\n",
    "# Returns the first column that is not null.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.rlike\n",
    "# Column.rlike(other)\n",
    "# SQL RLIKE expression (LIKE with Regex). Returns a boolean Column based on a regex match.\n",
    "\n",
    "# Parameters:\n",
    "# otherstr\n",
    "# an extended regex expression\n",
    "df.filter(df.name.rlike('ice$')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.startswith\n",
    "# Column.startswith(other)\n",
    "# String starts with. Returns a boolean Column based on a string match.\n",
    "\n",
    "# Parameters:\n",
    "# otherColumn or str\n",
    "# string at start of line (do not use a regex ^)\n",
    "df.filter(df.name.startswith('Al')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.substr\n",
    "# Column.substr(startPos, length)[source]\n",
    "# Return a Column which is a substring of the column.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# startPosColumn or int\n",
    "# start position\n",
    "\n",
    "# lengthColumn or int\n",
    "# length of the substring\n",
    "df.select(df.name.substr(1, 3).alias(\"col\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.getField\n",
    "# Column.getField(name)[source]\n",
    "# An expression that gets a field by name in a StructType.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Examples\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
    "df.select(df.r.getField(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_position\n",
    "# pyspark.sql.functions.array_position(col, value)[source]\n",
    "# Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# The position is not zero based, but 1 based index. Returns 0 if the given value could not be found in the array.\n",
    "df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
    "df.select(array_position(df.data, \"a\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.covar_pop\n",
    "# pyspark.sql.functions.covar_pop(col1, col2)[source]\n",
    "# Returns a new Column for the population covariance of col1 and col2.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.registerTempTable\n",
    "# DataFrame.registerTempTable(name)[source]\n",
    "# Registers this DataFrame as a temporary table using the given name.\n",
    "\n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df.registerTempTable(\"people\")\n",
    "df2 = spark.sql(\"select * from people\")\n",
    "sorted(df.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.getItem\n",
    "# Column.getItem(key)[source]\n",
    "# An expression that gets an item at position ordinal out of a list, or gets an item by key out of a dict.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
    "df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.isNotNull\n",
    "# Column.isNotNull()\n",
    "# True if the current expression is NOT null.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
    "df.filter(df.height.isNotNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.isNull\n",
    "# Column.isNull()\n",
    "# True if the current expression is null.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
    "df.filter(df.height.isNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.isin\n",
    "# Column.isin(*cols)[source]\n",
    "# A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df[df.name.isin(\"Bob\", \"Mike\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.like\n",
    "# Column.like(other)\n",
    "# SQL like expression. Returns a boolean Column based on a SQL LIKE match.\n",
    "\n",
    "# Parameters:\n",
    "# otherstr\n",
    "# a SQL LIKE pattern\n",
    "df.filter(df.name.like('Al%')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.checkpoint\n",
    "# DataFrame.checkpoint(eager=True)[source]\n",
    "# Returns a checkpointed version of this DataFrame. Checkpointing can be used to truncate the logical plan of this DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with SparkContext.setCheckpointDir().\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# eagerbool, optional\n",
    "# Whether to checkpoint this DataFrame immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.approx_count_distinct\n",
    "# pyspark.sql.functions.approx_count_distinct(col, rsd=None)[source]\n",
    "# Aggregate function: returns a new Column for approximate distinct count of column col.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# rsdfloat, optional\n",
    "# maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to use countDistinct()\n",
    "df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.concat_ws¶\n",
    "# pyspark.sql.functions.concat_ws(sep, *cols)[source]\n",
    "# Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat_ws('-', df.s, df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.translate\n",
    "# pyspark.sql.functions.translate(srcCol, matching, replace)[source]\n",
    "# A function translate any character in the srcCol by a character in matching. The characters in replace is corresponding to the characters in matching. The translate will happen when any character in the string matching with the character in the matching.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "\n",
    "# >>>\n",
    "# spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
    "#     .alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.trim\n",
    "# pyspark.sql.functions.trim(col)[source]\n",
    "# Trim the spaces from both ends for the specified string column.\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.bucket\n",
    "# pyspark.sql.functions.bucket(numBuckets, col)[source]\n",
    "# Partition transform function: A transform for any type that partitions by a hash of the input column.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    bucket(42, \"ts\")\n",
    ").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.crc32\n",
    "# pyspark.sql.functions.crc32(col)[source]\n",
    "# Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "    spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.covar_samp\n",
    "# pyspark.sql.functions.covar_samp(col1, col2)[source]\n",
    "# Returns a new Column for the sample covariance of col1 and col2.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.to_timestamp\n",
    "# pyspark.sql.functions.to_timestamp(col, format=None)[source]\n",
    "# Converts a Column into pyspark.sql.types.TimestampType using the optionally specified format. Specify formats according to datetime pattern. By default, it follows casting rules to pyspark.sql.types.TimestampType if the format is omitted. Equivalent to col.cast(\"timestamp\").\n",
    "\n",
    "# New in version 2.2.0.\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(to_timestamp(df.t).alias('dt')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.isnull\n",
    "# pyspark.sql.functions.isnull(col)[source]\n",
    "# An expression that returns true iff the column is null.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
    "df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hypot\n",
    "# pyspark.sql.functions.hypot(col1, col2)[source]\n",
    "# Computes sqrt(a^2 + b^2) without intermediate overflow or underflow.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.instr\n",
    "# pyspark.sql.functions.instr(str, substr)[source]\n",
    "# Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(instr(df.s, 'b').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.isnan\n",
    "# pyspark.sql.functions.isnan(col)[source]\n",
    "# An expression that returns true iff the column is NaN.\n",
    "\n",
    "# New in version 1.6.0.\n",
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hours\n",
    "# pyspark.sql.functions.hours(col)[source]\n",
    "# Partition transform function: A transform for timestamps to partition data into hours.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Notes\n",
    "\n",
    "# This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "df.writeTo(\"catalog.db.table\").partitionedBy(   \n",
    "    hours(\"ts\")\n",
    ").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.functions.rank\n",
    "pyspark.sql.functions.rank()[source]\n",
    "Window function: returns the rank of rows within a window partition.\n",
    "\n",
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "This is equivalent to the RANK function in SQL.\n",
    "\n",
    "New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.hypot\n",
    "# pyspark.sql.functions.hypot(col1, col2)[source]\n",
    "# Computes sqrt(a^2 + b^2) without intermediate overflow or underflow.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.initcap\n",
    "# pyspark.sql.functions.initcap(col)[source]\n",
    "# Translate the first letter of each word to upper case in the sentence.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "# Examples\n",
    "spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.input_file_name¶\n",
    "# pyspark.sql.functions.input_file_name()[source]\n",
    "# Creates a string column for the file name of the current Spark task.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.raise_error\n",
    "# pyspark.sql.functions.raise_error(errMsg)[source]\n",
    "# Throws an exception with the provided error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.rand\n",
    "# pyspark.sql.functions.rand(seed=None)[source]\n",
    "# Generates a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0).\n",
    "\n",
    "# New in version 1.4.0.\n",
    "df.withColumn('rand', rand(seed=42) * 3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.randn\n",
    "# pyspark.sql.functions.randn(seed=None)[source]\n",
    "# Generates a column with independent and identically distributed (i.i.d.) samples from the standard normal distribution.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "df.withColumn('randn', randn(seed=42)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.asinh\n",
    "# pyspark.sql.functions.asinh(col)[source]\n",
    "# Computes inverse hyperbolic sine of the input column.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n",
    "# Returns:\n",
    "# Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.regexp_extract\n",
    "# pyspark.sql.functions.regexp_extract(str, pattern, idx)[source]\n",
    "# Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "\n",
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.bround\n",
    "# pyspark.sql.functions.bround(col, scale=0)[source]\n",
    "# Round the given value to scale decimal places using HALF_EVEN rounding mode if scale >= 0 or at integral part when scale < 0.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.bin\n",
    "# pyspark.sql.functions.bin(col)[source]\n",
    "# Returns the string representation of the binary value of the given column.\n",
    "\n",
    "# New in version 1.5.0.\n",
    "df.select(bin(df.age).alias('c')).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.bitwiseNOT\n",
    "pyspark.sql.functions.bitwiseNOT(col)[source]\n",
    "# Computes bitwise not.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.broadcast\n",
    "pyspark.sql.functions.broadcast(df)[source]\n",
    "# Marks a DataFrame as small enough for use in broadcast joins.\n",
    "\n",
    "# New in version 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.ceil\n",
    "pyspark.sql.functions.ceil(col)[source]\n",
    "# Computes the ceiling of the given value.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.cbrt¶\n",
    "pyspark.sql.functions.cbrt(col)[source]\n",
    "# Computes the cube-root of the given value.\n",
    "\n",
    "# New in version 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.avg\n",
    "pyspark.sql.functions.avg(col)[source]\n",
    "# Aggregate function: returns the average of the values in a group.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.base64¶\n",
    "pyspark.sql.functions.base64(col)[source]\n",
    "# Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.ascii\n",
    "pyspark.sql.functions.ascii(col)[source]\n",
    "# Computes the numeric value of the first character of the string column.\n",
    "\n",
    "# New in version 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.asin\n",
    "pyspark.sql.functions.asin(col)[source]\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Returns:\n",
    "# Column\n",
    "# inverse sine of col, as if computed by java.lang.Math.asin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.arrays_overlap\n",
    "# pyspark.sql.functions.arrays_overlap(a1, a2)[source]\n",
    "# Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
    "df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.asc\n",
    "# pyspark.sql.functions.asc(col)[source]\n",
    "# Returns a sort expression based on the ascending order of the given column name.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.crosstab\n",
    "# DataFrame.crosstab(col1, col2)[source]\n",
    "# Computes a pair-wise frequency table of the given columns. Also known as a contingency table. The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned. The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2. The name of the first column will be col1_\n",
    "# col2. Pairs that have no occurrences will have zero as their counts. DataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases.\n",
    "\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# col1str\n",
    "# The name of the first column. Distinct items will make the first item of each row.\n",
    "\n",
    "# col2str\n",
    "# The name of the second column. Distinct items will make the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.fillna\n",
    "# DataFrame.fillna(value, subset=None)[source]\n",
    "# Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n",
    "\n",
    "# New in version 1.3.1.\n",
    "\n",
    "# Parameters:\n",
    "# valueint, float, string, bool or dict\n",
    "# Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "# subsetstr, tuple or list, optional\n",
    "# optional list of column names to consider. Columns specified in subset that do not have matching data type are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "df4.na.fill(50).show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_min\n",
    "# pyspark.sql.functions.array_min(col)[source]\n",
    "# Collection function: returns the minimum value of the array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_min(df.data).alias('min')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.acos\n",
    "pyspark.sql.functions.acos(col)[source]\n",
    "# New in version 1.4.0.\n",
    "\n",
    "# Returns:\n",
    "# Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.acosh\n",
    "pyspark.sql.functions.acosh(col)[source]\n",
    "# Computes inverse hyperbolic cosine of the input column.\n",
    "\n",
    "# New in version 3.1.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.array_max\n",
    "# pyspark.sql.functions.array_max(col)[source]\n",
    "# Collection function: returns the maximum value of the array.\n",
    "\n",
    "# New in version 2.4.0.\n",
    "\n",
    "# Parameters:\n",
    "# colColumn or str\n",
    "# name of column or expression\n",
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_max(df.data).alias('max')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.createTempView\n",
    "# DataFrame.createTempView(name)[source]\n",
    "# Creates a local temporary view with this DataFrame.\n",
    "\n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame. throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "# New in version 2.0.0.\n",
    "\n",
    "# Examples\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.storageLevel\n",
    "# property DataFrame.storageLevel\n",
    "# Get the DataFrame’s current storage level.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.createOrReplaceGlobalTempView¶\n",
    "# DataFrame.createOrReplaceGlobalTempView(name)[source]\n",
    "# Creates or replaces a global temporary view using the given name.\n",
    "\n",
    "# The lifetime of this temporary view is tied to this Spark application.\n",
    "\n",
    "# New in version 2.2.0.\n",
    "df.createOrReplaceGlobalTempView(\"people\")\n",
    "df2 = df.filter(df.age > 3)\n",
    "df2.createOrReplaceGlobalTempView(\"people\")\n",
    "df3 = spark.sql(\"select * from global_temp.people\")\n",
    "sorted(df3.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.crossJoin\n",
    "# DataFrame.crossJoin(other)[source]\n",
    "# Returns the cartesian product with another DataFrame.\n",
    "\n",
    "# New in version 2.1.0.\n",
    "\n",
    "# Parameters:\n",
    "# otherDataFrame\n",
    "# Right side of the cartesian product.\n",
    "df.select(\"age\", \"name\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.colRegex\n",
    "# DataFrame.colRegex(colName)[source]\n",
    "# Selects column based on the column name specified as a regex and returns it as Column.\n",
    "\n",
    "# New in version 2.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colNamestr\n",
    "# string, column name specified as a regex.\n",
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
    "df.select(df.colRegex(\"`(Col1)?+.+`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.groupBy\n",
    "# DataFrame.groupBy(*cols)[source]\n",
    "# Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions.\n",
    "\n",
    "# groupby() is an alias for groupBy().\n",
    "\n",
    "# New in version 1.3.0.\n",
    "\n",
    "# Parameters:\n",
    "# colslist, str or Column\n",
    "# columns to group by. Each element should be a column name (string) or an expression (Column).\n",
    "sorted(df.groupBy('name').agg({'age': 'mean'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.foreachPartition\n",
    "# Applies the f function to each partition of this DataFrame.\n",
    "\n",
    "# This a shorthand for df.rdd.foreachPartition().\n",
    "\n",
    "# New in version 1.3.0.\n",
    "def f(people):\n",
    "    for person in people:\n",
    "        print(person.name)\n",
    "df.foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.columns\n",
    "# property DataFrame.columns\n",
    "# Returns all column names as a list.\n",
    "\n",
    "# New in version 1.3.0.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.subtract\n",
    "# DataFrame.subtract(other)[source]\n",
    "# Return a new DataFrame containing rows in this DataFrame but not in another DataFrame.\n",
    "\n",
    "# This is equivalent to EXCEPT DISTINCT in SQL.\n",
    "\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.count\n",
    "# Returns the number of rows in this DataFrame.\n",
    "# New in version 1.3.0.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pyspark.sql.DataFrame.collect\n",
    "# # Returns all the records as a list of Row.\n",
    "# New in version 1.3.0.\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.filter\n",
    "# DataFrame.filter(condition)[source]\n",
    "# Filters rows using the given condition.\n",
    "# where() is an alias for filter().\n",
    "# New in version 1.3.0.\n",
    "# Parameters:\n",
    "# conditionColumn or str\n",
    "# a Column of types.BooleanType or a string of SQL expression.\n",
    "df.filter(df.age > 3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.approxCountDistinct\n",
    "# pyspark.sql.functions.approxCountDistinct(col, rsd=None)[source]\n",
    "# Deprecated since version 2.1.0: Use approx_count_distinct() instead.\n",
    "# New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.functions.abs\n",
    "# pyspark.sql.functions.abs(col)[source]\n",
    "# Computes the absolute value.New in version 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.limit\n",
    "DataFrame.limit(num)[source]\n",
    "df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.rollup\n",
    "# DataFrame.rollup(*cols)[source]\n",
    "df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.freqItems\n",
    "# DataFrame.freqItems(cols, support=None)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.show\n",
    "# DataFrame.show(n=20, truncate=True, vertical=False)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.printSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.selectExpr\n",
    "# Projects a set of SQL expressions and returns a new DataFrame.\n",
    "# This is a variant of select() that accepts SQL expressions.\n",
    "df.selectExpr(\"age * 2\", \"abs(age)\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.tail\n",
    "# # DataFrame.tail(num)\n",
    "# Returns the last num rows as a list of Row.\n",
    "# Running tail requires moving data into the application’s driver process, and doing so with a very large num can crash the driver process with OutOfMemoryError.\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.take\n",
    "# DataFrame.take(num)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.toDF\n",
    "# DataFrame.toDF(*cols)\n",
    "df.toDF('f1', 'f2').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.cov\n",
    "# DataFrame.cov(col1, col2)[source]\n",
    "# Calculate the sample covariance for the given columns, specified by their names, as a double value. DataFrame.cov() and DataFrameStatFunctions.cov() are aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.createGlobalTempView\n",
    "# Creates a global temporary view with this DataFrame.\n",
    "df.createGlobalTempView(\"people\")\n",
    "df2 = spark.sql(\"select * from global_temp.people\")\n",
    "sorted(df.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.cube\n",
    "DataFrame.cube(*cols)[source]\n",
    "df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.DataFrame.intersect\n",
    "DataFrame.intersect(other)[source]\n",
    "# Return a new DataFrame containing rows only in both this DataFrame and another DataFrame.\n",
    "# This is equivalent to INTERSECT in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.createOrReplaceTempView\n",
    "# DataFrame.createOrReplaceTempView(name)[source]\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "df2 = df.filter(df.age > 3)\n",
    "df2.createOrReplaceTempView(\"people\")\n",
    "df3 = spark.sql(\"select * from people\")\n",
    "sorted(df3.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.SparkSession.version\n",
    "# property SparkSession.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.first\n",
    "# Returns the first row as a Row.\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.dtypes\n",
    "# Returns all column names and their data types as a list.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.DataFrame.na\n",
    "# Returns a DataFrameNaFunctions for handling missing values.\n",
    "property DataFrame.na"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e57b5609a0adf6ef8af7ad6d2063e8e9c24ef6935f7306ae9ba467b68a2bc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
